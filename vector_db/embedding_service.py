#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Embedding Service for creating vector embeddings using OpenAI API
"""

import logging
import os
import re
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import List, Optional

import tiktoken
from openai import OpenAI
from env_utils import load_env

logger = logging.getLogger(__name__)


class EmbeddingService:
    """–°–µ—Ä–≤–∏—Å –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —á–µ—Ä–µ–∑ OpenAI API —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π rate limiting"""
    
    def __init__(
        self, 
        model: str = "text-embedding-3-small", 
        api_key: Optional[str] = None,
        chunk_size: int = 2048,  # OpenAI –ª–∏–º–∏—Ç: –¥–æ 2048 —Ç–µ–∫—Å—Ç–æ–≤ –≤ –æ–¥–Ω–æ–º –∑–∞–ø—Ä–æ—Å–µ
        delay_between_requests: float = 15.0,  # –ó–∞–¥–µ—Ä–∂–∫–∞ –¥–ª—è —Å–æ–±–ª—é–¥–µ–Ω–∏—è RPM –ª–∏–º–∏—Ç–æ–≤
        max_retries: int = 5,
        retry_delay: float = 2.0,
        max_retry_delay: float = 60.0,
        max_tokens_per_text: int = 8000,
        chunk_overlap: int = 100,
        max_workers: int = 1  # –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è rate limits
    ):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–µ—Ä–≤–∏—Å–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        
        Args:
            model: –ú–æ–¥–µ–ª—å OpenAI –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é text-embedding-3-small)
            api_key: API –∫–ª—é—á OpenAI (–µ—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω, –∑–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è –∏–∑ .env)
            chunk_size: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞ –¥–ª—è –æ–¥–Ω–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞ (OpenAI –ª–∏–º–∏—Ç: –¥–æ 2048 —Ç–µ–∫—Å—Ç–æ–≤)
            delay_between_requests: –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö (—É–º–µ–Ω—å—à–µ–Ω–æ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è)
            max_retries: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ø—ã—Ç–æ–∫ –ø—Ä–∏ –æ—à–∏–±–∫–µ rate limit
            retry_delay: –ù–∞—á–∞–ª—å–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –ø—Ä–∏ retry –≤ —Å–µ–∫—É–Ω–¥–∞—Ö
            max_retry_delay: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –ø—Ä–∏ retry –≤ —Å–µ–∫—É–Ω–¥–∞—Ö
            max_tokens_per_text: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ –Ω–∞ –æ–¥–∏–Ω —Ç–µ–∫—Å—Ç (–ª–∏–º–∏—Ç OpenAI: 8192)
            chunk_overlap: –ü–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ –º–µ–∂–¥—É —á–∞–Ω–∫–∞–º–∏ –ø—Ä–∏ —Ä–∞–∑–±–∏–µ–Ω–∏–∏ –¥–ª–∏–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤ (–≤ —Ç–æ–∫–µ–Ω–∞—Ö)
            max_workers: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ (1 –¥–ª—è –±–∞–∑–æ–≤—ã—Ö –ø–ª–∞–Ω–æ–≤, 2-3 –¥–ª—è –ø–ª–∞—Ç–Ω—ã—Ö)
        """
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –æ–∫—Ä—É–∂–µ–Ω–∏–µ –ü–ï–†–ï–î —Å–æ–∑–¥–∞–Ω–∏–µ–º –∫–ª–∏–µ–Ω—Ç–∞ (–∫–∞–∫ –≤ sarsekenov_processor)
        load_env()
        self.model = model
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ rate limiting
        self.chunk_size = min(chunk_size, 2048)  # OpenAI –ª–∏–º–∏—Ç: –º–∞–∫—Å–∏–º—É–º 2048 —Ç–µ–∫—Å—Ç–æ–≤
        self.delay_between_requests = delay_between_requests
        self.max_retries = max_retries
        self.retry_delay = retry_delay
        self.max_retry_delay = max_retry_delay
        self.max_workers = max_workers  # –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–ª–∏–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤
        self.max_tokens_per_text = max_tokens_per_text
        self.chunk_overlap = chunk_overlap
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è tiktoken –¥–ª—è –ø–æ–¥—Å—á–µ—Ç–∞ —Ç–æ–∫–µ–Ω–æ–≤ (cl100k_base –¥–ª—è text-embedding-3-small)
        self.encoding = tiktoken.get_encoding("cl100k_base")
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ—á–Ω–æ —Ç–∞–∫–æ–π –∂–µ –ø–æ–¥—Ö–æ–¥, –∫–∞–∫ –≤ sarsekenov_processor.py:55
        # –ï—Å–ª–∏ —è–≤–Ω–æ –ø–µ—Ä–µ–¥–∞–Ω api_key, –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ–≥–æ, –∏–Ω–∞—á–µ OpenAI() –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –±–µ—Ä–µ—Ç –∏–∑ –æ–∫—Ä—É–∂–µ–Ω–∏—è
        if api_key:
            self.client = OpenAI(api_key=api_key)
            logger.debug("–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —è–≤–Ω–æ –ø–µ—Ä–µ–¥–∞–Ω–Ω—ã–π API –∫–ª—é—á")
        else:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∫–ª—é—á–∞ –≤ –æ–∫—Ä—É–∂–µ–Ω–∏–∏ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
            env_key = os.getenv("OPENAI_API_KEY")
            if not env_key:
                logger.warning("‚ö†Ô∏è OPENAI_API_KEY –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –æ–∫—Ä—É–∂–µ–Ω–∏–∏, –Ω–æ –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º (OpenAI –∫–ª–∏–µ–Ω—Ç –ø–æ–ø—ã—Ç–∞–µ—Ç—Å—è –Ω–∞–π—Ç–∏ –µ–≥–æ —Å–∞–º)")
            else:
                logger.debug(f"‚úÖ OPENAI_API_KEY –Ω–∞–π–¥–µ–Ω –≤ –æ–∫—Ä—É–∂–µ–Ω–∏–∏ (–¥–ª–∏–Ω–∞: {len(env_key)} —Å–∏–º–≤–æ–ª–æ–≤)")
            # –¢–æ—á–Ω–æ —Ç–∞–∫ –∂–µ, –∫–∞–∫ –≤ sarsekenov_processor.py —Å—Ç—Ä–æ–∫–∞ 55
            self.client = OpenAI()  # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç OPENAI_API_KEY –∏–∑ –æ–∫—Ä—É–∂–µ–Ω–∏—è
        
        self.embedding_dim = 1536  # –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –¥–ª—è text-embedding-3-small
        
        logger.info(f"‚úÖ EmbeddingService –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω —Å –º–æ–¥–µ–ª—å—é: {model}, chunk_size: {self.chunk_size}, delay: {delay_between_requests}s, max_tokens: {max_tokens_per_text}, max_workers: {max_workers}")
    
    def _count_tokens(self, text: str) -> int:
        """
        –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ –≤ —Ç–µ–∫—Å—Ç–µ
        
        Args:
            text: –¢–µ–∫—Å—Ç –¥–ª—è –ø–æ–¥—Å—á–µ—Ç–∞
            
        Returns:
            –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤
        """
        return len(self.encoding.encode(text))
    
    def _truncate_or_chunk_text(self, text: str) -> List[str]:
        """
        –†–∞–∑–±–∏–≤–∞–µ—Ç –¥–ª–∏–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –Ω–∞ —á–∞–Ω–∫–∏ –ø–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º, –µ—Å–ª–∏ –ø—Ä–µ–≤—ã—à–∞–µ—Ç –ª–∏–º–∏—Ç —Ç–æ–∫–µ–Ω–æ–≤
        
        Args:
            text: –¢–µ–∫—Å—Ç –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
            
        Returns:
            –°–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —á–∞–Ω–∫–æ–≤ (–µ—Å–ª–∏ —Ç–µ–∫—Å—Ç –∫–æ—Ä–æ—Ç–∫–∏–π - —Å–ø–∏—Å–æ–∫ —Å –æ–¥–Ω–∏–º —ç–ª–µ–º–µ–Ω—Ç–æ–º)
        """
        token_count = self._count_tokens(text)
        
        # –ï—Å–ª–∏ —Ç–µ–∫—Å—Ç –≤ –ø—Ä–µ–¥–µ–ª–∞—Ö –ª–∏–º–∏—Ç–∞ - –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∫–∞–∫ –µ—Å—Ç—å
        if token_count <= self.max_tokens_per_text:
            return [text]
        
        logger.warning(f"‚ö†Ô∏è –¢–µ–∫—Å—Ç –ø—Ä–µ–≤—ã—à–∞–µ—Ç –ª–∏–º–∏—Ç —Ç–æ–∫–µ–Ω–æ–≤ ({token_count} > {self.max_tokens_per_text}). –†–∞–∑–±–∏–≤–∞—é –Ω–∞ —á–∞–Ω–∫–∏...")
        
        # –†–∞–∑–±–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è (—É—á–∏—Ç—ã–≤–∞–µ–º —Ç–æ—á–∫–∏, –≤–æ—Å–∫–ª–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ –∏ –≤–æ–ø—Ä–æ—Å–∏—Ç–µ–ª—å–Ω—ã–µ –∑–Ω–∞–∫–∏)
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –±–æ–ª–µ–µ –Ω–∞–¥–µ–∂–Ω–æ–µ —Ä–µ–≥—É–ª—è—Ä–Ω–æ–µ –≤—ã—Ä–∞–∂–µ–Ω–∏–µ
        sentence_endings = re.compile(r'([.!?]+\s+|\.\n+|\.$|\n\n)')
        parts = sentence_endings.split(text)
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–∏ —Å –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º–∏
        sentences = []
        for i in range(0, len(parts), 2):
            if i < len(parts):
                sentence = parts[i]
                if i + 1 < len(parts):
                    sentence += parts[i + 1]
                if sentence.strip():
                    sentences.append(sentence.strip())
        
        if not sentences:
            # –ï—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å —Ä–∞–∑–±–∏—Ç—å –ø–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º, —Ä–∞–∑–±–∏–≤–∞–µ–º –ø–æ –∞–±–∑–∞—Ü–∞–º
            sentences = [s.strip() for s in text.split('\n\n') if s.strip()]
        
        if not sentences:
            # –ï—Å–ª–∏ –∏ —ç—Ç–æ –Ω–µ –ø–æ–º–æ–≥–ª–æ, —Ä–∞–∑–±–∏–≤–∞–µ–º –ø–æ —Å—Ç—Ä–æ–∫–∞–º
            sentences = [s.strip() for s in text.split('\n') if s.strip()]
        
        if not sentences:
            # –ü–æ—Å–ª–µ–¥–Ω–∏–π –≤–∞—Ä–∏–∞–Ω—Ç - —Ä–∞–∑–±–∏–≤–∞–µ–º –ø–æ —Å–ª–æ–≤–∞–º
            words = text.split()
            sentences = [' '.join(words[i:i+100]) for i in range(0, len(words), 100)]
        
        chunks = []
        current_chunk = ""
        current_tokens = 0
        
        for sentence in sentences:
            sentence_tokens = self._count_tokens(sentence)
            
            # –ï—Å–ª–∏ –æ–¥–Ω–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –ø—Ä–µ–≤—ã—à–∞–µ—Ç –ª–∏–º–∏—Ç - –æ–±—Ä–µ–∑–∞–µ–º –µ–≥–æ
            if sentence_tokens > self.max_tokens_per_text:
                # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —Å–ª–æ–≤–∞ –∏ —Å–æ–∑–¥–∞–µ–º —á–∞–Ω–∫–∏
                words = sentence.split()
                word_chunk = ""
                for word in words:
                    word_with_space = word + " "
                    word_tokens = self._count_tokens(word_with_space)
                    
                    if current_tokens + word_tokens > self.max_tokens_per_text:
                        if current_chunk:
                            chunks.append(current_chunk.strip())
                            # –ù–∞—á–∏–Ω–∞–µ–º –Ω–æ–≤—ã–π —á–∞–Ω–∫ —Å –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ–º (–ø–æ—Å–ª–µ–¥–Ω–∏–µ —Å–ª–æ–≤–∞)
                            overlap_count = min(self.chunk_overlap // 10, len(current_chunk.split()))
                            overlap_words = current_chunk.split()[-overlap_count:] if overlap_count > 0 else []
                            current_chunk = " ".join(overlap_words) + " " + word
                            current_tokens = self._count_tokens(current_chunk)
                        else:
                            # –î–∞–∂–µ –æ–¥–Ω–æ —Å–ª–æ–≤–æ —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω–æ–µ - –æ–±—Ä–µ–∑–∞–µ–º –ø–æ —Å–∏–º–≤–æ–ª–∞–º
                            max_chars = self.max_tokens_per_text * 4  # –ü—Ä–∏–º–µ—Ä–Ω–æ 4 —Å–∏–º–≤–æ–ª–∞ –Ω–∞ —Ç–æ–∫–µ–Ω
                            current_chunk = word[:max_chars]
                            current_tokens = self._count_tokens(current_chunk)
                    else:
                        current_chunk += word_with_space
                        current_tokens += word_tokens
                continue
            
            # –ï—Å–ª–∏ –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –ø—Ä–µ–≤—ã—Å–∏—Ç –ª–∏–º–∏—Ç - —Å–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–µ–∫—É—â–∏–π —á–∞–Ω–∫
            if current_tokens + sentence_tokens > self.max_tokens_per_text:
                if current_chunk:
                    chunks.append(current_chunk.strip())
                    # –ù–∞—á–∏–Ω–∞–µ–º –Ω–æ–≤—ã–π —á–∞–Ω–∫ —Å –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ–º (–ø–æ—Å–ª–µ–¥–Ω–∏–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è)
                    # –ë–µ—Ä–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –¥–ª—è –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏—è
                    prev_sentences = current_chunk.split('.')
                    overlap_sentences = prev_sentences[-2:] if len(prev_sentences) >= 2 else [current_chunk]
                    overlap_text = '. '.join([s.strip() for s in overlap_sentences if s.strip()])
                    current_chunk = overlap_text + '. ' + sentence if overlap_text else sentence
                    current_tokens = self._count_tokens(current_chunk)
                else:
                    current_chunk = sentence
                    current_tokens = sentence_tokens
            else:
                # –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –∫ —Ç–µ–∫—É—â–µ–º—É —á–∞–Ω–∫—É
                separator = " " if current_chunk and not current_chunk.endswith(('.', '!', '?')) else ""
                current_chunk += separator + sentence
                current_tokens += sentence_tokens
        
        # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —á–∞–Ω–∫
        if current_chunk.strip():
            chunks.append(current_chunk.strip())
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –≤—Å–µ —á–∞–Ω–∫–∏ –≤ –ø—Ä–µ–¥–µ–ª–∞—Ö –ª–∏–º–∏—Ç–∞
        valid_chunks = []
        for chunk in chunks:
            chunk_tokens = self._count_tokens(chunk)
            if chunk_tokens > self.max_tokens_per_text:
                logger.warning(f"‚ö†Ô∏è –ß–∞–Ω–∫ –≤—Å–µ –µ—â–µ –ø—Ä–µ–≤—ã—à–∞–µ—Ç –ª–∏–º–∏—Ç ({chunk_tokens} —Ç–æ–∫–µ–Ω–æ–≤), –æ–±—Ä–µ–∑–∞—é...")
                # –û–±—Ä–µ–∑–∞–µ–º –ø–æ —Ç–æ–∫–µ–Ω–∞–º
                tokens = self.encoding.encode(chunk)
                truncated_tokens = tokens[:self.max_tokens_per_text]
                chunk = self.encoding.decode(truncated_tokens)
            valid_chunks.append(chunk)
        
        logger.info(f"üìù –¢–µ–∫—Å—Ç —Ä–∞–∑–±–∏—Ç –Ω–∞ {len(valid_chunks)} —á–∞–Ω–∫–æ–≤ (—Ç–æ–∫–µ–Ω—ã: {[self._count_tokens(c) for c in valid_chunks]})")
        return valid_chunks
    
    def _average_embeddings(self, embeddings: List[List[float]]) -> List[float]:
        """
        –£—Å—Ä–µ–¥–Ω—è–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –≤ –æ–¥–∏–Ω –≤–µ–∫—Ç–æ—Ä
        
        Args:
            embeddings: –°–ø–∏—Å–æ–∫ –≤–µ–∫—Ç–æ—Ä–æ–≤ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
            
        Returns:
            –£—Å—Ä–µ–¥–Ω–µ–Ω–Ω—ã–π –≤–µ–∫—Ç–æ—Ä
        """
        if not embeddings:
            return [0.0] * self.embedding_dim
        
        if len(embeddings) == 1:
            return embeddings[0]
        
        # –£—Å—Ä–µ–¥–Ω—è–µ–º –ø–æ –∫–∞–∂–¥–æ–º—É –∏–∑–º–µ—Ä–µ–Ω–∏—é
        averaged = []
        for i in range(self.embedding_dim):
            avg_value = sum(emb[i] for emb in embeddings) / len(embeddings)
            averaged.append(avg_value)
        
        return averaged
    
    def _make_request_with_retry(self, texts: List[str], attempt: int = 0) -> List[List[float]]:
        """
        –í—ã–ø–æ–ª–Ω—è–µ—Ç –∑–∞–ø—Ä–æ—Å –∫ API —Å retry –ª–æ–≥–∏–∫–æ–π
        
        Args:
            texts: –°–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤ –¥–ª—è –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏
            attempt: –ù–æ–º–µ—Ä —Ç–µ–∫—É—â–µ–π –ø–æ–ø—ã—Ç–∫–∏
            
        Returns:
            –°–ø–∏—Å–æ–∫ –≤–µ–∫—Ç–æ—Ä–æ–≤ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        """
        try:
            response = self.client.embeddings.create(
                model=self.model,
                input=texts
            )
            return [item.embedding for item in response.data]
        except Exception as e:
            error_msg = str(e)
            
            # –û–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ–π –∫–≤–æ—Ç—ã (insufficient_quota) - –ø—Ä–æ–≤–µ—Ä—è–µ–º –ü–ï–†–í–´–ú
            # —Ç–∞–∫ –∫–∞–∫ —ç—Ç–æ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞, –∫–æ—Ç–æ—Ä—É—é –Ω–µ–ª—å–∑—è —Ä–µ—à–∏—Ç—å retry
            if "insufficient_quota" in error_msg.lower() or "quota" in error_msg.lower():
                logger.error(f"‚ùå –ü—Ä–µ–≤—ã—à–µ–Ω–∞ –∫–≤–æ—Ç–∞ OpenAI API. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –±–∏–ª–ª–∏–Ω–≥: https://platform.openai.com/account/billing")
                logger.error(f"   –î–µ—Ç–∞–ª–∏ –æ—à–∏–±–∫–∏: {error_msg}")
                raise RuntimeError("–ü—Ä–µ–≤—ã—à–µ–Ω–∞ –∫–≤–æ—Ç–∞ OpenAI API. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –±–∏–ª–ª–∏–Ω–≥ –∏ –ø–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ.") from e
            
            # –û–±—Ä–∞–±–æ—Ç–∫–∞ rate limit (429) - –ø–æ–≤—Ç–æ—Ä—è–µ–º —Å –∑–∞–¥–µ—Ä–∂–∫–æ–π
            elif "429" in error_msg or "rate_limit" in error_msg.lower():
                if attempt < self.max_retries:
                    delay = min(self.retry_delay * (2 ** attempt), self.max_retry_delay)
                    logger.warning(f"‚ö†Ô∏è Rate limit –¥–æ—Å—Ç–∏–≥–Ω—É—Ç. –ü–æ–ø—ã—Ç–∫–∞ {attempt + 1}/{self.max_retries}. –û–∂–∏–¥–∞–Ω–∏–µ {delay:.1f}s...")
                    time.sleep(delay)
                    return self._make_request_with_retry(texts, attempt + 1)
                else:
                    logger.error(f"‚ùå –ü—Ä–µ–≤—ã—à–µ–Ω–æ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ø—ã—Ç–æ–∫ ({self.max_retries}). Rate limit –Ω–µ —Å–Ω—è—Ç.")
                    raise RuntimeError(f"Rate limit –Ω–µ —Å–Ω—è—Ç –ø–æ—Å–ª–µ {self.max_retries} –ø–æ–ø—ã—Ç–æ–∫. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ.") from e
            
            # –û–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–µ–≤–µ—Ä–Ω–æ–≥–æ –∫–ª—é—á–∞ (401)
            elif "401" in error_msg or "invalid_api_key" in error_msg.lower():
                logger.error(f"‚ùå –ù–µ–≤–µ—Ä–Ω—ã–π API –∫–ª—é—á OpenAI. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ OPENAI_API_KEY –≤ .env")
                raise ValueError("–ù–µ–≤–µ—Ä–Ω—ã–π API –∫–ª—é—á OpenAI") from e
            
            # –î—Ä—É–≥–∏–µ –æ—à–∏–±–∫–∏
            else:
                logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {e}")
                raise
    
    def create_embedding(self, text: str) -> List[float]:
        """
        –°–æ–∑–¥–∞–µ—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è –æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º —Ä–∞–∑–±–∏–µ–Ω–∏–µ–º –¥–ª–∏–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤
        
        Args:
            text: –¢–µ–∫—Å—Ç –¥–ª—è –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏
            
        Returns:
            –°–ø–∏—Å–æ–∫ —á–∏—Å–µ–ª (–≤–µ–∫—Ç–æ—Ä —ç–º–±–µ–¥–¥–∏–Ω–≥–∞)
        """
        if not text or not text.strip():
            logger.warning("–ü—É—Å—Ç–æ–π —Ç–µ–∫—Å—Ç –ø–µ—Ä–µ–¥–∞–Ω –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–∞")
            return [0.0] * self.embedding_dim
        
        text = text.strip()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–ª–∏–Ω—É –∏ —Ä–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —á–∞–Ω–∫–∏ –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏
        text_chunks = self._truncate_or_chunk_text(text)
        
        # –ï—Å–ª–∏ —Ç–µ–∫—Å—Ç —Ä–∞–∑–±–∏—Ç –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–æ —á–∞–Ω–∫–æ–≤ - —Å–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∏ —É—Å—Ä–µ–¥–Ω—è–µ–º
        if len(text_chunks) > 1:
            chunk_embeddings = []
            for i, chunk in enumerate(text_chunks):
                logger.debug(f"  –°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –¥–ª—è —á–∞–Ω–∫–∞ {i+1}/{len(text_chunks)} ({self._count_tokens(chunk)} —Ç–æ–∫–µ–Ω–æ–≤)")
                chunk_emb = self._make_request_with_retry([chunk])
                chunk_embeddings.append(chunk_emb[0])
                # –ù–µ–±–æ–ª—å—à–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É —á–∞–Ω–∫–∞–º–∏ –æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
                if i < len(text_chunks) - 1:
                    time.sleep(0.1)
            
            # –£—Å—Ä–µ–¥–Ω—è–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –≤—Å–µ—Ö —á–∞–Ω–∫–æ–≤
            averaged_embedding = self._average_embeddings(chunk_embeddings)
            logger.debug(f"‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥ —Å–æ–∑–¥–∞–Ω –∏–∑ {len(text_chunks)} —á–∞–Ω–∫–æ–≤ (—É—Å—Ä–µ–¥–Ω–µ–Ω)")
            return averaged_embedding
        else:
            # –û–±—ã—á–Ω—ã–π —Å–ª—É—á–∞–π - –æ–¥–∏–Ω —á–∞–Ω–∫
            embeddings = self._make_request_with_retry([text_chunks[0]])
            return embeddings[0]
    
    def create_embeddings_batch(self, texts: List[str]) -> List[List[float]]:
        """
        –°–æ–∑–¥–∞–µ—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è —Å–ø–∏—Å–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤ (—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –±–∞—Ç—á-–æ–±—Ä–∞–±–æ—Ç–∫–∞)
        
        –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥:
        1. –ì—Ä—É–ø–ø–∏—Ä—É–µ—Ç –∫–æ—Ä–æ—Ç–∫–∏–µ —Ç–µ–∫—Å—Ç—ã –≤ –±–∞—Ç—á–∏ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
        2. –î–ª–∏–Ω–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã —Ä–∞–∑–±–∏–≤–∞–µ—Ç –Ω–∞ —á–∞–Ω–∫–∏ –∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –æ—Ç–¥–µ–ª—å–Ω–æ
        3. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –±–∞—Ç—á-–∑–∞–ø—Ä–æ—Å—ã –∫ API –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è
        
        Args:
            texts: –°–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤ –¥–ª—è –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏
            
        Returns:
            –°–ø–∏—Å–æ–∫ –≤–µ–∫—Ç–æ—Ä–æ–≤ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        """
        if not texts:
            return []
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º –ø—É—Å—Ç—ã–µ —Ç–µ–∫—Å—Ç—ã –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –∏–Ω–¥–µ–∫—Å—ã
        valid_texts = []
        valid_indices = []
        for i, text in enumerate(texts):
            if text and text.strip():
                valid_texts.append(text.strip())
                valid_indices.append(i)
        
        if not valid_texts:
            logger.warning("–í—Å–µ —Ç–µ–∫—Å—Ç—ã –ø—É—Å—Ç—ã–µ")
            return [[0.0] * self.embedding_dim] * len(texts)
        
        logger.info(f"üì¶ –û–±—Ä–∞–±–æ—Ç–∫–∞ {len(valid_texts)} —Ç–µ–∫—Å—Ç–æ–≤ (–±–∞—Ç—á-—Ä–µ–∂–∏–º, —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞: {self.chunk_size})...")
        
        # –†–∞–∑–¥–µ–ª—è–µ–º —Ç–µ–∫—Å—Ç—ã –Ω–∞ –∫–æ—Ä–æ—Ç–∫–∏–µ (–¥–ª—è –±–∞—Ç—á-–æ–±—Ä–∞–±–æ—Ç–∫–∏) –∏ –¥–ª–∏–Ω–Ω—ã–µ (–¥–ª—è —Ä–∞–∑–±–∏–µ–Ω–∏—è)
        short_texts = []  # –¢–µ–∫—Å—Ç—ã, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–∂–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –±–∞—Ç—á–∞–º–∏
        short_indices = []  # –ò–Ω–¥–µ–∫—Å—ã –∫–æ—Ä–æ—Ç–∫–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤
        long_texts = []  # –¢–µ–∫—Å—Ç—ã, –∫–æ—Ç–æ—Ä—ã–µ –Ω—É–∂–Ω–æ —Ä–∞–∑–±–∏—Ç—å
        long_indices = []  # –ò–Ω–¥–µ–∫—Å—ã –¥–ª–∏–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤
        
        for i, text in enumerate(valid_texts):
            token_count = self._count_tokens(text)
            if token_count > self.max_tokens_per_text:
                long_texts.append(text)
                long_indices.append(i)
            else:
                short_texts.append(text)
                short_indices.append(i)
        
        all_embeddings = [None] * len(valid_texts)
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–æ—Ä–æ—Ç–∫–∏–µ —Ç–µ–∫—Å—Ç—ã –±–∞—Ç—á–∞–º–∏ —Å –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º–æ–º (–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–æ!)
        if short_texts:
            logger.info(f"  üìä –ö–æ—Ä–æ—Ç–∫–∏–µ —Ç–µ–∫—Å—Ç—ã ({len(short_texts)}): –æ–±—Ä–∞–±–æ—Ç–∫–∞ –±–∞—Ç—á–∞–º–∏ (—Ä–∞–∑–º–µ—Ä: {self.chunk_size}, –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ: {self.max_workers})...")
            
            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –±–∞—Ç—á–µ–π
            batches = []
            batch_indices = []
            for batch_idx in range(0, len(short_texts), self.chunk_size):
                batch = short_texts[batch_idx:batch_idx + self.chunk_size]
                batch_start_idx = batch_idx
                batches.append(batch)
                batch_indices.append(batch_start_idx)
            
            total_batches = len(batches)
            
            # –§—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ–¥–Ω–æ–≥–æ –±–∞—Ç—á–∞
            def process_batch(batch_data):
                batch_idx, batch = batch_data
                batch_num = batch_idx + 1
                try:
                    logger.debug(f"    –ë–∞—Ç—á {batch_num}/{total_batches}: {len(batch)} —Ç–µ–∫—Å—Ç–æ–≤ (–æ–±—Ä–∞–±–æ—Ç–∫–∞...)")
                    batch_embeddings = self._make_request_with_retry(batch)
                    logger.debug(f"    ‚úÖ –ë–∞—Ç—á {batch_num}/{total_batches} –æ–±—Ä–∞–±–æ—Ç–∞–Ω")
                    return batch_idx, batch_embeddings, None
                except Exception as e:
                    logger.error(f"    ‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –±–∞—Ç—á–∞ {batch_num}/{total_batches}: {e}")
                    return batch_idx, None, e
            
            # –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –±–∞—Ç—á–µ–π
            if self.max_workers > 1 and total_batches > 1:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º ThreadPoolExecutor –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
                with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
                    # –ó–∞–ø—É—Å–∫–∞–µ–º –≤—Å–µ –±–∞—Ç—á–∏ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
                    future_to_batch = {
                        executor.submit(process_batch, (idx, batch)): idx 
                        for idx, batch in enumerate(batches)
                    }
                    
                    # –°–æ–±–∏—Ä–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ –º–µ—Ä–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è
                    completed = 0
                    for future in as_completed(future_to_batch):
                        batch_idx, batch_embeddings, error = future.result()
                        completed += 1
                        
                        if batch_embeddings:
                            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
                            start_idx = batch_indices[batch_idx]
                            for j, embedding in enumerate(batch_embeddings):
                                original_idx = short_indices[start_idx + j]
                                all_embeddings[original_idx] = embedding
                        else:
                            # –í —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏ –ù–ï –¥–æ–±–∞–≤–ª—è–µ–º –Ω—É–ª–µ–≤—ã–µ –≤–µ–∫—Ç–æ—Ä—ã - —ç—Ç–æ –ø—Ä–∏–≤–µ–¥–µ—Ç –∫ –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏
                            # –û—Å—Ç–∞–≤–ª—è–µ–º None, —á—Ç–æ–±—ã –∏–Ω–¥–µ–∫—Å–∞—Ç–æ—Ä –ø–æ–Ω—è–ª —á—Ç–æ –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞
                            start_idx = batch_indices[batch_idx]
                            batch = batches[batch_idx]
                            for j in range(len(batch)):
                                original_idx = short_indices[start_idx + j]
                                all_embeddings[original_idx] = None  # None –≤–º–µ—Å—Ç–æ –Ω—É–ª–µ–≤—ã—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤
                        
                        # –ù–µ–±–æ–ª—å—à–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –¥–ª—è rate limiting (—Ç–æ–ª—å–∫–æ –º–µ–∂–¥—É –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è–º–∏)
                        if completed < total_batches:
                            time.sleep(self.delay_between_requests)
            else:
                # –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ (–µ—Å–ª–∏ max_workers=1 –∏–ª–∏ –æ–¥–∏–Ω –±–∞—Ç—á)
                for batch_idx, batch in enumerate(batches):
                    batch_num = batch_idx + 1
                    logger.debug(f"    –ë–∞—Ç—á {batch_num}/{total_batches}: {len(batch)} —Ç–µ–∫—Å—Ç–æ–≤")
                    
                    # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –±–∞—Ç—á–∞–º–∏ (–∫—Ä–æ–º–µ –ø–µ—Ä–≤–æ–≥–æ)
                    if batch_idx > 0:
                        time.sleep(self.delay_between_requests)
                    
                    try:
                        batch_embeddings = self._make_request_with_retry(batch)
                        start_idx = batch_indices[batch_idx]
                        for j, embedding in enumerate(batch_embeddings):
                            original_idx = short_indices[start_idx + j]
                            all_embeddings[original_idx] = embedding
                        logger.debug(f"    ‚úÖ –ë–∞—Ç—á {batch_num}/{total_batches} –æ–±—Ä–∞–±–æ—Ç–∞–Ω")
                    except Exception as e:
                        logger.error(f"    ‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –±–∞—Ç—á–∞ {batch_num}/{total_batches}: {e}")
                        # –ü—Ä–∏ –æ—à–∏–±–∫–µ –ù–ï –¥–æ–±–∞–≤–ª—è–µ–º –Ω—É–ª–µ–≤—ã–µ –≤–µ–∫—Ç–æ—Ä—ã - —ç—Ç–æ –ø—Ä–∏–≤–µ–¥–µ—Ç –∫ –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏
                        # –í—ã–±—Ä–∞—Å—ã–≤–∞–µ–º –∏—Å–∫–ª—é—á–µ–Ω–∏–µ, —á—Ç–æ–±—ã –∏–Ω–¥–µ–∫—Å–∞—Ç–æ—Ä –ø–æ–Ω—è–ª —á—Ç–æ –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞
                        raise RuntimeError(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è –±–∞—Ç—á–∞ {batch_num}/{total_batches}: {e}") from e
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –¥–ª–∏–Ω–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã –æ—Ç–¥–µ–ª—å–Ω–æ (—Å —Ä–∞–∑–±–∏–µ–Ω–∏–µ–º –Ω–∞ —á–∞–Ω–∫–∏)
        if long_texts:
            logger.info(f"  üìè –î–ª–∏–Ω–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã ({len(long_texts)}): —Ä–∞–∑–±–∏–µ–Ω–∏–µ –Ω–∞ —á–∞–Ω–∫–∏...")
            for i, text in enumerate(long_texts):
                original_idx = long_indices[i]
                token_count = self._count_tokens(text)
                logger.debug(f"    –¢–µ–∫—Å—Ç {i+1}/{len(long_texts)}: {token_count} —Ç–æ–∫–µ–Ω–æ–≤ (–±—É–¥–µ—Ç —Ä–∞–∑–±–∏—Ç)")
                
                # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
                if i > 0 or short_texts:  # –ó–∞–¥–µ—Ä–∂–∫–∞ –µ—Å–ª–∏ –Ω–µ –ø–µ—Ä–≤—ã–π –∏–ª–∏ –±—ã–ª–∏ –∫–æ—Ä–æ—Ç–∫–∏–µ —Ç–µ–∫—Å—Ç—ã
                    time.sleep(self.delay_between_requests)
                
                try:
                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º create_embedding, –∫–æ—Ç–æ—Ä—ã–π —Å–∞–º —Ä–∞–∑–æ–±—å–µ—Ç —Ç–µ–∫—Å—Ç
                    embedding = self.create_embedding(text)
                    all_embeddings[original_idx] = embedding
                except Exception as e:
                    logger.error(f"    ‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –¥–ª–∏–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ {i+1}/{len(long_texts)}: {e}")
                    # –ü—Ä–∏ –æ—à–∏–±–∫–µ –≤—ã–±—Ä–∞—Å—ã–≤–∞–µ–º –∏—Å–∫–ª—é—á–µ–Ω–∏–µ –≤–º–µ—Å—Ç–æ –Ω—É–ª–µ–≤—ã—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤
                    raise RuntimeError(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è –¥–ª–∏–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ {i+1}/{len(long_texts)}: {e}") from e
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –≤—Å–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ —Å–æ–∑–¥–∞–Ω—ã —É—Å–ø–µ—à–Ω–æ
        failed_count = sum(1 for emb in all_embeddings if emb is None)
        if failed_count > 0:
            raise RuntimeError(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å {failed_count} –∏–∑ {len(valid_texts)} —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ª–æ–≥–∏ –¥–ª—è –¥–µ—Ç–∞–ª–µ–π.")
        
        # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –ø–æ—Ä—è–¥–æ–∫ —Å —É—á–µ—Ç–æ–º –ø—É—Å—Ç—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤
        result = []
        embedding_idx = 0
        for i, original_text in enumerate(texts):
            if i in valid_indices:
                result.append(all_embeddings[embedding_idx])
                embedding_idx += 1
            else:
                result.append([0.0] * self.embedding_dim)  # –ü—É—Å—Ç—ã–µ —Ç–µ–∫—Å—Ç—ã –ø–æ–ª—É—á–∞—é—Ç –Ω—É–ª–µ–≤—ã–µ –≤–µ–∫—Ç–æ—Ä—ã
        
        processed_count = sum(1 for emb in all_embeddings if emb is not None)
        logger.info(f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {processed_count}/{len(valid_texts)} —Ç–µ–∫—Å—Ç–æ–≤ (–±–∞—Ç—á–∞–º–∏: {len(short_texts)}, –æ—Ç–¥–µ–ª—å–Ω–æ: {len(long_texts)})")
        return result
    
    @property
    def dimension(self) -> int:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤"""
        return self.embedding_dim

