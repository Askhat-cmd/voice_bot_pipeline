#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Voice Bot Pipeline Orchestrator (SAG v2.0 Optimized)
Coordinates the pipeline: YouTube Subtitles -> SAG v2.0 Final Data
"""

import argparse
import json
import logging
import os
import sys
import time
from pathlib import Path
from typing import Dict, List, Any

import yaml

from env_utils import load_env
from subtitle_extractor.get_subtitles import YouTubeSubtitlesExtractor
from text_processor.sarsekenov_processor import SarsekenovProcessor
from vector_db import VectorDBManager, EmbeddingService, VectorIndexer

class PipelineOrchestrator:
    def __init__(self, config_path: str, domain: str = "sarsekenov"):
        """Initialize the pipeline orchestrator for SAG v2.0"""
        # 1) env
        load_env()
        # 2) config
        with open(config_path, 'r', encoding='utf-8') as f:
            self.config = yaml.safe_load(f)
        # 3) logging
        self._setup_logging()
        self.logger = logging.getLogger("pipeline")
        self._setup_dirs()
        # 4) stages - —Ç–æ–ª—å–∫–æ SAG v2.0
        self.subtitle_extractor = YouTubeSubtitlesExtractor(
            output_dir=self.config['pipeline']['subtitles']['output_dir']
        )
        # –¢–æ–ª—å–∫–æ SarsekenovProcessor –¥–ª—è SAG v2.0
        self.text_processor = SarsekenovProcessor()
        
        # 5) Vector DB (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ, –µ—Å–ª–∏ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–æ)
        self.vector_indexer = None
        if self.config.get('vector_db', {}).get('auto_index', False):
            try:
                db_manager = VectorDBManager(
                    db_path=self.config['vector_db']['db_path'],
                    collection_prefix=self.config['vector_db']['collection_prefix']
                )
                
                # –ú–æ–¥–µ–ª—å: —Å–Ω–∞—á–∞–ª–∞ –∏–∑ env, –ø–æ—Ç–æ–º –∏–∑ config
                embedding_model = os.getenv("SENTENCE_TRANSFORMERS_MODEL") or self.config['vector_db']['embedding'].get('model')
                embedding_service = EmbeddingService(model=embedding_model)
                self.vector_indexer = VectorIndexer(
                    db_manager=db_manager,
                    embedding_service=embedding_service,
                    batch_size=self.config['vector_db'].get('batch_size', 100)
                )
                self.logger.info("‚úÖ Vector DB –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è –∞–∫—Ç–∏–≤–∏—Ä–æ–≤–∞–Ω–∞ —Å rate limiting")
            except Exception as e:
                self.logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å Vector DB: {e}")
                self.vector_indexer = None
    
    def _setup_logging(self) -> None:
        log_cfg = self.config['logging']
        Path(Path(log_cfg['log_file']).parent).mkdir(parents=True, exist_ok=True)
        logging.basicConfig(
            level=getattr(logging, log_cfg['level'], logging.INFO),
            format="%(asctime)s | %(levelname)s | %(name)s | %(message)s",
            handlers=[
                logging.FileHandler(log_cfg['log_file'], encoding='utf-8'),
                logging.StreamHandler()
            ],
        )

    def _setup_dirs(self) -> None:
        for d in [
            self.config['pipeline']['subtitles']['output_dir'],
            self.config['pipeline']['text_processing']['output_dir'],
        ]:
            Path(d).mkdir(parents=True, exist_ok=True)
    
    def run_full_pipeline(self, youtube_url: str, custom_name: str = None) -> Dict[str, Any]:
        """Run the complete SAG v2.0 pipeline for a single YouTube URL"""
        pipeline_start = time.time()
        results = {
            "youtube_url": youtube_url,
            "custom_name": custom_name,
            "pipeline_start": time.strftime("%Y-%m-%d %H:%M:%S"),
            "schema_version": "2.0",
            "processing_version": "v2.1",
            "stages": {}
        }
        
        self.logger.info(f"üöÄ Starting SAG v2.0 pipeline for: {youtube_url}")
        
        try:
            # Stage 1: Get Subtitles (–≤—Ä–µ–º–µ–Ω–Ω–∞—è –ø–∞–ø–∫–∞)
            self.logger.info("üì• Stage 1: Downloading subtitles from YouTube")
            stage1_start = time.time()
            
            video_id = self.subtitle_extractor.extract_video_id(youtube_url)
            if not video_id:
                raise ValueError(f"Could not extract video_id from URL: {youtube_url}")

            subtitles = self.subtitle_extractor.get_subtitles(video_id)
            if not subtitles:
                raise RuntimeError(f"Failed to retrieve subtitles for video_id: {video_id}")

            filename_id = custom_name if custom_name else video_id
            saved_files = self.subtitle_extractor.save_subtitles(filename_id, subtitles)

            results["stages"]["subtitles"] = {
                "status": "success",
                "duration": time.time() - stage1_start,
                "json_path": saved_files["json"],
                "srt_path": saved_files["srt"],
                "txt_path": saved_files["txt"]
            }
            self.logger.info(f"Stage 1 complete: {saved_files['json']}")
            
            # Stage 2: Text Processing (SAG v2.0)
            self.logger.info("üìù Stage 2: Processing text for SAG v2.0")
            stage2_start = time.time()
            
            transcript_path = Path(saved_files["json"])
            output_dir = Path(self.config['pipeline']['text_processing']['output_dir'])
            
            # –ü—Ä—è–º–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –≤ SAG v2.0
            text_result = self.text_processor.process_subtitles_file(
                transcript_path, output_dir
            )
            
            results["stages"]["text_processing"] = {
                "status": "success", 
                "duration": time.time() - stage2_start,
                "sag_v2_output": text_result["json_output"],
                "review_markdown": text_result["md_output"],
                "blocks_created": text_result["blocks_created"],
                "schema_version": "2.0"
            }
            self.logger.info(f"‚úÖ Stage 2 complete: {text_result['blocks_created']} SAG v2.0 blocks created")
            
            # Stage 3: Pipeline –∑–∞–≤–µ—Ä—à–µ–Ω
            self.logger.info("‚úÖ Stage 3: Pipeline complete")
            results["stages"]["pipeline_complete"] = {
                "status": "success",
                "duration": 0.0
            }
            
            # Pipeline Summary
            total_duration = time.time() - pipeline_start
            results.update({
                "status": "success",
                "total_duration": total_duration,
                "pipeline_end": time.strftime("%Y-%m-%d %H:%M:%S"),
                "final_outputs": {
                    "sag_v2_json": text_result["json_output"],
                    "review_markdown": text_result["md_output"]
                }
            })
            
            self.logger.info(f"üéØ SAG v2.0 Pipeline complete! Total time: {total_duration:.1f}s")
            self.logger.info(f"üìÅ SAG v2.0 JSON: {text_result['json_output']}")
            self.logger.info(f"üìñ Review Markdown: {text_result['md_output']}")
            
            # Stage 4: Vector DB Indexing (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
            if self.vector_indexer:
                self.logger.info("üîç Stage 4: Indexing to Vector DB")
                stage4_start = time.time()
                try:
                    index_levels = self.config['vector_db'].get('index_levels', ['documents', 'blocks', 'graph_entities'])
                    index_result = self.vector_indexer.index_sag_file(
                        Path(text_result['json_output']),
                        index_levels=index_levels
                    )
                    results["stages"]["vector_indexing"] = {
                        "status": "success" if index_result["success"] else "failed",
                        "duration": time.time() - stage4_start,
                        "indexed": index_result["indexed"]
                    }
                    if index_result["success"]:
                        self.logger.info(f"‚úÖ Vector DB –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {index_result['indexed']}")
                    else:
                        self.logger.warning(f"‚ö†Ô∏è Vector DB –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —Å –æ—à–∏–±–∫–∞–º–∏")
                except Exception as e:
                    self.logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ –≤ Vector DB: {e}", exc_info=True)
                    results["stages"]["vector_indexing"] = {
                        "status": "failed",
                        "duration": time.time() - stage4_start,
                        "error": str(e)
                    }
            
            return results
            
        except Exception as e:
            self.logger.error(f"Pipeline failed: {e}", exc_info=True)
            results.update({
                "status": "failed",
                "error": str(e),
                "total_duration": time.time() - pipeline_start
            })
            return results
    
    def run_batch_pipeline(self, urls_file: str) -> List[Dict[str, Any]]:
        """Run pipeline for multiple URLs from file"""
        with open(urls_file, 'r') as f:
            urls = [line.strip() for line in f if line.strip() and not line.startswith('#')]
        
        self.logger.info(f"üöÄ Starting SAG v2.0 batch pipeline for {len(urls)} URLs")
        results = []
        
        for i, url in enumerate(urls, 1):
            self.logger.info(f"üìù Processing URL {i}/{len(urls)}: {url}")
            result = self.run_full_pipeline(url)
            results.append(result)
            
            # Save batch results in raw_subtitles folder (–Ω–µ –≤ SAG!)
            batch_dir = Path(self.config['pipeline']['subtitles']['output_dir'])
            batch_file = batch_dir / "batch_results.json"
            with open(batch_file, "w") as f:
                json.dump(results, f, indent=2, ensure_ascii=False)
            
            # –û—á–∏—â–∞–µ–º —Å—Ç–∞—Ä—ã–µ batch —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã (–æ—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 2)
            batch_results = list(batch_dir.glob("batch_pipeline_results_*.json"))
            if len(batch_results) > 2:
                batch_results.sort(key=lambda x: x.stat().st_mtime, reverse=True)
                for old_batch in batch_results[2:]:
                    old_batch.unlink()
                    self.logger.info(f"üóëÔ∏è Old batch result removed: {old_batch.name}")
        
        return results

def main():
    parser = argparse.ArgumentParser(description="Voice Bot Pipeline Orchestrator (SAG v2.0)")
    parser.add_argument("--url", help="Single YouTube URL to process")
    parser.add_argument("--urls-file", help="File containing multiple URLs (one per line)")
    parser.add_argument("--name", help="Custom name for the output files (defaults to video_id)")
    parser.add_argument("--config", required=True, help="Configuration file")
    parser.add_argument("--domain", default="sarsekenov", help="Domain processor (always sarsekenov for SAG v2.0)")
    parser.add_argument("--index-to-vector-db", action="store_true", help="Index to vector database after processing")
    
    args = parser.parse_args()
    
    # –ï—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ —É–∫–∞–∑–∞–Ω–æ - –ø—ã—Ç–∞–µ–º—Å—è –≤–∑—è—Ç—å urls.txt —Ä—è–¥–æ–º —Å–æ —Å–∫—Ä–∏–ø—Ç–æ–º
    default_urls = Path(__file__).resolve().parent / "urls.txt"
    if not args.url and not args.urls_file and default_urls.exists():
        args.urls_file = str(default_urls)
    if not args.url and not args.urls_file:
        parser.error("Specify --url or --urls-file (or create urls.txt in project root)")
    
    orchestrator = PipelineOrchestrator(args.config, domain=args.domain)
    
    # –í–∫–ª—é—á–∞–µ–º –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—é –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω —Ñ–ª–∞–≥
    if args.index_to_vector_db:
        if orchestrator.config.get('vector_db'):
            orchestrator.config['vector_db']['auto_index'] = True
            # –ü–µ—Ä–µ–∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –≤–µ–∫—Ç–æ—Ä–Ω—ã–π –∏–Ω–¥–µ–∫—Å–∞—Ç–æ—Ä
            try:
                from vector_db import VectorDBManager, EmbeddingService, VectorIndexer
                db_manager = VectorDBManager(
                    db_path=orchestrator.config['vector_db']['db_path'],
                    collection_prefix=orchestrator.config['vector_db']['collection_prefix']
                )
                
                # –ú–æ–¥–µ–ª—å: —Å–Ω–∞—á–∞–ª–∞ –∏–∑ env, –ø–æ—Ç–æ–º –∏–∑ config
                embedding_model = os.getenv("SENTENCE_TRANSFORMERS_MODEL") or orchestrator.config['vector_db']['embedding'].get('model')
                embedding_service = EmbeddingService(model=embedding_model)
                orchestrator.vector_indexer = VectorIndexer(
                    db_manager=db_manager,
                    embedding_service=embedding_service,
                    batch_size=orchestrator.config['vector_db'].get('batch_size', 100)
                )
                orchestrator.logger.info("‚úÖ Vector DB –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è –∞–∫—Ç–∏–≤–∏—Ä–æ–≤–∞–Ω–∞ —á–µ—Ä–µ–∑ CLI —Ñ–ª–∞–≥")
            except Exception as e:
                orchestrator.logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ Vector DB: {e}")
                return 1
        else:
            orchestrator.logger.error("Vector DB –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω –≤ config.yaml")
            return 1
    
    # Check OpenAI API key (still needed for text processing)
    if not os.getenv("OPENAI_API_KEY"):
        orchestrator.logger.error("OPENAI_API_KEY environment variable not set")
        orchestrator.logger.info("Make sure your .env file contains: OPENAI_API_KEY=your-key-here")
        return 1
    
    if args.url:
        result = orchestrator.run_full_pipeline(args.url, args.name)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ raw_subtitles –ø–∞–ø–∫—É (–Ω–µ –≤ SAG!)
        results_dir = Path(orchestrator.config['pipeline']['subtitles']['output_dir'])
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        results_file = results_dir / f"pipeline_result_{timestamp}.json"
        with open(results_file, "w") as f:
            json.dump(result, f, indent=2, ensure_ascii=False)
        
        print(f"\n[SUCCESS] Pipeline results saved: {results_file}")
        
        if result["status"] == "success":
            print(f"[OUTPUT] SAG v2.0 JSON: {result['final_outputs']['sag_v2_json']}")
            print(f"[OUTPUT] Review MD: {result['final_outputs']['review_markdown']}")
            return 0
        else:
            print(f"[ERROR] Pipeline failed: {result.get('error', 'Unknown error')}")
            return 1
    
    elif args.urls_file:
        results = orchestrator.run_batch_pipeline(args.urls_file)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º batch —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ raw_subtitles –ø–∞–ø–∫—É (–Ω–µ –≤ SAG!)
        results_dir = Path(orchestrator.config['pipeline']['subtitles']['output_dir'])
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        results_file = results_dir / f"batch_pipeline_results_{timestamp}.json"
        with open(results_file, "w") as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        
        successful = sum(1 for r in results if r["status"] == "success")
        print(f"\n[BATCH COMPLETE] SAG v2.0: {successful}/{len(results)} URLs processed successfully")
        print(f"[RESULTS] Saved to: {results_file}")
        
        return 0 if successful == len(results) else 1

if __name__ == "__main__":
    sys.exit(main())