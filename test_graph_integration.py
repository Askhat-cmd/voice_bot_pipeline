#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
–¢–µ—Å—Ç –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–π –≥—Ä–∞—Ñ-–∫–æ–ª–ª–µ–∫—Ü–∏–∏ (442 —É–∑–ª–∞ + 259 –æ—Ç–Ω–æ—à–µ–Ω–∏–π)
–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Ä–∞–±–æ—Ç—É –Ω–æ–≤—ã—Ö –º–µ—Ç–æ–¥–æ–≤ –∞–Ω–∞–ª–∏–∑–∞ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö —Å–≤—è–∑–µ–π
"""

import sys
from pathlib import Path

# –î–æ–±–∞–≤–ª—è–µ–º –∫–æ—Ä–µ–Ω—å –ø—Ä–æ–µ–∫—Ç–∞ –≤ sys.path
PROJECT_ROOT = Path(__file__).resolve().parents[0]
if str(PROJECT_ROOT) not in sys.path:
    sys.path.insert(0, str(PROJECT_ROOT))

from text_processor.sarsekenov_processor import SarsekenovProcessor

def test_graph_integration():
    """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—é —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–π –≥—Ä–∞—Ñ-–∫–æ–ª–ª–µ–∫—Ü–∏–∏"""
    print("üöÄ –¢–ï–°–¢ –ò–ù–¢–ï–ì–†–ê–¶–ò–ò –†–ê–°–®–ò–†–ï–ù–ù–û–ô –ì–†–ê–§-–ö–û–õ–õ–ï–ö–¶–ò–ò")
    print("=" * 60)
    
    # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä
    processor = SarsekenovProcessor()
    
    # –¢–µ—Å—Ç 1: –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–º–µ—Ä–∞ –≥—Ä–∞—Ñ-–∫–æ–ª–ª–µ–∫—Ü–∏–∏
    print(f"\nüìä –†–ê–ó–ú–ï–† –ì–†–ê–§-–ö–û–õ–õ–ï–ö–¶–ò–ò:")
    print(f"   –£–∑–ª—ã: {len(processor.graph_nodes)} (–æ–∂–∏–¥–∞–ª–æ—Å—å: 442)")
    print(f"   –û—Ç–Ω–æ—à–µ–Ω–∏—è: {len(processor.graph_relationships)} (–æ–∂–∏–¥–∞–ª–æ—Å—å: 259)")
    print(f"   –°–∏–Ω–æ–Ω–∏–º—ã: {len(processor.synonym_map)}")
    print(f"   –°—Ç–æ–ø-—Å–ª–æ–≤–∞: {len(processor.stop_words)}")
    
    # –¢–µ—Å—Ç 2: –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–ª—é—á–µ–≤—ã—Ö –∫–æ–Ω—Ü–µ–ø—Ç–æ–≤ –Ω–µ–π—Ä–æ—Å—Ç–∞–ª–∫–∏–Ω–≥–∞
    print(f"\nüéØ –ö–õ–Æ–ß–ï–í–´–ï –ö–û–ù–¶–ï–ü–¢–´ –ù–ï–ô–†–û–°–¢–ê–õ–ö–ò–ù–ì–ê:")
    key_concepts = ["–Ω–µ–π—Ä–æ—Å—Ç–∞–ª–∫–∏–Ω–≥", "–Ω–µ–æ—Å—Ç–∞–ª–∫–∏–Ω–≥", "–º–µ—Ç–∞–Ω–∞–±–ª—é–¥–µ–Ω–∏–µ", "–ø–æ–ª–µ –≤–Ω–∏–º–∞–Ω–∏—è", "–æ—Å–æ–∑–Ω–∞–≤–∞–Ω–∏–µ"]
    for concept in key_concepts:
        status = "‚úÖ" if concept in processor.graph_nodes else "‚ùå"
        print(f"   {status} {concept}")
    
    # –¢–µ—Å—Ç 3: –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –≥—Ä–∞—Ñ-—Å—É—â–Ω–æ—Å—Ç–µ–π
    print(f"\nüîç –¢–ï–°–¢ –ò–ó–í–õ–ï–ß–ï–ù–ò–Ø –ì–†–ê–§-–°–£–©–ù–û–°–¢–ï–ô:")
    test_content = """
    –í –Ω–µ–π—Ä–æ—Å—Ç–∞–ª–∫–∏–Ω–≥–µ –º—ã –∏—Å–ø–æ–ª—å–∑—É–µ–º –º–µ—Ç–∞–Ω–∞–±–ª—é–¥–µ–Ω–∏–µ –¥–ª—è –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –ø–æ–ª—è –≤–Ω–∏–º–∞–Ω–∏—è. 
    –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –Ω–∞–º –æ—Å–æ–∑–Ω–∞–≤–∞—Ç—å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ —Ä–µ–∞–∫—Ü–∏–∏ –∏ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –ø–æ–≤–µ–¥–µ–Ω–∏—è. 
    –ß–µ—Ä–µ–∑ —Ü–µ–Ω—Ç—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –∑–∞–∑–µ–º–ª–µ–Ω–∏–µ –º—ã –ø—Ä–∏—Ö–æ–¥–∏–º –∫ –≥–ª—É–±–æ–∫–æ–º—É –ø–æ–Ω–∏–º–∞–Ω–∏—é —Å–µ–±—è.
    """
    
    test_keywords = ["–º–µ—Ç–∞–Ω–∞–±–ª—é–¥–µ–Ω–∏–µ", "–≤–Ω–∏–º–∞–Ω–∏–µ", "–æ—Å–æ–∑–Ω–∞–≤–∞–Ω–∏–µ", "–ø–∞—Ç—Ç–µ—Ä–Ω—ã", "—Ü–µ–Ω—Ç—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ"]
    
    entities = processor.extract_graph_entities(test_content, test_keywords)
    print(f"   –ò–∑–≤–ª–µ—á–µ–Ω–æ —Å—É—â–Ω–æ—Å—Ç–µ–π: {len(entities)} (–æ–∂–∏–¥–∞–ª–æ—Å—å: 5-15)")
    print(f"   –°—É—â–Ω–æ—Å—Ç–∏: {entities[:10]}...")  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 10
    
    # –¢–µ—Å—Ç 4: –ü—Ä–æ–≤–µ—Ä–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö —Å–≤—è–∑–µ–π
    print(f"\nüîó –¢–ï–°–¢ –ê–ù–ê–õ–ò–ó–ê –°–ï–ú–ê–ù–¢–ò–ß–ï–°–ö–ò–• –°–í–Ø–ó–ï–ô:")
    relationships = processor.analyze_semantic_relationships(entities)
    
    print(f"   –ö–æ–Ω—Ü–µ–ø—Ç—É–∞–ª—å–Ω—ã–µ —Å–≤—è–∑–∏: {len(relationships.get('conceptual_links', []))}")
    print(f"   –ö–∞—É–∑–∞–ª—å–Ω—ã–µ —Å–≤—è–∑–∏: {len(relationships.get('causal_links', []))}")
    print(f"   –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —Å–≤—è–∑–∏: {len(relationships.get('practical_links', []))}")
    
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–∏–º–µ—Ä—ã —Å–≤—è–∑–µ–π
    if relationships.get('conceptual_links'):
        print(f"   –ü—Ä–∏–º–µ—Ä –∫–æ–Ω—Ü–µ–ø—Ç—É–∞–ª—å–Ω–æ–π —Å–≤—è–∑–∏: {relationships['conceptual_links'][0]}")
    
    # –¢–µ—Å—Ç 5: –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–∏–æ—Ä–∏—Ç–∏–∑–∞—Ü–∏–∏
    print(f"\n‚≠ê –¢–ï–°–¢ –ü–†–ò–û–†–ò–¢–ò–ó–ê–¶–ò–ò:")
    print(f"   –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã–µ –∫–æ–Ω—Ü–µ–ø—Ç—ã: {entities[:5]}")
    print(f"   –í—Å–µ —Å—É—â–Ω–æ—Å—Ç–∏ –≤ –≥—Ä–∞—Ñ-–∫–æ–ª–ª–µ–∫—Ü–∏–∏: {sum(1 for e in entities if e in processor.graph_nodes)}")
    
    # –¢–µ—Å—Ç 6: –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–∏–Ω–æ–Ω–∏–º–æ–≤
    print(f"\nüîÑ –¢–ï–°–¢ –°–ò–ù–û–ù–ò–ú–û–í:")
    test_synonyms = ["–æ—Å–æ–∑–Ω–∞–Ω–Ω–æ—Å—Ç—å", "–Ω–µ–æ—Å—Ç–∞–ª–∫–∏–Ω–≥", "–∂–∏–≤–æ–µ –ø–µ—Ä–µ–∂–∏–≤–∞–Ω–∏–µ"]
    for synonym in test_synonyms:
        canonical = processor.synonym_map.get(synonym, synonym)
        status = "‚úÖ" if canonical != synonym else "‚ùå"
        print(f"   {status} {synonym} ‚Üí {canonical}")
    
    print(f"\n" + "=" * 60)
    print("üéâ –¢–ï–°–¢ –ó–ê–í–ï–†–®–ï–ù!")
    
    return True

def test_performance():
    """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–π —Å–∏—Å—Ç–µ–º—ã"""
    print(f"\n‚ö° –¢–ï–°–¢ –ü–†–û–ò–ó–í–û–î–ò–¢–ï–õ–¨–ù–û–°–¢–ò:")
    
    processor = SarsekenovProcessor()
    
    # –¢–µ—Å—Ç —Å –±–æ–ª—å—à–∏–º –æ–±—ä–µ–º–æ–º —Ç–µ–∫—Å—Ç–∞
    large_content = """
    """ * 1000  # –°–æ–∑–¥–∞–µ–º –±–æ–ª—å—à–æ–π —Ç–µ–∫—Å—Ç
    
    import time
    start_time = time.time()
    
    # –¢–µ—Å—Ç–∏—Ä—É–µ–º –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π
    entities = processor.extract_graph_entities(large_content, ["—Ç–µ—Å—Ç", "–ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å"])
    
    extraction_time = time.time() - start_time
    
    start_time = time.time()
    
    # –¢–µ—Å—Ç–∏—Ä—É–µ–º –∞–Ω–∞–ª–∏–∑ —Å–≤—è–∑–µ–π
    relationships = processor.analyze_semantic_relationships(entities)
    
    analysis_time = time.time() - start_time
    
    print(f"   –í—Ä–µ–º—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Å—É—â–Ω–æ—Å—Ç–µ–π: {extraction_time:.3f}—Å")
    print(f"   –í—Ä–µ–º—è –∞–Ω–∞–ª–∏–∑–∞ —Å–≤—è–∑–µ–π: {analysis_time:.3f}—Å")
    print(f"   –û–±—â–µ–µ –≤—Ä–µ–º—è: {extraction_time + analysis_time:.3f}—Å")
    
    return True

if __name__ == "__main__":
    try:
        test_graph_integration()
        test_performance()
        print(f"\n‚úÖ –í–°–ï –¢–ï–°–¢–´ –ü–†–û–ô–î–ï–ù–´ –£–°–ü–ï–®–ù–û!")
    except Exception as e:
        print(f"\n‚ùå –û–®–ò–ë–ö–ê –í –¢–ï–°–¢–ê–•: {e}")
        import traceback
        traceback.print_exc()
