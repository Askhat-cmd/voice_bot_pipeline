#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Sarsekenov-Specific Subtitle Processor
–ê–¥–∞–ø—Ç–∞—Ü–∏—è –ø–æ–¥ –ª–µ–∫—Ü–∏–∏ –°–∞–ª–∞–º–∞—Ç–∞ –°–∞—Ä—Å–µ–∫–µ–Ω–æ–≤–∞ (–Ω–µ–π—Ä–æ—Å—Ç–∞–ª–∫–∏–Ω–≥ / –Ω–µ–æ—Å—Ç–∞–ª–∫–∏–Ω–≥)
–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Å—Ç–∏–ª—å, —Ç–µ—Ä–º–∏–Ω–æ–ª–æ–≥–∏—é –∏ –∫–ª—é—á–µ–≤—ã–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è.
"""

import argparse
import json
import os
import sys
import time
from pathlib import Path
from typing import Any, Dict, List, Optional

import orjson
import tiktoken
from openai import OpenAI

"""
–ü–æ–∑–≤–æ–ª—è–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å –∫–∞–∫ –Ω–∞–ø—Ä—è–º—É—é –ø–æ .json —Å—É–±—Ç–∏—Ç—Ä–∞–º, —Ç–∞–∫ –∏ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏
–∏–∑–≤–ª–µ–∫–∞—Ç—å —Å—É–±—Ç–∏—Ç—Ä—ã –ø–æ URL –∏–∑ —Ñ–∞–π–ª–∞ urls.txt –≤ –∫–æ—Ä–Ω–µ –ø—Ä–æ–µ–∫—Ç–∞.
"""

# –î–æ–±–∞–≤–ª—è–µ–º –∫–æ—Ä–µ–Ω—å –ø—Ä–æ–µ–∫—Ç–∞ (.. –æ—Ç text_processor) –≤ sys.path –¥–ª—è –∏–º–ø–æ—Ä—Ç–∞ —É—Ç–∏–ª–∏—Ç
PROJECT_ROOT = Path(__file__).resolve().parents[1]
if str(PROJECT_ROOT) not in sys.path:
    sys.path.insert(0, str(PROJECT_ROOT))

from env_utils import load_env
from subtitle_extractor.get_subtitles import YouTubeSubtitlesExtractor


def _hms(seconds: Optional[float]) -> Optional[str]:
    if seconds is None:
        return None
    seconds = int(seconds)
    return f"{seconds//3600:02d}:{(seconds%3600)//60:02d}:{seconds%60:02d}"


class SarsekenovProcessor:
    """–°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä –ø–æ–¥ –ª–µ–∫—Ü–∏–∏ –°. –°–∞—Ä—Å–µ–∫–µ–Ω–æ–≤–∞.

    –§–æ–∫—É—Å –Ω–∞ —Ç–µ—Ä–º–∏–Ω–∞—Ö –∏ –∫–æ–Ω—Ü–µ–ø—Ü–∏—è—Ö: –Ω–µ–π—Ä–æ—Å—Ç–∞–ª–∫–∏–Ω–≥, –Ω–µ–æ—Å—Ç–∞–ª–∫–∏–Ω–≥,
    –Ω–∞–±–ª—é–¥–µ–Ω–∏–µ –∑–∞ —É–º–æ–º, –≤–Ω–∏–º–∞–Ω–∏–µ, –º–µ—Ç–∞–Ω–∞–±–ª—é–¥–µ–Ω–∏–µ, –ø–∞—Ç—Ç–µ—Ä–Ω—ã, —Ç—Ä–∏–≥–≥–µ—Ä—ã,
    –ø–æ–ª–µ –≤–Ω–∏–º–∞–Ω–∏—è, –æ—Å–æ–∑–Ω–∞–≤–∞–Ω–∏–µ, –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–µ—Ä–µ–∂–∏–≤–∞–Ω–∏–π, –ø—Ä–∞–∫—Ç–∏–∫–∏ –∏ —É–ø—Ä–∞–∂–Ω–µ–Ω–∏—è.
    """

    def __init__(self, primary_model: str = "gpt-4o-mini", refine_model: str = "gpt-5-mini"):
        load_env()
        self.primary_model = primary_model
        self.refine_model = refine_model
        self.client = OpenAI()
        self.encoding = tiktoken.get_encoding("cl100k_base")

        # –î–æ–º–µ–Ω–Ω—ã–µ —É–∫–∞–∑–∞–Ω–∏—è –¥–ª—è –±–æ–ª–µ–µ —Ç–æ—á–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ä–µ—á–∏ –°–∞—Ä—Å–µ–∫–µ–Ω–æ–≤–∞
        self.domain_context = (
            "–≠—Ç–æ –ª–µ–∫—Ü–∏—è –°–∞–ª–∞–º–∞—Ç–∞ –°–∞—Ä—Å–µ–∫–µ–Ω–æ–≤–∞. –ù–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ: –Ω–µ–π—Ä–æ—Å—Ç–∞–ª–∫–∏–Ω–≥/–Ω–µ–æ—Å—Ç–∞–ª–∫–∏–Ω–≥. "
            "–°—Ç—Ä–æ–≥–æ —Å–æ—Ö—Ä–∞–Ω—è–π —Ç–µ—Ä–º–∏–Ω–æ–ª–æ–≥–∏—é –∏ –∞–≤—Ç–æ—Ä—Å–∫–∏–µ —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∏. "
            "–¢–∏–ø–∏—á–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã –∏ —Ç–µ–º—ã: –Ω–µ–π—Ä–æ—Å—Ç–∞–ª–∫–∏–Ω–≥, –Ω–µ–æ—Å—Ç–∞–ª–∫–∏–Ω–≥, –≤–Ω–∏–º–∞–Ω–∏–µ, –ø–æ–ª–µ –≤–Ω–∏–º–∞–Ω–∏—è, –Ω–∞–±–ª—é–¥–µ–Ω–∏–µ –∑–∞ —É–º–æ–º, "
            "–æ—Å–æ–∑–Ω–∞–≤–∞–Ω–∏–µ, –º–µ—Ç–∞–Ω–∞–±–ª—é–¥–µ–Ω–∏–µ, –ø–∞—Ç—Ç–µ—Ä–Ω—ã, —Ç—Ä–∏–≥–≥–µ—Ä—ã, –∞–≤—Ç–æ–º–∞—Ç–∏–∑–º—ã, –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–µ—Ä–µ–∂–∏–≤–∞–Ω–∏–π, –ø—Ä–∞–∫—Ç–∏–∫–∞, —É–ø—Ä–∞–∂–Ω–µ–Ω–∏—è, –≤–æ–ø—Ä–æ—Å—ã –∞—É–¥–∏—Ç–æ—Ä–∏–∏, —Ä–∞–∑–±–æ—Ä—ã. "
            "–ù–µ –ø–µ—Ä–µ–ø–∏—Å—ã–≤–∞–π —Å–º—ã—Å–ª —Å–≤–æ–∏–º–∏ —Å–ª–æ–≤–∞–º–∏ ‚Äî –æ—á–∏—â–∞–π —Ä–µ—á—å –æ—Ç —à—É–º–∞, —Å–æ—Ö—Ä–∞–Ω—è—è —Å—Ç–∏–ª—å.")

    # ---------------------- Internal helpers ---------------------- #
    def _tok(self, text: str) -> int:
        return len(self.encoding.encode(text))

    def _ask(self, model: str, prompt: str, max_tokens: int = 2200, temperature: float = 0.3) -> str:
        r = self.client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": "–¢—ã —Ä–µ–¥–∞–∫—Ç–æ—Ä –ª–µ–∫—Ü–∏–π –°–∞–ª–∞–º–∞—Ç–∞ –°–∞—Ä—Å–µ–∫–µ–Ω–æ–≤–∞. –°–æ—Ö—Ä–∞–Ω—è–π —Ç–µ—Ä–º–∏–Ω–æ–ª–æ–≥–∏—é –Ω–µ–π—Ä–æ—Å—Ç–∞–ª–∫–∏–Ω–≥–∞/–Ω–µ–æ—Å—Ç–∞–ª–∫–∏–Ω–≥–∞ –∏ –∞–≤—Ç–æ—Ä—Å–∫–∏–π —Å—Ç–∏–ª—å."},
                {"role": "user", "content": prompt},
            ],
            temperature=temperature,
            max_tokens=max_tokens,
        )
        return r.choices[0].message.content

    # –û—á–µ–Ω—å –ª—ë–≥–∫–∞—è –æ—á–∏—Å—Ç–∫–∞ —à—É–º–∞, –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏—è —Å–º—ã—Å–ª–∞
    def _light_clean(self, text: str) -> str:
        replacements = [
            ("[–º—É–∑—ã–∫–∞]", ""),
            ("[Music]", ""),
            (">>", ""),
        ]
        cleaned = text
        for a, b in replacements:
            cleaned = cleaned.replace(a, b)
        cleaned = " ".join(cleaned.split())
        return cleaned.strip()

    # ---------------------- IO ---------------------- #
    def load_input(self, file_path: Path) -> Dict[str, Any]:
        """–ü–æ–¥–¥–µ—Ä–∂–∫–∞ –≤—Ö–æ–¥–∞ –∏–∑ get_subtitles.json —Ñ–æ—Ä–º–∞—Ç–∞ –∏ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã—Ö —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç–æ–≤."""
        with open(file_path, "r", encoding="utf-8") as f:
            data = json.load(f)

        if "subtitles" in data:
            raw_segments = data["subtitles"]
        elif "transcript" in data and isinstance(data["transcript"], dict):
            raw_segments = data["transcript"].get("segments", [])
        else:
            raw_segments = data.get("segments", [])

        segments: List[Dict[str, Any]] = []
        for i, s in enumerate(raw_segments):
            start = s.get("start", 0.0)
            dur = s.get("duration", max(0.0, s.get("end", 0.0) - s.get("start", 0.0)))
            end = start + dur
            segments.append({
                "id": i,
                "start": start,
                "end": end,
                "duration": dur,
                "text": s.get("text", ""),
            })

        return {
            "segments": segments,
            "metadata": data.get("metadata", data.get("file_info", {})),
            "full_text": " ".join(s.get("text", "") for s in segments),
        }

    # ---------------------- Chunking ---------------------- #
    def chunk_segments(self, segments: List[Dict[str, Any]], *, max_tokens: int = 4800, overlap_tokens: int = 200) -> List[Dict[str, Any]]:
        """–£–º–Ω–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ: —É—á–∏—Ç—ã–≤–∞–µ–º –º–∞—Ä–∫–µ—Ä—ã —Å–º–µ–Ω—ã —Ç–µ–º—ã –∏ –¥–ª–∏–Ω–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã."""
        chunks: List[Dict[str, Any]] = []
        current: List[Dict[str, Any]] = []
        cur_tokens = 0

        def flush_chunk():
            nonlocal current, cur_tokens
            if not current:
                return
            chunks.append({
                "text": " ".join(s.get("text", "") for s in current),
                "start_time": current[0]["start"],
                "end_time": current[-1]["end"],
                "segments": list(current),
            })
            # overlap –ø–æ –ø–æ—Å–ª–µ–¥–Ω–∏–º 5 —Å–µ–≥–º–µ–Ω—Ç–∞–º, –µ—Å–ª–∏ –Ω–µ –ø—Ä–µ–≤—ã—à–∞–µ—Ç overlap_tokens
            tail = current[-5:]
            tail_tok = sum(self._tok(s.get("text", "")) for s in tail)
            if tail_tok <= overlap_tokens:
                current, cur_tokens = tail, tail_tok
            else:
                current, cur_tokens = [], 0

        for s in segments:
            t = s.get("text", "")
            n = self._tok(t)

            is_boundary = any(marker in t for marker in [
                "–ò—Ç–∞–∫", "–ü–æ–¥–≤–µ–¥—ë–º –∏—Ç–æ–≥", "–ü–æ–¥–≤–æ–¥—è –∏—Ç–æ–≥", "–¢–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º",
                "–î–∞–≤–∞–π—Ç–µ", "–¢–µ–ø–µ—Ä—å", "–°–ª–µ–¥—É—é—â–∏–π", "–í–æ–ø—Ä–æ—Å –∏–∑ –∑–∞–ª–∞", "–ü—Ä–∞–∫—Ç–∏–∫–∞", "–£–ø—Ä–∞–∂–Ω–µ–Ω–∏–µ",
            ])

            # –î–µ–ª–∞–µ–º –±–ª–æ–∫–∏ –∫—Ä—É–ø–Ω–µ–µ: –º–∏–Ω–∏–º—É–º 2000 —Ç–æ–∫–µ–Ω–æ–≤, –º–∞–∫—Å–∏–º—É–º 4800
            if current and (cur_tokens + n > max_tokens or (is_boundary and cur_tokens > 2000)):
                flush_chunk()

            current.append(s)
            cur_tokens += n

        flush_chunk()
        return chunks

    # ---------------------- LLM prompts ---------------------- #
    def process_chunk(self, chunk: Dict[str, Any]) -> List[Dict[str, Any]]:
        source_text = self._light_clean(chunk['text'])
        prompt = f"""{self.domain_context}

–ó–ê–î–ê–ß–ê: –†–∞–∑–±–µ–π —Ñ—Ä–∞–≥–º–µ–Ω—Ç –ª–µ–∫—Ü–∏–∏ –Ω–∞ –ö–†–£–ü–ù–´–ï –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–µ –±–ª–æ–∫–∏ (4‚Äì8 –º–∏–Ω—É—Ç –∫–∞–∂–¥—ã–π, –º–∏–Ω–∏–º—É–º 300-500 —Å–ª–æ–≤). –°–¢–†–û–ì–û —Å–æ—Ö—Ä–∞–Ω—è–π –∞–≤—Ç–æ—Ä—Å–∫–∏–µ —Ç–µ—Ä–º–∏–Ω—ã –∏ —Å—Ç–∏–ª—å —Ä–µ—á–∏.

–¢–†–ï–ë–û–í–ê–ù–ò–Ø:
1) –ö–†–£–ü–ù–´–ï –ë–õ–û–ö–ò: –ö–∞–∂–¥—ã–π –±–ª–æ–∫ –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å 300-500+ —Å–ª–æ–≤. –ù–ï –¥–µ–ª–∏ –Ω–∞ –º–µ–ª–∫–∏–µ —á–∞—Å—Ç–∏.
2) –¢–µ—Ä–º–∏–Ω—ã –Ω–µ —Ç—Ä–æ–≥–∞—Ç—å: –Ω–µ–π—Ä–æ—Å—Ç–∞–ª–∫–∏–Ω–≥, –Ω–µ–æ—Å—Ç–∞–ª–∫–∏–Ω–≥, –ø–æ–ª–µ –≤–Ω–∏–º–∞–Ω–∏—è, –Ω–∞–±–ª—é–¥–µ–Ω–∏–µ –∑–∞ —É–º–æ–º, –º–µ—Ç–∞–Ω–∞–±–ª—é–¥–µ–Ω–∏–µ, –∞–≤—Ç–æ–º–∞—Ç–∏–∑–º—ã, –ø–∞—Ç—Ç–µ—Ä–Ω—ã, —Ç—Ä–∏–≥–≥–µ—Ä—ã, –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–µ—Ä–µ–∂–∏–≤–∞–Ω–∏–π, –ø—Ä–∞–∫—Ç–∏–∫–∞, —É–ø—Ä–∞–∂–Ω–µ–Ω–∏–µ, —Ä–∞–∑–±–æ—Ä –∏ –¥—Ä.
3) –í–ï–°–¨ –¢–ï–ö–°–¢ –≤–∫–ª—é—á–∞—Ç—å: –ù–ï –ø–µ—Ä–µ—Ñ—Ä–∞–∑–∏—Ä—É–π –∏ –ù–ï –ø–µ—Ä–µ—Å–∫–∞–∑—ã–≤–∞–π. –í–∫–ª—é—á–∏ –í–°–Å —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ –≤ –±–ª–æ–∫–∏. –£–¥–∞–ª—è–π —Ç–æ–ª—å–∫–æ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π –º—É—Å–æ—Ä.
4) –£–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏: –∫–∞–∂–¥—ã–π title –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —É–Ω–∏–∫–∞–ª—å–Ω—ã–º –∏ –æ—Ç—Ä–∞–∂–∞—Ç—å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—É—é —Ç–µ–º—É –±–ª–æ–∫–∞.
5) –§–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞ ‚Äî JSON-–º–∞—Å—Å–∏–≤:
[
  {{
    "start": "HH:MM:SS",
    "end": "HH:MM:SS",
    "title": "–£–Ω–∏–∫–∞–ª—å–Ω—ã–π –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π –∑–∞–≥–æ–ª–æ–≤–æ–∫ (5‚Äì12 —Å–ª–æ–≤)",
    "summary": "–ö—Ä–∞—Ç–∫–æ–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ (2‚Äì3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è) —Å —Ç–µ—Ä–º–∏–Ω–∞–º–∏ –∏–∑ –ª–µ–∫—Ü–∏–∏",
    "keywords": ["—Ç–µ—Ä–º–∏–Ω1", "—Ç–µ—Ä–º–∏–Ω2", "–∫–æ–Ω—Ü–µ–ø—Ç3"],
    "content": "–ü–û–õ–ù–´–ô –æ—á–∏—â–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –±–ª–æ–∫–∞ (300-500+ —Å–ª–æ–≤, –≤–µ—Å—å –∫–æ–Ω—Ç–µ–Ω—Ç —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞)"
  }}
]

–§–†–ê–ì–ú–ï–ù–¢ –¢–ï–ö–°–¢–ê (–Ω–µ –ø–µ—Ä–µ—Å–∫–∞–∑—ã–≤–∞—Ç—å, –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∫–∞–∫ –µ—Å—Ç—å; —Ç–æ–ª—å–∫–æ –º–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —á–∏—Å—Ç–∫–∞):
{source_text}
"""

        try:
            resp = self._ask(self.primary_model, prompt, max_tokens=4200, temperature=0.2)
            txt = resp.strip()
            if txt.startswith("```json"):
                txt = txt[7:]
            elif txt.startswith("```"):
                txt = txt[3:]
            if txt.endswith("```"):
                txt = txt[:-3]
            blocks = orjson.loads(txt)

            st, et = _hms(chunk.get("start_time")), _hms(chunk.get("end_time"))
            for b in blocks:
                b.setdefault("start", st)
                b.setdefault("end", et)
            return blocks
        except Exception:
            # –ë–µ–∑–æ–ø–∞—Å–Ω—ã–π —Ä–µ–∑–µ—Ä–≤: –≤–µ—Ä–Ω—É—Ç—å –∏—Å—Ö–æ–¥–Ω—ã–π —Ñ—Ä–∞–≥–º–µ–Ω—Ç –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π —Å–º—ã—Å–ª–∞
            return [{
                "start": _hms(chunk.get("start_time")),
                "end": _hms(chunk.get("end_time")),
                "title": "–§—Ä–∞–≥–º–µ–Ω—Ç –ª–µ–∫—Ü–∏–∏ (–±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π)",
                "summary": "–ë–ª–æ–∫ —Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω –∏–∑ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ (–º–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞).",
                "keywords": ["–Ω–µ–π—Ä–æ—Å—Ç–∞–ª–∫–∏–Ω–≥"],
                "content": source_text
            }]

    def refine_blocks(self, blocks: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        if not self.refine_model:
            return blocks

        prompt = f"""{self.domain_context}

–ü–û–õ–ò–†–û–í–ö–ê –ë–õ–û–ö–û–í: —É–ª—É—á—à–∏ —á–∏—Ç–∞–µ–º–æ—Å—Ç—å, –Ω–æ —Å—Ç—Ä–æ–≥–æ —Å–æ—Ö—Ä–∞–Ω–∏ —Ç–µ—Ä–º–∏–Ω—ã –∏ –∞–≤—Ç–æ—Ä—Å–∫–∏–µ —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∏. 
–ú–æ–∂–Ω–æ: –ø—Ä–∞–≤–∏—Ç—å –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é, –∞–±–∑–∞—Ü—ã, –¥–µ–ª–∞—Ç—å –∑–∞–≥–æ–ª–æ–≤–∫–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω–µ–µ, —Ä–∞—Å—à–∏—Ä—è—Ç—å keywords. –ù–µ–ª—å–∑—è: –ø–µ—Ä–µ–ø–∏—Å—ã–≤–∞—Ç—å content —Å–≤–æ–∏–º–∏ —Å–ª–æ–≤–∞–º–∏.

–í–µ—Ä–Ω–∏ JSON-–º–∞—Å—Å–∏–≤ –≤ —Ç–æ–º –∂–µ —Ñ–æ—Ä–º–∞—Ç–µ:
{orjson.dumps(blocks, option=orjson.OPT_INDENT_2).decode()}
"""

        try:
            resp = self._ask(self.refine_model, prompt, max_tokens=5200, temperature=0.2)
            if resp.startswith("```json"):
                resp = resp[7:]
            elif resp.startswith("```"):
                resp = resp[3:]
            if resp.endswith("```"):
                resp = resp[:-3]
            return orjson.loads(resp)
        except Exception:
            return blocks

    # ---------------------- Save ---------------------- #
    def save_markdown(self, md_path: Path, data: Dict[str, Any], base_name: str) -> None:
        lines: List[str] = [
            "# üß≠ –õ–µ–∫—Ü–∏–∏ –°. –°–∞—Ä—Å–µ–∫–µ–Ω–æ–≤–∞ ‚Äî –∫–æ–Ω—Å–ø–µ–∫—Ç",
            f"## {data.get('document_title', base_name)}",
            f"*{data.get('document_summary', '–û–ø–∏—Å–∞–Ω–∏–µ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–æ')}*",
            "",
            # –ö–æ—Ä–æ—Ç–∫–æ–µ –æ–≥–ª–∞–≤–ª–µ–Ω–∏–µ –∏ –æ–±–æ–±—â—ë–Ω–Ω–æ–µ —Ä–µ–∑—é–º–µ —Å–≤–µ—Ä—Ö—É
            ("**–ö–æ—Ä–æ—Ç–∫–æ–µ –æ–≥–ª–∞–≤–ª–µ–Ω–∏–µ:** " + data.get('overview_toc', '')).strip(),
            ("**–û–±—â–µ–µ —Ä–µ–∑—é–º–µ:** " + data.get('overview_summary', '')).strip(),
            "",
            "---",
            "## üìë –û–≥–ª–∞–≤–ª–µ–Ω–∏–µ",
        ]

        for i, b in enumerate(data.get("blocks", [])):
            lines.append(f"{i+1}. [{b.get('title','–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è')}](#{i+1}-block)")

        lines.append("\n---\n")

        for i, b in enumerate(data.get("blocks", [])):
            lines.extend([
                f"## <a id='{i+1}-block'></a> {i+1}. {b.get('title','–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è')}",
                f"**‚è± –í—Ä–µ–º—è:** [{b.get('start','N/A')} - {b.get('end','N/A')}]",
                f"**üìù –ö—Ä–∞—Ç–∫–æ–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ:** {b.get('summary','')}",
                f"**üè∑ –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞:** {', '.join(b.get('keywords', []))}",
                "",
                "### –°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ:",
                b.get("content", ""),
                "",
                "---\n",
            ])

        md_path.write_text("\n".join(lines), encoding="utf-8")

    # ---------------------- Main processing ---------------------- #
    def process_file(self, input_path: Path, output_dir: Path) -> Dict[str, Any]:
        data = self.load_input(input_path)
        segments = data.get("segments", [])
        if not segments:
            raise RuntimeError("No segments found in input")

        chunks = self.chunk_segments(segments)
        all_blocks: List[Dict[str, Any]] = []
        for i, ch in enumerate(chunks, 1):
            print(f"[INFO] Processing chunk {i}/{len(chunks)}")
            blocks = self.process_chunk(ch)
            all_blocks.extend(blocks)
            time.sleep(0.4)

        if self.refine_model:
            all_blocks = self.refine_blocks(all_blocks)

        base = input_path.stem
        video_id = base
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Å—Ç–∞–±–∏–ª—å–Ω—ã–µ ID –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–ª—è RAG
        for i, block in enumerate(all_blocks):
            block["block_id"] = f"{video_id}_{i:03d}"
            block["video_id"] = video_id
            block["source_url"] = f"https://youtube.com/watch?v={video_id}"
            # –í—Ä–µ–º–µ–Ω–Ω–∞—è —Å—Å—ã–ª–∫–∞ –Ω–∞ YouTube
            if block.get("start"):
                start_seconds = self._hms_to_seconds(block["start"])
                if start_seconds is not None:
                    block["youtube_link"] = f"https://youtube.com/watch?v={video_id}&t={start_seconds}s"
        
        doc = {
            "document_title": f"–õ–µ–∫—Ü–∏—è –°–∞—Ä—Å–µ–∫–µ–Ω–æ–≤–∞: {base}",
            "document_summary": "–ö–æ–Ω—Å–ø–µ–∫—Ç –ª–µ–∫—Ü–∏–∏, —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–æ —Ç–µ–º–∞–º –Ω–µ–π—Ä–æ—Å—Ç–∞–ª–∫–∏–Ω–≥–∞/–Ω–µ–æ—Å—Ç–∞–ª–∫–∏–Ω–≥–∞.",
            "document_metadata": {
                "total_blocks": len(all_blocks),
                "language": "ru",
                "domain": "sarsekenov_neurostalking",
                "video_id": video_id,
                "source_url": f"https://youtube.com/watch?v={video_id}",
                "schema_version": "1.0",
            },
            "blocks": all_blocks,
            "full_text": data.get("full_text", ""),
        }

        # –°—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞—Ç—å ¬´–∫–æ—Ä–æ—Ç–∫–æ–µ –æ–≥–ª–∞–≤–ª–µ–Ω–∏–µ¬ª –∏ ¬´–æ–±—â–µ–µ —Ä–µ–∑—é–º–µ¬ª –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤—Å–µ—Ö –±–ª–æ–∫–æ–≤
        overview = self.generate_overview(doc)
        doc.update({
            "overview_toc": overview.get("overview_toc", ""),
            "overview_summary": overview.get("overview_summary", ""),
        })

        output_dir.mkdir(parents=True, exist_ok=True)
        json_path = output_dir / f"{base}.for_vector.json"
        md_path = output_dir / f"{base}.for_review.md"
        with open(json_path, "wb") as f:
            f.write(orjson.dumps(doc, option=orjson.OPT_INDENT_2))
        self.save_markdown(md_path, doc, base)

        print(f"[SUCCESS] JSON: {json_path}")
        print(f"[SUCCESS] MD  : {md_path}")
        return {"json_output": str(json_path), "md_output": str(md_path), "blocks": len(all_blocks)}

    # –°–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å —Å –æ—Ä–∫–µ—Å—Ç—Ä–∞—Ç–æ—Ä–æ–º (—Ç–∞ –∂–µ —Å–∏–≥–Ω–∞—Ç—É—Ä–∞, —á—Ç–æ —É SubtitlesProcessor)
    def process_subtitles_file(self, transcript_path: Path, output_dir: Path) -> Dict[str, Any]:
        result = self.process_file(transcript_path, output_dir)
        return {
            "json_output": result["json_output"],
            "md_output": result["md_output"],
            "blocks_created": result.get("blocks", 0),
        }

    def _hms_to_seconds(self, hms_str: str) -> Optional[int]:
        """–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç HH:MM:SS –≤ —Å–µ–∫—É–Ω–¥—ã"""
        try:
            parts = hms_str.split(':')
            if len(parts) == 3:
                h, m, s = map(int, parts)
                return h * 3600 + m * 60 + s
        except:
            pass
        return None

    # ---------------------- Overview generation ---------------------- #
    def generate_overview(self, doc: Dict[str, Any]) -> Dict[str, str]:
        # –ë–∞–∑–æ–≤—ã–π –≤–∞—Ä–∏–∞–Ω—Ç ¬´–æ–≥–ª–∞–≤–ª–µ–Ω–∏—è –æ–¥–Ω–∏–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ–º¬ª –∏–∑ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ –±–ª–æ–∫–æ–≤
        titles = [b.get("title", "") for b in doc.get("blocks", []) if b.get("title")]
        toc_sentence_default = " ‚Ä¢ ".join(titles[:20])  # –æ–≥—Ä–∞–Ω–∏—á–∏–º –¥–æ 20 –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤

        # –î–ª—è –æ–±–æ–±—â—ë–Ω–Ω–æ–≥–æ —Å–∞–º–º–∞—Ä–∏ –≤–æ—Å–ø–æ–ª—å–∑—É–µ–º—Å—è refine-–º–æ–¥–µ–ª—å—é, –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–∞
        try:
            blocks_for_prompt = [
                {
                    "title": b.get("title", ""),
                    "summary": b.get("summary", ""),
                    "start": b.get("start", ""),
                    "end": b.get("end", ""),
                }
                for b in doc.get("blocks", [])
            ]
            prompt = (
                "–°—Ñ–æ—Ä–º–∏—Ä—É–π:\n"
                "1) –ö–æ—Ä–æ—Ç–∫–æ–µ –æ–≥–ª–∞–≤–ª–µ–Ω–∏–µ –æ–¥–Ω–∏–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ–º (–≥–ª–∞–≤–Ω—ã–µ —Ç–µ–º—ã –ø–æ –ø–æ—Ä—è–¥–∫—É).\n"
                "2) –û–±–æ–±—â—ë–Ω–Ω–æ–µ —Ä–µ–∑—é–º–µ –Ω–∞ 2‚Äì4 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è (—Å–æ—Ö—Ä–∞–Ω—è–π —Å—Ç–∏–ª—å –∞–≤—Ç–æ—Ä–∞, –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ —Å–º—ã—Å–ª–∞).\n\n"
                "–î–∞–Ω–Ω—ã–µ –ø–æ –±–ª–æ–∫–∞–º:\n" + orjson.dumps(blocks_for_prompt, option=orjson.OPT_INDENT_2).decode()
            )
            resp = self._ask(self.refine_model or self.primary_model, prompt, max_tokens=600, temperature=0.2)
            # –ü—Ä–æ—Å—Ç–æ–π —Ö—ç–Ω–¥–ª–∏–Ω–≥: –∏—Å–∫–∞—Ç—å —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–∏
            lines = [l.strip() for l in resp.splitlines() if l.strip()]
            toc_line = ""
            summary_lines: List[str] = []
            for line in lines:
                low = line.lower()
                if not toc_line and ("–æ–≥–ª–∞–≤–ª–µ–Ω" in low or "—Ç–µ–º" in low):
                    toc_line = line.split(":", 1)[-1].strip() if ":" in line else line
                elif any(k in low for k in ["—Ä–µ–∑—é–º–µ", "–∏—Ç–æ–≥", "summary", "–æ–±–æ–±—â"]):
                    # –ø–æ—Å–ª–µ–¥—É—é—â–∏–µ —Å—Ç—Ä–æ–∫–∏ —Å—á–∏—Ç–∞–µ–º —Å–∞–º–º–∞—Ä–∏
                    continue
                elif toc_line and not summary_lines:
                    # –≤–æ–∑–º–æ–∂–Ω–æ —Å–ª–µ–¥—É—é—â–∏–π –±–ª–æ–∫ ‚Äî —ç—Ç–æ —Ä–µ–∑—é–º–µ, –µ—Å–ª–∏ —Å—Ç—Ä–æ–∫–∞ –¥–ª–∏–Ω–Ω–µ–µ
                    summary_lines.append(line)
                elif summary_lines:
                    summary_lines.append(line)
            overview_summary = " ".join(summary_lines).strip()
            return {
                "overview_toc": toc_line or toc_sentence_default,
                "overview_summary": overview_summary[:1200],
            }
        except Exception:
            return {"overview_toc": toc_sentence_default, "overview_summary": ""}


def main() -> int:
    ap = argparse.ArgumentParser(description="–ü—Ä–æ—Ü–µ—Å—Å–æ—Ä –ª–µ–∫—Ü–∏–π –°. –°–∞—Ä—Å–µ–∫–µ–Ω–æ–≤–∞")
    ap.add_argument("--input", help="–§–∞–π–ª –∏–ª–∏ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å —Å—É–±—Ç–∏—Ç—Ä–∞–º–∏ (.json). –ï—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω–æ, –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω urls.txt")
    ap.add_argument("--output", default="data/vector_ready", help="–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
    ap.add_argument("--primary-model", default="gpt-4o-mini", help="–û—Å–Ω–æ–≤–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –Ω–∞—Ä–µ–∑–∫–∏")
    ap.add_argument("--refine-model", default="gpt-5-mini", help="–ú–æ–¥–µ–ª—å –¥–ª—è –ø–æ–ª–∏—Ä–æ–≤–∫–∏")
    ap.add_argument("--urls-file", default=str(PROJECT_ROOT / "urls.txt"), help="–§–∞–π–ª —Å–æ —Å–ø–∏—Å–∫–æ–º URL (–ø–æ –æ–¥–Ω–æ–º—É –Ω–∞ —Å—Ç—Ä–æ–∫—É)")
    ap.add_argument("--language", default="ru", help="–ü—Ä–µ–¥–ø–æ—á–∏—Ç–∞–µ–º—ã–π —è–∑—ã–∫ —Å—É–±—Ç–∏—Ç—Ä–æ–≤ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏")
    args = ap.parse_args()

    # –ó–∞–≥—Ä—É–∂–∞–µ–º .env –¥–æ –ø—Ä–æ–≤–µ—Ä–∫–∏ –∫–ª—é—á–∞
    try:
        load_env()
    except Exception:
        pass

    if not os.getenv("OPENAI_API_KEY"):
        print("[ERROR] OPENAI_API_KEY –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
        return 1

    proc = SarsekenovProcessor(args.primary_model, args.refine_model)

    # –ï—Å–ª–∏ --input –Ω–µ –∑–∞–¥–∞–Ω, –∏–ª–∏ –≤ –ø–∞–ø–∫–µ –Ω–µ—Ç json, –ø—ã—Ç–∞–µ–º—Å—è –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å—É–±—Ç–∏—Ç—Ä—ã –∏–∑ urls.txt
    input_candidate: Optional[Path] = Path(args.input) if args.input else None
    urls_file = Path(args.urls_file)
    subtitles_dir = PROJECT_ROOT / "data" / "subtitles"

    if input_candidate is None:
        # –ê–≤—Ç–æ–∑–∞–≥—Ä—É–∑–∫–∞ –ø–æ urls.txt
        if urls_file.exists():
            print(f"[INFO] --input –Ω–µ —É–∫–∞–∑–∞–Ω. –ò—Å–ø–æ–ª—å–∑—É—é URLs –∏–∑: {urls_file}")
            extractor = YouTubeSubtitlesExtractor(str(subtitles_dir))
            with open(urls_file, "r", encoding="utf-8") as f:
                urls = [ln.strip() for ln in f if ln.strip()]
            for u in urls:
                try:
                    extractor.process_url(u, args.language)
                except Exception as e:
                    print(f"[ERROR] –ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å URL {u}: {e}")
            input_candidate = subtitles_dir
        else:
            print("[ERROR] –ù–µ —É–∫–∞–∑–∞–Ω --input –∏ –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç urls.txt. –£–∫–∞–∂–∏—Ç–µ –ø—É—Ç—å –∫ .json –∏–ª–∏ —Å–æ–∑–¥–∞–π—Ç–µ urls.txt")
            return 1

    in_path = input_candidate
    out_dir = Path(args.output)
    files: List[Path] = []
    if in_path.is_file():
        files = [in_path]
    else:
        files = list(in_path.glob("*.json"))
    if not files:
        print(f"[ERROR] –ù–µ –Ω–∞–π–¥–µ–Ω–æ JSON —Ñ–∞–π–ª–æ–≤ –≤ {in_path}")
        return 1

    out_dir.mkdir(parents=True, exist_ok=True)
    for jf in files:
        try:
            proc.process_file(jf, out_dir)
        except Exception as e:
            print(f"[ERROR] {jf.name}: {e}")
    return 0


if __name__ == "__main__":
    import sys
    sys.exit(main())


