#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Sarsekenov-Specific Subtitle Processor
–ê–¥–∞–ø—Ç–∞—Ü–∏—è –ø–æ–¥ –ª–µ–∫—Ü–∏–∏ –°–∞–ª–∞–º–∞—Ç–∞ –°–∞—Ä—Å–µ–∫–µ–Ω–æ–≤–∞ (–Ω–µ–π—Ä–æ—Å—Ç–∞–ª–∫–∏–Ω–≥ / –Ω–µ–æ—Å—Ç–∞–ª–∫–∏–Ω–≥)
–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Å—Ç–∏–ª—å, —Ç–µ—Ä–º–∏–Ω–æ–ª–æ–≥–∏—é –∏ –∫–ª—é—á–µ–≤—ã–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è.
"""

import argparse
import json
import os
import re
import sys
import time
from pathlib import Path
from typing import Any, Dict, List, Optional

import orjson
import tiktoken
import yaml
from openai import OpenAI

"""
–ü–æ–∑–≤–æ–ª—è–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å –∫–∞–∫ –Ω–∞–ø—Ä—è–º—É—é –ø–æ .json —Å—É–±—Ç–∏—Ç—Ä–∞–º, —Ç–∞–∫ –∏ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏
–∏–∑–≤–ª–µ–∫–∞—Ç—å —Å—É–±—Ç–∏—Ç—Ä—ã –ø–æ URL –∏–∑ —Ñ–∞–π–ª–∞ urls.txt –≤ –∫–æ—Ä–Ω–µ –ø—Ä–æ–µ–∫—Ç–∞.
"""

# –î–æ–±–∞–≤–ª—è–µ–º –∫–æ—Ä–µ–Ω—å –ø—Ä–æ–µ–∫—Ç–∞ (.. –æ—Ç text_processor) –≤ sys.path –¥–ª—è –∏–º–ø–æ—Ä—Ç–∞ —É—Ç–∏–ª–∏—Ç
PROJECT_ROOT = Path(__file__).resolve().parents[1]
if str(PROJECT_ROOT) not in sys.path:
    sys.path.insert(0, str(PROJECT_ROOT))

from env_utils import load_env
from subtitle_extractor.get_subtitles import YouTubeSubtitlesExtractor
from .extractors import (
    SafetyInformationExtractor,
    CausalChainExtractor,
    ConceptHierarchyExtractor,
    CaseStudyExtractor,
    PrerequisiteExtractor
)


def _hms(seconds: Optional[float]) -> Optional[str]:
    if seconds is None:
        return None
    seconds = int(seconds)
    return f"{seconds//3600:02d}:{(seconds%3600)//60:02d}:{seconds%60:02d}"


class SarsekenovProcessor:
    """–°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä –ø–æ–¥ –ª–µ–∫—Ü–∏–∏ –°. –°–∞—Ä—Å–µ–∫–µ–Ω–æ–≤–∞.

    –§–æ–∫—É—Å –Ω–∞ —Ç–µ—Ä–º–∏–Ω–∞—Ö –∏ –∫–æ–Ω—Ü–µ–ø—Ü–∏—è—Ö: –Ω–µ–π—Ä–æ—Å—Ç–∞–ª–∫–∏–Ω–≥, –Ω–µ–æ—Å—Ç–∞–ª–∫–∏–Ω–≥,
    –Ω–∞–±–ª—é–¥–µ–Ω–∏–µ –∑–∞ —É–º–æ–º, –≤–Ω–∏–º–∞–Ω–∏–µ, –º–µ—Ç–∞–Ω–∞–±–ª—é–¥–µ–Ω–∏–µ, –ø–∞—Ç—Ç–µ—Ä–Ω—ã, —Ç—Ä–∏–≥–≥–µ—Ä—ã,
    –ø–æ–ª–µ –≤–Ω–∏–º–∞–Ω–∏—è, –æ—Å–æ–∑–Ω–∞–≤–∞–Ω–∏–µ, –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–µ—Ä–µ–∂–∏–≤–∞–Ω–∏–π, –ø—Ä–∞–∫—Ç–∏–∫–∏ –∏ —É–ø—Ä–∞–∂–Ω–µ–Ω–∏—è.
    """

    def __init__(self, primary_model: str = "gpt-4o-mini", refine_model: str = "gpt-5-mini"):
        load_env()
        self.primary_model = primary_model
        self.refine_model = refine_model
        self.client = OpenAI()
        self.encoding = tiktoken.get_encoding("cl100k_base")
        
        # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏ –∫ OpenAI API (–≤ —Å–µ–∫—É–Ω–¥–∞—Ö)
        # –ú–æ–∂–Ω–æ –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å —á–µ—Ä–µ–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –æ–∫—Ä—É–∂–µ–Ω–∏—è OPENAI_API_DELAY (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 1.0 —Å–µ–∫—É–Ω–¥–∞)
        self.api_delay = float(os.getenv("OPENAI_API_DELAY", "1.0"))

        # –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –¥–ª—è —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä–æ–≤
        self.config = self._load_config()
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä–æ–≤ SAG v2.0
        sag_config = self.config.get('pipeline', {}).get('sag_v2', {})
        models_config = sag_config.get('models', {})
        
        self.safety_extractor = SafetyInformationExtractor(
            client=self.client,
            model=models_config.get('safety', 'gpt-4o-mini')
        ) if sag_config.get('use_safety_extractor', True) else None
        
        self.causal_extractor = CausalChainExtractor(
            llm_client=self.client,
            use_llm=True
        ) if sag_config.get('use_causal_chain_extractor', True) else None
        
        self.hierarchy_extractor = ConceptHierarchyExtractor(
            llm_client=self.client,
            use_llm=True
        ) if sag_config.get('use_concept_hierarchy_extractor', True) else None
        
        self.case_extractor = CaseStudyExtractor(
            client=self.client,
            model=models_config.get('case_study', 'gpt-4o-mini')
        ) if sag_config.get('use_case_study_extractor', True) else None
        
        self.prereq_extractor = PrerequisiteExtractor(
            client=self.client,
            model=models_config.get('prerequisite', 'gpt-4o-mini')
        ) if sag_config.get('use_prerequisite_extractor', True) else None

        # –î–æ–º–µ–Ω–Ω—ã–µ —É–∫–∞–∑–∞–Ω–∏—è –¥–ª—è –±–æ–ª–µ–µ —Ç–æ—á–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ä–µ—á–∏ –°–∞—Ä—Å–µ–∫–µ–Ω–æ–≤–∞
        self.domain_context = (
            "–≠—Ç–æ –ª–µ–∫—Ü–∏—è –°–∞–ª–∞–º–∞—Ç–∞ –°–∞—Ä—Å–µ–∫–µ–Ω–æ–≤–∞ –ø–æ –Ω–µ–π—Ä–æ—Å—Ç–∞–ª–∫–∏–Ω–≥—É/–Ω–µ–æ—Å—Ç–∞–ª–∫–∏–Ω–≥—É. "
            "–°–¢–†–û–ì–û —Å–æ—Ö—Ä–∞–Ω—è–π —Ç–µ—Ä–º–∏–Ω–æ–ª–æ–≥–∏—é –∏ –∞–≤—Ç–æ—Ä—Å–∫–∏–µ —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∏: –Ω–µ–π—Ä–æ—Å—Ç–∞–ª–∫–∏–Ω–≥, –Ω–µ–æ—Å—Ç–∞–ª–∫–∏–Ω–≥, "
            "–ø–æ–ª–µ –≤–Ω–∏–º–∞–Ω–∏—è, –Ω–∞–±–ª—é–¥–µ–Ω–∏–µ –∑–∞ —É–º–æ–º, –º–µ—Ç–∞–Ω–∞–±–ª—é–¥–µ–Ω–∏–µ, –æ—Å–æ–∑–Ω–∞–≤–∞–Ω–∏–µ, –ø–∞—Ç—Ç–µ—Ä–Ω—ã, —Ç—Ä–∏–≥–≥–µ—Ä—ã, "
            "–∞–≤—Ç–æ–º–∞—Ç–∏–∑–º—ã, –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–µ—Ä–µ–∂–∏–≤–∞–Ω–∏–π. "
            "–î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–û: –∏—Å–ø—Ä–∞–≤—å –æ—á–µ–≤–∏–¥–Ω—ã–µ —Ä–µ—á–µ–≤—ã–µ —Å–±–æ–∏ –∏ –≥—Ä–∞–º–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –æ—à–∏–±–∫–∏ "
            "–ë–ï–ó –∏–∑–º–µ–Ω–µ–Ω–∏—è —Å–º—ã—Å–ª–∞. –£–±–µ—Ä–∏ –∏–∑–±—ã—Ç–æ—á–Ω—ã–µ –º–µ–∂–¥–æ–º–µ—Ç–∏—è, –Ω–æ —Å–æ—Ö—Ä–∞–Ω–∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å —Ä–µ—á–∏.")

        # üöÄ –†–ê–°–®–ò–†–ï–ù–ù–ê–Ø –ì–†–ê–§-–ö–û–õ–õ–ï–ö–¶–ò–Ø: 442 —É–∑–ª–∞ + 259 –æ—Ç–Ω–æ—à–µ–Ω–∏–π
        self.graph_nodes = {
            # 1. –ö–û–ù–¶–ï–ü–¢–´ –ù–ï–ô–†–û–°–¢–ê–õ–ö–ò–ù–ì–ê (74 —É–∑–ª–∞)
            "–Ω–µ–π—Ä–æ—Å—Ç–∞–ª–∫–∏–Ω–≥", "–Ω–µ–æ—Å—Ç–∞–ª–∫–∏–Ω–≥", "—Å—Ç–∞–ª–∫–∏–Ω–≥ —É–º–∞", "–º–µ—Ç–∞–Ω–∞–±–ª—é–¥–µ–Ω–∏–µ", "–ø–æ–ª–µ –≤–Ω–∏–º–∞–Ω–∏—è", 
            "–ø–æ–ª–µ –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è", "—Å–≤–æ–±–æ–¥–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ", "—á–∏—Å—Ç–æ–µ –æ—Å–æ–∑–Ω–∞–≤–∞–Ω–∏–µ", "—Å–∞–º–æ-–æ—Å–æ–∑–Ω–∞–≤–∞–Ω–∏–µ",
            "—Å–∞–º–æ–æ—Å–æ–∑–Ω–∞–Ω–∏–µ", "–ø—Ä–∏—Å—É—Ç—Å—Ç–≤–∏–µ", "–∑–¥–µ—Å—å –∏ —Å–µ–π—á–∞—Å", "–∂–∏–≤–æ–µ –ø–µ—Ä–µ–∂–∏–≤–∞–Ω–∏–µ", "–∂–∏–≤–æ–µ –∑–Ω–∞–Ω–∏–µ",
            "–Ω–µ–ø–æ—Å—Ä–µ–¥—Å—Ç–≤–µ–Ω–Ω–æ–µ –≤–æ—Å–ø—Ä–∏—è—Ç–∏–µ", "–∫–≤–∞–Ω—Ç–æ–≤—ã–π —Å–∫–∞—á–æ–∫ —Å–æ–∑–Ω–∞–Ω–∏—è", "–∏–Ω—Ç–µ–≥—Ä–∞—Ç–∏–≤–Ω–∞—è –ø—Å–∏—Ö–∏—á–µ—Å–∫–∞—è —Å–∞–º–æ—Ä–µ–≥—É–ª—è—Ü–∏—è",
            "—Å–∞–º–æ–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ", "—Å–∞–º–æ–ø–æ–∑–Ω–∞–Ω–∏–µ", "—Å—Ç–∞–ª–∫–∏–Ω–≥", "–Ω–∞–±–ª—é–¥–µ–Ω–∏–µ –∑–∞ —É–º–æ–º", "–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ —É–º–∞",
            "—Ä–∞–∑–æ—Ç–æ–∂–¥–µ—Å—Ç–≤–ª–µ–Ω–∏–µ", "—Ü–µ–Ω—Ç—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ", "–æ—Å–æ–∑–Ω–∞–≤–∞–Ω–∏–µ", "–≤–Ω–∏–º–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å", "–æ—Å–æ–∑–Ω–∞–Ω–Ω–æ—Å—Ç—å",
            "mindfulness", "—Å–∞—Ç–∏–ø–∞—Ç—Ç—Ö–∞–Ω–∞", "–≤–∏–ø–∞—à—å—è–Ω–∞", "—à–∞–º–∞—Ç—Ö–∞", "–º–µ–¥–∏—Ç–∞—Ü–∏—è", "—Å–æ–∑–µ—Ä—Ü–∞–Ω–∏–µ", "—Å–∞–º–æ—Å–æ–∑–µ—Ä—Ü–∞–Ω–∏–µ",
            "–ø—Ä–æ–±—É–∂–¥–µ–Ω–∏–µ", "–ø—Ä–æ—Å–≤–µ—Ç–ª–µ–Ω–∏–µ", "–±–æ–¥—Ö–∏", "–Ω–∏—Ä–≤–∞–Ω–∞", "—Å–∞–º–∞–¥—Ö–∏", "—Å–∞—Ç–æ—Ä–∏", "–º–æ–∫—à–∞", "–æ—Å–≤–æ–±–æ–∂–¥–µ–Ω–∏–µ",
            "–µ–¥–∏–Ω—Å—Ç–≤–æ", "–Ω–µ–¥—É–∞–ª—å–Ω–æ—Å—Ç—å", "–∞–¥–≤–∞–π—Ç–∞", "–±–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å", "–∞–±—Å–æ–ª—é—Ç", "–∏—Å—Ç–∏–Ω–∞", "—Å—É—â–Ω–æ—Å—Ç—å",
            "–¥—É—Ö", "–¥—É—à–∞", "—Å–æ–∑–Ω–∞–Ω–∏–µ", "—á–∏—Å—Ç–æ–µ —Å–æ–∑–Ω–∞–Ω–∏–µ", "–±—ã—Ç–∏–µ", "–µ—Å—Ç—å-–Ω–æ—Å—Ç—å", "–ø—Ä–∏—Å—É—Ç—Å—Ç–≤–∏–µ –±—ã—Ç–∏—è",
            "–ª–∏—á–Ω–æ—Å—Ç—å", "—ç–≥–æ", "–ª–æ–∂–Ω–æ–µ —è", "–∏—Å—Ç–∏–Ω–Ω–æ–µ —è", "–≤—ã—Å—à–µ–µ —è", "—Å–∞–º–æ—Å—Ç—å", "–∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω–æ—Å—Ç—å",
            "–∏–¥–µ–Ω—Ç–∏—á–Ω–æ—Å—Ç—å", "—Å—É–±—ä–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å", "–æ–±—ä–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å", "–ø—Ä–æ–µ–∫—Ü–∏—è", "–∏–Ω—Ç—Ä–æ–µ–∫—Ü–∏—è", "—Ç—Ä–∞–Ω—Å—Ñ–µ—Ä–µ–Ω—Ü–∏—è",
            "–∫–æ–Ω—Ç—Ä—Ç—Ä–∞–Ω—Å—Ñ–µ—Ä–µ–Ω—Ü–∏—è", "–∑–∞—â–∏—Ç–Ω—ã–µ –º–µ—Ö–∞–Ω–∏–∑–º—ã", "–∫–æ–ø–∏–Ω–≥-—Å—Ç—Ä–∞—Ç–µ–≥–∏–∏", "–∞–¥–∞–ø—Ç–∞—Ü–∏—è", "–∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è",
            
            # 2. –ü–°–ò–•–û–õ–û–ì–ò–ß–ï–°–ö–ò–ï –°–û–°–¢–û–Ø–ù–ò–Ø (94 —É–∑–ª–∞)
            "—Å—Ç—Ä–∞–¥–∞–Ω–∏–µ", "–±–æ–ª—å", "–ø–µ—á–∞–ª—å", "–≥–æ—Ä–µ", "–¥–µ–ø—Ä–µ—Å—Å–∏—è", "—Ç—Ä–µ–≤–æ–≥–∞", "—Å—Ç—Ä–∞—Ö", "–ø–∞–Ω–∏–∫–∞", "—É–∂–∞—Å",
            "–æ–±–∏–¥–∞", "–≥–Ω–µ–≤", "—è—Ä–æ—Å—Ç—å", "–Ω–µ–Ω–∞–≤–∏—Å—Ç—å", "–∑–∞–≤–∏—Å—Ç—å", "—Ä–µ–≤–Ω–æ—Å—Ç—å", "–≤–∏–Ω–∞", "—Å—Ç—ã–¥", "—Å–º—É—â–µ–Ω–∏–µ",
            "–æ—Ç–≤—Ä–∞—â–µ–Ω–∏–µ", "–ø—Ä–µ–∑—Ä–µ–Ω–∏–µ", "–æ–¥–∏–Ω–æ—á–µ—Å—Ç–≤–æ", "–∏–∑–æ–ª—è—Ü–∏—è", "–æ—Ç—á—É–∂–¥–µ–Ω–∏–µ", "–æ–ø—É—Å—Ç–æ—à–µ–Ω–Ω–æ—Å—Ç—å", "–±–µ–∑–Ω–∞–¥–µ–∂–Ω–æ—Å—Ç—å",
            "–±–µ—Å—Å–º—ã—Å–ª–µ–Ω–Ω–æ—Å—Ç—å", "—Å–∫—É–∫–∞", "–∞–ø–∞—Ç–∏—è", "—É—Å—Ç–∞–ª–æ—Å—Ç—å", "–∏—Å—Ç–æ—â–µ–Ω–∏–µ", "–≤—ã–≥–æ—Ä–∞–Ω–∏–µ", "—Ä–∞–¥–æ—Å—Ç—å", "—Å—á–∞—Å—Ç—å–µ",
            "–±–ª–∞–∂–µ–Ω—Å—Ç–≤–æ", "–≤–æ—Å—Ç–æ—Ä–≥", "–≤–æ—Å—Ö–∏—â–µ–Ω–∏–µ", "–ª—é–±–æ–≤—å", "—Å–æ—Å—Ç—Ä–∞–¥–∞–Ω–∏–µ", "—ç–º–ø–∞—Ç–∏—è", "–±–ª–∞–≥–æ–¥–∞—Ä–Ω–æ—Å—Ç—å",
            "–ø—Ä–æ—â–µ–Ω–∏–µ", "–ø—Ä–∏–Ω—è—Ç–∏–µ", "–¥–æ–≤–µ—Ä–∏–µ", "–≤–µ—Ä–∞", "–Ω–∞–¥–µ–∂–¥–∞", "–æ–ø—Ç–∏–º–∏–∑–º", "—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å", "–ø–æ–∫–æ–π",
            "—É–º–∏—Ä–æ—Ç–≤–æ—Ä–µ–Ω–∏–µ", "–≥–∞—Ä–º–æ–Ω–∏—è", "–±–∞–ª–∞–Ω—Å", "—Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç—å", "–µ–¥–∏–Ω–µ–Ω–∏–µ", "—Å–≤–æ–±–æ–¥–∞", "–ª–µ–≥–∫–æ—Å—Ç—å",
            "—Å–ø–æ–Ω—Ç–∞–Ω–Ω–æ—Å—Ç—å", "–∏–≥—Ä–∏–≤–æ—Å—Ç—å", "—Ç–≤–æ—Ä—á–µ—Å—Ç–≤–æ", "–∏–Ω—Å–∞–π—Ç", "–ø—Ä–æ–∑—Ä–µ–Ω–∏–µ", "–æ–∑–∞—Ä–µ–Ω–∏–µ", "–æ—Ç–∫—Ä–æ–≤–µ–Ω–∏–µ",
            "–ø–æ–Ω–∏–º–∞–Ω–∏–µ", "—è—Å–Ω–æ—Å—Ç—å", "–æ—Å–æ–∑–Ω–∞–Ω–∏–µ", "–ø—Ä–æ–±—É–∂–¥–µ–Ω–∏–µ", "–ø—Ä–æ—Å–≤–µ—Ç–ª–µ–Ω–∏–µ", "—Ç—Ä–∞–Ω—Å—Ü–µ–Ω–¥–µ–Ω—Ü–∏—è", "—ç–∫—Å—Ç–∞–∑",
            "—Å–∞–º–∞–¥—Ö–∏", "—Å–∞—Ç–æ—Ä–∏", "–º–∏—Å—Ç–∏—á–µ—Å–∫–∏–π –æ–ø—ã—Ç", "–ø–∏–∫–æ–≤–æ–µ –ø–µ—Ä–µ–∂–∏–≤–∞–Ω–∏–µ", "–ø–æ—Ç–æ–∫", "–ø—Ä–∏—Å—É—Ç—Å—Ç–≤–∏–µ",
            "—Ü–µ–Ω—Ç—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ—Å—Ç—å", "–∑–∞–∑–µ–º–ª–µ–Ω–Ω–æ—Å—Ç—å", "—Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å", "–∂–∏–≤–æ—Å—Ç—å", "–ø–æ–¥–ª–∏–Ω–Ω–æ—Å—Ç—å", "–∏—Å–∫—Ä–µ–Ω–Ω–æ—Å—Ç—å",
            "—á–µ—Å—Ç–Ω–æ—Å—Ç—å", "–æ—Ç–∫—Ä—ã—Ç–æ—Å—Ç—å", "—É—è–∑–≤–∏–º–æ—Å—Ç—å", "—Å–º–µ–ª–æ—Å—Ç—å", "–º—É–∂–µ—Å—Ç–≤–æ", "–≥–µ—Ä–æ–∏–∑–º", "–ø–æ–∑–µ—Ä—Å—Ç–≤–æ",
            "—Ñ–∞–ª—å—à—å", "–º–∞—Å–∫–∏—Ä–æ–≤–∫–∞", "–∑–∞—â–∏—â–µ–Ω–Ω–æ—Å—Ç—å", "–∑–∞–∫—Ä—ã—Ç–æ—Å—Ç—å", "—Å–æ–ø—Ä–æ—Ç–∏–≤–ª–µ–Ω–∏–µ", "–∫–æ–Ω—Ç—Ä–æ–ª—å",
            
            # 3. –ü–†–û–¶–ï–°–°–´ –¢–†–ê–ù–°–§–û–†–ú–ê–¶–ò–ò (101 —É–∑–µ–ª)
            "–ø–æ–∑–Ω–∞–Ω–∏–µ", "–∏–∑—É—á–µ–Ω–∏–µ", "–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ", "–∞–Ω–∞–ª–∏–∑", "—Å–∏–Ω—Ç–µ–∑", "–∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è", "–¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏–∞—Ü–∏—è",
            "—Ä–∞–∑–ª–∏—á–µ–Ω–∏–µ", "—Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ", "—É–∑–Ω–∞–≤–∞–Ω–∏–µ", "–æ–ø–æ–∑–Ω–∞–Ω–∏–µ", "–ø–æ–Ω–∏–º–∞–Ω–∏–µ", "–ø–æ—Å—Ç–∏–∂–µ–Ω–∏–µ", "–æ—Å–º—ã—Å–ª–µ–Ω–∏–µ",
            "–æ—Å–æ–∑–Ω–∞–Ω–∏–µ", "–ø—Ä–æ–∑—Ä–µ–Ω–∏–µ", "–∏–Ω—Å–∞–π—Ç", "–∏–Ω—Ç—É–∏—Ü–∏—è", "–ø—Ä–µ–¥—á—É–≤—Å—Ç–≤–∏–µ", "–ø—Ä–µ–¥–≤–∏–¥–µ–Ω–∏–µ", "—è—Å–Ω–æ–≤–∏–¥–µ–Ω–∏–µ",
            "–∏–∑–º–µ–Ω–µ–Ω–∏–µ", "—Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—è", "—ç–≤–æ–ª—é—Ü–∏—è", "—Ä–∞–∑–≤–∏—Ç–∏–µ", "—Ä–æ—Å—Ç", "—Å–æ–∑—Ä–µ–≤–∞–Ω–∏–µ", "–ø—Ä–æ–≥—Ä–µ—Å—Å", "—Ä–µ–≥—Ä–µ—Å—Å",
            "–¥–µ–≥—Ä–∞–¥–∞—Ü–∏—è", "—Ä–∞—Å–ø–∞–¥", "—Ä–∞–∑—Ä—É—à–µ–Ω–∏–µ", "—Å–º–µ—Ä—Ç—å", "–≤–æ–∑—Ä–æ–∂–¥–µ–Ω–∏–µ", "–≤–æ—Å–∫—Ä–µ—à–µ–Ω–∏–µ", "—Ä–µ–∏–Ω–∫–∞—Ä–Ω–∞—Ü–∏—è",
            "–ø–µ—Ä–µ—Ä–æ–∂–¥–µ–Ω–∏–µ", "–º–µ—Ç–∞–º–æ—Ä—Ñ–æ–∑–∞", "–∞–ª—Ö–∏–º–∏—è", "—Ç—Ä–∞–Ω—Å–º—É—Ç–∞—Ü–∏—è", "—Å—É–±–ª–∏–º–∞—Ü–∏—è", "–æ—á–∏—â–µ–Ω–∏–µ", "–∫–∞—Ç–∞—Ä—Å–∏—Å",
            "–∏—Å—Ü–µ–ª–µ–Ω–∏–µ", "–∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –æ–ø—ã—Ç–∞", "—Å–∏–Ω—Ç–µ–∑", "–æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ", "—Å–ª–∏—è–Ω–∏–µ", "–µ–¥–∏–Ω–µ–Ω–∏–µ", "–≥–∞—Ä–º–æ–Ω–∏–∑–∞—Ü–∏—è",
            "–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞", "—Å—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏—è", "—Ü–µ–Ω—Ç—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ", "–∑–∞–∑–µ–º–ª–µ–Ω–∏–µ", "—É–∫–æ—Ä–µ–Ω–µ–Ω–∏–µ", "–æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ",
            "—É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ", "–ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ", "–≤–∞–ª–∏–¥–∞—Ü–∏—è", "–ø—Ä–∏–∑–Ω–∞–Ω–∏–µ", "–ø—Ä–∏–Ω—è—Ç–∏–µ", "–æ–¥–æ–±—Ä–µ–Ω–∏–µ", "–ø–æ–¥–¥–µ—Ä–∂–∫–∞",
            "–ø–æ–æ—â—Ä–µ–Ω–∏–µ", "–ø–æ—Ö–≤–∞–ª–∞", "–æ—Å–≤–æ–±–æ–∂–¥–µ–Ω–∏–µ", "–æ—Ç–ø—É—Å–∫–∞–Ω–∏–µ", "–æ—Ç–¥–∞—á–∞", "—Å–¥–∞—á–∞", "–∫–∞–ø–∏—Ç—É–ª—è—Ü–∏—è", "—Å–º–∏—Ä–µ–Ω–∏–µ",
            "–ø–æ–∫–æ—Ä–Ω–æ—Å—Ç—å", "–ø–æ—Å–ª—É—à–∞–Ω–∏–µ", "–¥–æ–≤–µ—Ä–∏–µ", "–≤–µ—Ä–∞", "–ø—Ä–µ–¥–∞–Ω–Ω–æ—Å—Ç—å", "—Å–ª—É–∂–µ–Ω–∏–µ", "–∂–µ—Ä—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å",
            "–∞–ª—å—Ç—Ä—É–∏–∑–º", "–±–µ—Å–∫–æ—Ä—ã—Å—Ç–∏–µ", "–±–µ–∑—É—Å–ª–æ–≤–Ω–æ—Å—Ç—å", "–Ω–µ–ø—Ä–∏–≤—è–∑–∞–Ω–Ω–æ—Å—Ç—å", "–æ—Ç—Ä–µ—à–µ–Ω–Ω–æ—Å—Ç—å", "–±–µ–∑—Ä–∞–∑–ª–∏—á–∏–µ",
            "—Ä–∞–≤–Ω–æ–¥—É—à–∏–µ", "–Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ—Å—Ç—å", "–æ–±—ä–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å",
            
            # 4. –ü–†–ê–ö–¢–ò–ö–ò –ò –¢–ï–•–ù–ò–ö–ò (77 —É–∑–ª–æ–≤)
            "–º–µ—Ç–∞–Ω–∞–±–ª—é–¥–µ–Ω–∏–µ", "–Ω–∞–±–ª—é–¥–µ–Ω–∏–µ –∑–∞ –Ω–∞–±–ª—é–¥–∞—é—â–∏–º", "—Å—Ç–∞–ª–∫–∏–Ω–≥ —É–º–∞", "–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–ª—è –≤–Ω–∏–º–∞–Ω–∏—è",
            "—Ä–∞–±–æ—Ç–∞ —Å –ø—Ä–∏—Å—É—Ç—Å—Ç–≤–∏–µ–º", "—Ü–µ–Ω—Ç—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ", "–∑–∞–∑–µ–º–ª–µ–Ω–∏–µ", "–¥—ã—Ö–∞—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–∞–∫—Ç–∏–∫–∏", "—Ç–µ–ª–µ—Å–Ω—ã–µ –ø—Ä–∞–∫—Ç–∏–∫–∏",
            "–æ—Å–æ–∑–Ω–∞–Ω–Ω–æ—Å—Ç—å —Ç–µ–ª–∞", "–º–µ–¥–∏—Ç–∞—Ü–∏—è", "–≤–∏–ø–∞—à—å—è–Ω–∞", "—à–∞–º–∞—Ç—Ö–∞", "–¥–∑–µ–Ω", "–¥–∑–∞–¥–∑–µ–Ω", "–∫–æ–∞–Ω", "–º–∞–Ω—Ç—Ä–∞",
            "–¥–∂–∞–ø–∞", "–∫–∏—Ä—Ç–∞–Ω", "–±—Ö–∞–¥–∂–∞–Ω", "–ø—Ä–∞–Ω–∞—è–º–∞", "–∞—Å–∞–Ω–∞", "–º—É–¥—Ä–∞", "–±–∞–Ω–¥—Ö–∞", "—Ç—Ä–∞—Ç–∞–∫–∞", "—è–Ω—Ç—Ä–∞",
            "–º–∞–Ω–¥–∞–ª–∞", "–≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è", "–≤–æ–æ–±—Ä–∞–∂–µ–Ω–∏–µ", "—Ñ–∞–Ω—Ç–∞–∑–∏—è", "–ø—Å–∏—Ö–æ–∞–Ω–∞–ª–∏–∑", "—Å–≤–æ–±–æ–¥–Ω—ã–µ –∞—Å—Å–æ—Ü–∏–∞—Ü–∏–∏",
            "–∞–Ω–∞–ª–∏–∑ —Å–Ω–æ–≤–∏–¥–µ–Ω–∏–π", "—Ä–∞–±–æ—Ç–∞ —Å —Ç–µ–Ω—å—é", "–∞–∫—Ç–∏–≤–Ω–æ–µ –≤–æ–æ–±—Ä–∞–∂–µ–Ω–∏–µ", "–ø–µ—Å–æ—á–Ω–∞—è —Ç–µ—Ä–∞–ø–∏—è", "–∞—Ä—Ç-—Ç–µ—Ä–∞–ø–∏—è",
            "—Ç–∞–Ω—Ü–µ–≤–∞–ª—å–Ω–∞—è —Ç–µ—Ä–∞–ø–∏—è", "–¥—Ä–∞–º–∞—Ç–µ—Ä–∞–ø–∏—è", "–º—É–∑—ã–∫–∞–ª—å–Ω–∞—è —Ç–µ—Ä–∞–ø–∏—è", "–±–∏–±–ª–∏–æ—Ç–µ—Ä–∞–ø–∏—è", "–≥–µ—à—Ç–∞–ª—å—Ç-—Ç–µ—Ä–∞–ø–∏—è",
            "—Ç–µ–ª–µ—Å–Ω–æ-–æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Ç–µ—Ä–∞–ø–∏—è", "—Å–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∞–∫—Ç–∏–∫–∏", "–±–∏–æ—ç–Ω–µ—Ä–≥–µ—Ç–∏–∫–∞", "—Å–∞–º–æ–Ω–∞–±–ª—é–¥–µ–Ω–∏–µ",
            "—Å–∞–º–æ–∞–Ω–∞–ª–∏–∑", "—Å–∞–º–æ—Ä–µ—Ñ–ª–µ–∫—Å–∏—è", "—Å–∞–º–æ–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ", "—Å–∞–º–æ–ø–æ–∑–Ω–∞–Ω–∏–µ", "—Å–∞–º–æ–∏–∑—É—á–µ–Ω–∏–µ",
            "—Å–∞–º–æ—ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ", "—Å–∞–º–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ", "—Å–∞–º–æ–æ—Ü–µ–Ω–∫–∞", "—Å–∞–º–æ–∫—Ä–∏—Ç–∏–∫–∞", "—Å–∞–º–æ–ø—Ä–∏–Ω—è—Ç–∏–µ",
            "—Å–∞–º–æ–ø—Ä–æ—â–µ–Ω–∏–µ", "—Å–∞–º–æ–ª—é–±–æ–≤—å", "—Å–∞–º–æ—É–≤–∞–∂–µ–Ω–∏–µ", "—Å–∞–º–æ—Ü–µ–Ω–Ω–æ—Å—Ç—å", "—Å–∞–º–æ—É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ", "—Å–∞–º–æ—Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è",
            
            # 5. –ò–ù–°–¢–†–£–ú–ï–ù–¢–´ –ò –ú–ï–¢–û–î–´ (65 —É–∑–ª–æ–≤)
            "–≤–Ω–∏–º–∞–Ω–∏–µ", "–æ—Å–æ–∑–Ω–∞–Ω–Ω–æ—Å—Ç—å", "–ø—Ä–∏—Å—É—Ç—Å—Ç–≤–∏–µ", "–Ω–∞–±–ª—é–¥–µ–Ω–∏–µ", "—Å–≤–∏–¥–µ—Ç–µ–ª—å—Å—Ç–≤–æ–≤–∞–Ω–∏–µ", "—Å–æ–∑–µ—Ä—Ü–∞–Ω–∏–µ",
            "–º–µ–¥–∏—Ç–∞—Ü–∏—è", "–∫–æ–Ω—Ü–µ–Ω—Ç—Ä–∞—Ü–∏—è", "—Ñ–æ–∫—É—Å", "—Ü–µ–Ω—Ç—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ", "–∑–∞–∑–µ–º–ª–µ–Ω–∏–µ", "—É–∫–æ—Ä–µ–Ω–µ–Ω–∏–µ", "—Å—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏—è",
            "–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞", "–≥–∞—Ä–º–æ–Ω–∏–∑–∞—Ü–∏—è", "–æ—Å—Ç–∞–Ω–æ–≤–∫–∞ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–≥–æ –¥–∏–∞–ª–æ–≥–∞", "–º–æ–ª—á–∞–Ω–∏–µ —É–º–∞", "–ø—É—Å—Ç–æ—Ç–∞",
            "—Ç–∏—à–∏–Ω–∞", "–ø–æ–∫–æ–π", "–Ω–µ–ø–æ–¥–≤–∏–∂–Ω–æ—Å—Ç—å", "—Å—Ç–∞—Ç–∏—á–Ω–æ—Å—Ç—å", "–¥–∏–Ω–∞–º–∏—á–Ω–æ—Å—Ç—å", "–¥–≤–∏–∂–µ–Ω–∏–µ", "–ø–æ—Ç–æ–∫", "—Ä–∏—Ç–º",
            "–ø—É–ª—å—Å–∞—Ü–∏—è", "–≤–∏–±—Ä–∞—Ü–∏—è", "—Ä–µ–∑–æ–Ω–∞–Ω—Å", "—Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è", "–Ω–∞—Å—Ç—Ä–æ–π–∫–∞", "–¥—ã—Ö–∞–Ω–∏–µ", "–ø—Ä–∞–Ω–∞—è–º–∞",
            "—Ö–æ–ª–æ—Ç—Ä–æ–ø–Ω–æ–µ –¥—ã—Ö–∞–Ω–∏–µ", "—Ä–µ–±—ë—Ñ–∏–Ω–≥", "–≤–∞–π–≤–µ–π—à–Ω", "—Ç–µ–ª–µ—Å–Ω—ã–µ –ø—Ä–∞–∫—Ç–∏–∫–∏", "–π–æ–≥–∞", "—Ç–∞–π-—á–∏", "—Ü–∏–≥—É–Ω",
            "–∞–π–∫–∏–¥–æ", "–∫–∞—Ä–∞—Ç–µ", "—Ç–∞–Ω–µ—Ü", "–¥–≤–∏–∂–µ–Ω–∏–µ", "–º–∞—Å—Å–∞–∂", "–ø—Ä–∏–∫–æ—Å–Ω–æ–≤–µ–Ω–∏–µ", "–æ–±—ä—è—Ç–∏–µ", "–∫–æ–Ω—Ç–∞–∫—Ç",
            "—Ä–∏—Å–æ–≤–∞–Ω–∏–µ", "–∂–∏–≤–æ–ø–∏—Å—å", "—Å–∫—É–ª—å–ø—Ç—É—Ä–∞", "–ª–µ–ø–∫–∞", "–º—É–∑—ã–∫–∞", "–ø–µ–Ω–∏–µ", "–∏–≥—Ä–∞ –Ω–∞ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞—Ö",
            "–ø–æ—ç–∑–∏—è", "–ø—Ä–æ–∑–∞", "–∂—É—Ä–Ω–∞–ª–∏—Ä–æ–≤–∞–Ω–∏–µ", "–¥–Ω–µ–≤–Ω–∏–∫", "–ø–∏—Å—å–º–æ", "–∫–∞–ª–ª–∏–≥—Ä–∞—Ñ–∏—è", "—Ç–µ–∞—Ç—Ä", "–¥—Ä–∞–º–∞",
            "–∏–º–ø—Ä–æ–≤–∏–∑–∞—Ü–∏—è", "–∏–≥—Ä–∞",
            
            # 6. –°–ü–ï–¶–ò–ê–õ–¨–ù–´–ï –£–ó–õ–´ –î–õ–Ø SAG-–°–ò–°–¢–ï–ú–´ (31 —É–∑–µ–ª)
            "–∞–≤—Ç–æ—Ä", "—Å–∏—Å—Ç–µ–º–∞", "—Ü–µ–ª—å", "–∫–æ–Ω—Ç–µ–∫—Å—Ç", "–∞—É–¥–∏—Ç–æ—Ä–∏—è", "–ø–æ–ª—è—Ä–Ω–æ—Å—Ç—å", "—É—Ä–æ–≤–µ–Ω—å —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏",
            "—Ñ–∏–∑–∏—á–µ—Å–∫–∏–π –ø–ª–∞–Ω", "—ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π –ø–ª–∞–Ω", "–º–µ–Ω—Ç–∞–ª—å–Ω—ã–π –ø–ª–∞–Ω", "–∏–Ω—Ç—É–∏—Ç–∏–≤–Ω—ã–π –ø–ª–∞–Ω", "–¥—É—Ö–æ–≤–Ω—ã–π –ø–ª–∞–Ω",
            "–∫–∞—É–∑–∞–ª—å–Ω—ã–π –ø–ª–∞–Ω", "–±—É–¥–¥—Ö–∏—á–µ—Å–∫–∏–π –ø–ª–∞–Ω", "–∞—Ç–º–∏—á–µ—Å–∫–∏–π –ø–ª–∞–Ω", "–ª–æ–≥–æ–∏—á–µ—Å–∫–∏–π –ø–ª–∞–Ω", "—Å–æ–∑–Ω–∞—Ç–µ–ª—å–Ω–æ–µ",
            "–±–µ—Å—Å–æ–∑–Ω–∞—Ç–µ–ª—å–Ω–æ–µ", "–æ—Å–æ–∑–Ω–∞–Ω–Ω–æ–µ", "–Ω–µ–æ—Å–æ–∑–Ω–∞–Ω–Ω–æ–µ", "–∂–∏–≤–æ–µ", "–º–µ—Ä—Ç–≤–æ–µ", "–ø–æ–¥–ª–∏–Ω–Ω–æ–µ", "—Ñ–∞–ª—å—à–∏–≤–æ–µ",
            "—Å–ø–æ–Ω—Ç–∞–Ω–Ω–æ–µ", "–∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º–æ–µ", "–µ–¥–∏–Ω—Å—Ç–≤–æ", "–¥–≤–æ–π—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å", "—Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç—å", "—Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞—Ä–Ω–æ—Å—Ç—å",
            "–∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è", "–¥–µ–∑–∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è", "–≥–∞—Ä–º–æ–Ω–∏—è", "–¥–∏—Å–≥–∞—Ä–º–æ–Ω–∏—è", "–ø–æ—Ä—è–¥–æ–∫", "—Ö–∞–æ—Å"
        }

        # üîó –°–ï–ú–ê–ù–¢–ò–ß–ï–°–ö–ò–ï –û–¢–ù–û–®–ï–ù–ò–Ø (259 —ç–ª–µ–º–µ–Ω—Ç–æ–≤)
        self.graph_relationships = {
            # –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ –æ—Ç–Ω–æ—à–µ–Ω–∏—è (33)
            "–≤–∫–ª—é—á–∞–µ—Ç_–≤_—Å–µ–±—è", "—Å–æ–¥–µ—Ä–∂–∏—Ç", "—Å–æ—Å—Ç–æ–∏—Ç_–∏–∑", "—è–≤–ª—è–µ—Ç—Å—è_—á–∞—Å—Ç—å—é", "–æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç—Å—è_–∫–∞–∫",
            "–æ–∑–Ω–∞—á–∞–µ—Ç", "–ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç", "—Å–∏–º–≤–æ–ª–∏–∑–∏—Ä—É–µ—Ç", "–≤—ã—Ä–∞–∂–∞–µ—Ç", "–ø—Ä–æ—è–≤–ª—è–µ—Ç—Å—è_–∫–∞–∫",
            "–≤–æ–ø–ª–æ—â–∞–µ—Ç—Å—è_–≤", "—Ä–µ–∞–ª–∏–∑—É–µ—Ç—Å—è_—á–µ—Ä–µ–∑", "–ø–æ—Ö–æ–∂–µ_–Ω–∞", "–∞–Ω–∞–ª–æ–≥–∏—á–Ω–æ", "–ø–æ–¥–æ–±–Ω–æ", "—Å—Ö–æ–∂–µ_—Å",
            "–Ω–∞–ø–æ–º–∏–Ω–∞–µ—Ç", "–ø—Ä–æ—Ç–∏–≤–æ–ø–æ–ª–æ–∂–Ω–æ", "–∫–æ–Ω—Ç—Ä–∞—Å—Ç–∏—Ä—É–µ—Ç_—Å", "–æ—Ç–ª–∏—á–∞–µ—Ç—Å—è_–æ—Ç", "–Ω–µ_—è–≤–ª—è–µ—Ç—Å—è",
            "–ø—Ä–æ—Ç–∏–≤–æ—Å—Ç–æ–∏—Ç", "–∫–æ–Ω—Ñ–ª–∏–∫—Ç—É–µ—Ç_—Å", "–Ω–µ—Å–æ–≤–º–µ—Å—Ç–∏–º–æ_—Å", "—Å–ª–µ–¥—É–µ—Ç_–∏–∑", "–≤—ã—Ç–µ–∫–∞–µ—Ç_–∏–∑",
            "–ª–æ–≥–∏—á–µ—Å–∫–∏_—Å–≤—è–∑–∞–Ω–æ", "–ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ—Ç", "–ø–æ–¥—Ä–∞–∑—É–º–µ–≤–∞–µ—Ç", "–∏–º–ø–ª–∏—Ü–∏—Ä—É–µ—Ç", "–∏—Å–∫–ª—é—á–∞–µ—Ç",
            "–æ—Ç—Ä–∏—Ü–∞–µ—Ç", "–æ–ø—Ä–æ–≤–µ—Ä–≥–∞–µ—Ç",
            
            # –ö–∞—É–∑–∞–ª—å–Ω—ã–µ –æ—Ç–Ω–æ—à–µ–Ω–∏—è (40)
            "–ø—Ä–∏—á–∏–Ω–∞", "—Å–ª–µ–¥—Å—Ç–≤–∏–µ", "–≤–µ–¥–µ—Ç_–∫", "–ø—Ä–∏–≤–æ–¥–∏—Ç_–∫", "—Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏—Ä—É–µ—Ç—Å—è_–≤", "–≤—ã–∑—ã–≤–∞–µ—Ç",
            "–ø–æ—Ä–æ–∂–¥–∞–µ—Ç", "—Å–æ–∑–¥–∞–µ—Ç", "–≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç", "–ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç", "–∏–Ω–∏—Ü–∏–∏—Ä—É–µ—Ç", "–∑–∞–ø—É—Å–∫–∞–µ—Ç",
            "–∞–∫—Ç–∏–≤–∏—Ä—É–µ—Ç", "—Å—Ç–∏–º—É–ª–∏—Ä—É–µ—Ç", "–ø—Ä–æ–≤–æ—Ü–∏—Ä—É–µ—Ç", "–ø—Ä–µ–ø—è—Ç—Å—Ç–≤—É–µ—Ç", "–±–ª–æ–∫–∏—Ä—É–µ—Ç", "–º–µ—à–∞–µ—Ç",
            "—Ç–æ—Ä–º–æ–∑–∏—Ç", "–∑–∞–º–µ–¥–ª—è–µ—Ç", "–æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç", "–ø—Ä–µ—Ä—ã–≤–∞–µ—Ç", "–Ω–∞—Ä—É—à–∞–µ—Ç", "–∏—Å–∫–∞–∂–∞–µ—Ç",
            "–ø–æ–¥–∞–≤–ª—è–µ—Ç", "–∏–Ω–≥–∏–±–∏—Ä—É–µ—Ç", "–¥–µ–∞–∫—Ç–∏–≤–∏—Ä—É–µ—Ç", "–Ω–µ–π—Ç—Ä–∞–ª–∏–∑—É–µ—Ç", "–∞–Ω–Ω—É–ª–∏—Ä—É–µ—Ç",
            "—Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∏—Ä—É–µ—Ç—Å—è_–≤", "–ø—Ä–µ–≤—Ä–∞—â–∞–µ—Ç—Å—è_–≤", "–ø–µ—Ä–µ—Ö–æ–¥–∏—Ç_–≤", "—Å—Ç–∞–Ω–æ–≤–∏—Ç—Å—è", "—Ä–∞–∑–≤–∏–≤–∞–µ—Ç—Å—è_–≤",
            "—ç–≤–æ–ª—é—Ü–∏–æ–Ω–∏—Ä—É–µ—Ç_–≤", "–ø–µ—Ä–µ—Ä–∞—Å—Ç–∞–µ—Ç_–≤", "–º–æ–¥–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç—Å—è_–≤", "–ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç—Å—è_–≤",
            "–º—É—Ç–∏—Ä—É–µ—Ç_–≤", "–º–µ—Ç–∞–º–æ—Ä—Ñ–∏–∑–∏—Ä—É–µ—Ç—Å—è_–≤",
            
            # –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –æ—Ç–Ω–æ—à–µ–Ω–∏—è (39)
            "–∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è_–¥–ª—è", "–ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è_–≤", "—Å–ª—É–∂–∏—Ç_–¥–ª—è", "–ø—Ä–µ–¥–Ω–∞–∑–Ω–∞—á–µ–Ω–æ_–¥–ª—è", "–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–æ_–Ω–∞",
            "–æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–æ_–Ω–∞", "—Ñ–æ–∫—É—Å–∏—Ä—É–µ—Ç—Å—è_–Ω–∞", "–Ω–∞—Ü–µ–ª–µ–Ω–æ_–Ω–∞", "—Ä–∞–±–æ—Ç–∞–µ—Ç_—Å", "–≤–æ–∑–¥–µ–π—Å—Ç–≤—É–µ—Ç_–Ω–∞",
            "–≤–ª–∏—è–µ—Ç_–Ω–∞", "–º–æ–¥–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç", "–∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç_–¥–ª—è", "–º–µ—Ç–æ–¥_–¥–ª—è", "—Å–ø–æ—Å–æ–±_–¥–ª—è", "—Å—Ä–µ–¥—Å—Ç–≤–æ_–¥–ª—è",
            "—Ç–µ—Ö–Ω–∏–∫–∞_–¥–ª—è", "–ø—Ä–∞–∫—Ç–∏–∫–∞_–¥–ª—è", "—É–ø—Ä–∞–∂–Ω–µ–Ω–∏–µ_–¥–ª—è", "–º–µ–¥–∏—Ç–∞—Ü–∏—è_–¥–ª—è", "—Ç–µ—Ä–∞–ø–∏—è_–¥–ª—è",
            "–ª–µ—á–µ–Ω–∏–µ_–¥–ª—è", "–∏—Å—Ü–µ–ª–µ–Ω–∏–µ_–¥–ª—è", "—Ä–∞–∑–≤–∏—Ç–∏–µ_–¥–ª—è", "—Ç—Ä–µ–±—É–µ—Ç", "–Ω—É–∂–¥–∞–µ—Ç—Å—è_–≤", "–∑–∞–≤–∏—Å–∏—Ç_–æ—Ç",
            "–æ—Å–Ω–æ–≤–∞–Ω–æ_–Ω–∞", "—Å—Ç—Ä–æ–∏—Ç—Å—è_–Ω–∞", "–ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ—Ç", "–ø–æ–¥—Ä–∞–∑—É–º–µ–≤–∞–µ—Ç", "–≤–∫–ª—é—á–∞–µ—Ç", "–∏—Å–∫–ª—é—á–∞–µ—Ç",
            "–¥–æ–ø—É—Å–∫–∞–µ—Ç", "—É—Å–∏–ª–∏–≤–∞–µ—Ç", "–æ—Å–ª–∞–±–ª—è–µ—Ç", "–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç", "–ø–æ–¥—Ä—ã–≤–∞–µ—Ç", "—É–∫—Ä–µ–ø–ª—è–µ—Ç",
            
            # –í—Ä–µ–º–µ–Ω–Ω—ã–µ –æ—Ç–Ω–æ—à–µ–Ω–∏—è (36)
            "–ø—Ä–µ–¥—à–µ—Å—Ç–≤—É–µ—Ç", "—Å–ª–µ–¥—É–µ—Ç_–∑–∞", "–ø–æ—Å–ª–µ", "–ø–µ—Ä–µ–¥", "–∑–∞—Ç–µ–º", "–ø–æ—Ç–æ–º", "–¥–∞–ª–µ–µ",
            "–≤–ø–æ—Å–ª–µ–¥—Å—Ç–≤–∏–∏", "—Ä–∞–Ω–µ–µ", "–ø—Ä–µ–∂–¥–µ", "—Å–Ω–∞—á–∞–ª–∞", "–≤_–∫–æ–Ω—Ü–µ", "–æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ_—Å",
            "—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ_—Å", "–ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ", "—Å–æ–≤–ø–∞–¥–∞–µ—Ç_—Å", "—Å–æ–ø—É—Ç—Å—Ç–≤—É–µ—Ç", "—Å–æ–ø—Ä–æ–≤–æ–∂–¥–∞–µ—Ç",
            "–∫–æ—Ä—Ä–µ–ª–∏—Ä—É–µ—Ç_—Å", "—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç", "–ø–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∏_—Å–≤—è–∑–∞–Ω–æ", "—Ü–∏–∫–ª–∏—á–µ—Å–∫–∏_–ø–æ–≤—Ç–æ—Ä—è–µ—Ç—Å—è",
            "—Ä–∏—Ç–º–∏—á–Ω–æ_—Å–≤—è–∑–∞–Ω–æ", "–∫–æ–ª–µ–±–ª–µ—Ç—Å—è_—Å", "—Ñ–ª—É–∫—Ç—É–∏—Ä—É–µ—Ç_—Å", "–ø—É–ª—å—Å–∏—Ä—É–µ—Ç_—Å",
            "–≤–æ–ª–Ω–æ–æ–±—Ä–∞–∑–Ω–æ_—Å–≤—è–∑–∞–Ω–æ", "—Ä–∞–∑–≤–∏–≤–∞–µ—Ç—Å—è_–∏–∑", "–ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç_–∏–∑", "–±–µ—Ä–µ—Ç_–Ω–∞—á–∞–ª–æ_–≤",
            "–∫–æ—Ä–Ω–∏_–≤", "–æ—Å–Ω–æ–≤—ã_–≤", "–∏—Å—Ç–æ–∫–∏_–≤", "–∑–∞–≤–µ—Ä—à–∞–µ—Ç—Å—è_–≤", "–∫—É–ª—å–º–∏–Ω–∏—Ä—É–µ—Ç_–≤", "–ø–∏–∫_–≤",
            
            # –°—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã–µ –æ—Ç–Ω–æ—à–µ–Ω–∏—è (36)
            "–≤—ã—à–µ", "–Ω–∏–∂–µ", "–≥–ª–∞–≤–Ω–µ–µ", "–ø–æ–¥—á–∏–Ω–µ–Ω–Ω–µ–µ", "–¥–æ–º–∏–Ω–∏—Ä—É–µ—Ç", "–ø–æ–¥—á–∏–Ω—è–µ—Ç—Å—è", "—Ä—É–∫–æ–≤–æ–¥–∏—Ç",
            "—É–ø—Ä–∞–≤–ª—è–µ—Ç", "–∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ—Ç", "–Ω–∞–ø—Ä–∞–≤–ª—è–µ—Ç", "–≤–µ–¥–µ—Ç", "—ç–ª–µ–º–µ–Ω—Ç_—Å–∏—Å—Ç–µ–º—ã", "–ø–æ–¥—Å–∏—Å—Ç–µ–º–∞",
            "–Ω–∞–¥—Å–∏—Å—Ç–µ–º–∞", "–∫–æ–º–ø–æ–Ω–µ–Ω—Ç", "–º–æ–¥—É–ª—å", "–±–ª–æ–∫", "—É–∑–µ–ª", "–∑–≤–µ–Ω–æ", "—Å–≤—è–∑—É—é—â–µ–µ",
            "—Ü–µ–Ω—Ç—Ä–∞–ª—å–Ω–æ–µ", "–ø–µ—Ä–∏—Ñ–µ—Ä–∏–π–Ω–æ–µ", "–∏–Ω—Ç–µ–≥—Ä–∏—Ä—É—é—â–µ–µ", "–∫–æ–æ—Ä–¥–∏–Ω–∏—Ä—É—é—â–µ–µ", "–≤–Ω—É—Ç—Ä–∏", "—Å–Ω–∞—Ä—É–∂–∏",
            "–Ω–∞–¥", "–ø–æ–¥", "—Ä—è–¥–æ–º", "–º–µ–∂–¥—É", "—Å—Ä–µ–¥–∏", "–æ–∫—Ä—É–∂–∞–µ—Ç", "–æ—Ö–≤–∞—Ç—ã–≤–∞–µ—Ç", "–ø—Ä–æ–Ω–∏–∑—ã–≤–∞–µ—Ç",
            "–ø–µ—Ä–µ—Å–µ–∫–∞–µ—Ç", "–∫–∞—Å–∞–µ—Ç—Å—è",
            
            # –ö–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –æ—Ç–Ω–æ—à–µ–Ω–∏—è (29)
            "—É—Å–∏–ª–∏–≤–∞–µ—Ç", "–æ—Å–ª–∞–±–ª—è–µ—Ç", "—É–≤–µ–ª–∏—á–∏–≤–∞–µ—Ç", "—É–º–µ–Ω—å—à–∞–µ—Ç", "–∞–º–ø–ª–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç", "–ø—Ä–∏–≥–ª—É—à–∞–µ—Ç",
            "–æ–±–æ—Å—Ç—Ä—è–µ—Ç", "–ø—Ä–∏—Ç—É–ø–ª—è–µ—Ç", "–∫–æ–Ω—Ü–µ–Ω—Ç—Ä–∏—Ä—É–µ—Ç", "—Ä–∞—Å—Å–µ–∏–≤–∞–µ—Ç", "—É–ª—É—á—à–∞–µ—Ç", "—É—Ö—É–¥—à–∞–µ—Ç",
            "–æ—á–∏—â–∞–µ—Ç", "–∑–∞–≥—Ä—è–∑–Ω—è–µ—Ç", "–æ–±–ª–∞–≥–æ—Ä–∞–∂–∏–≤–∞–µ—Ç", "–≤—É–ª—å–≥–∞—Ä–∏–∑–∏—Ä—É–µ—Ç", "—É—Ç–æ–Ω—á–∞–µ—Ç", "–æ–≥—Ä—É–±–ª—è–µ—Ç",
            "–≥–∞—Ä–º–æ–Ω–∏–∑–∏—Ä—É–µ—Ç", "—Ä–∞–∑—Ä—É—à–∞–µ—Ç", "—Å—Ç–∞–±–∏–ª–∏–∑–∏—Ä—É–µ—Ç", "–¥–µ—Å—Ç–∞–±–∏–ª–∏–∑–∏—Ä—É–µ—Ç", "–±–∞–ª–∞–Ω—Å–∏—Ä—É–µ—Ç",
            "—Ä–∞–∑–±–∞–ª–∞–Ω—Å–∏—Ä—É–µ—Ç", "—É–∫—Ä–µ–ø–ª—è–µ—Ç", "—Ä–∞—Å—à–∞—Ç—ã–≤–∞–µ—Ç", "—Ñ–∏–∫—Å–∏—Ä—É–µ—Ç", "–æ—Å–≤–æ–±–æ–∂–¥–∞–µ—Ç", "–∑–∞–∫—Ä–µ–ø–ª—è–µ—Ç",
            
            # –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ –æ—Ç–Ω–æ—à–µ–Ω–∏—è –¥–ª—è –Ω–µ–π—Ä–æ—Å—Ç–∞–ª–∫–∏–Ω–≥–∞ (46)
            "–Ω–∞–±–ª—é–¥–∞–µ—Ç", "—Å–≤–∏–¥–µ—Ç–µ–ª—å—Å—Ç–≤—É–µ—Ç", "–æ—Å–æ–∑–Ω–∞–µ—Ç", "—Å—Ç–∞–ª–∫–∏—Ç", "–∏—Å—Å–ª–µ–¥—É–µ—Ç", "–∏–Ω—Ç–µ–≥—Ä–∏—Ä—É–µ—Ç",
            "—Ü–µ–Ω—Ç—Ä–∏—Ä—É–µ—Ç", "–∑–∞–∑–µ–º–ª—è–µ—Ç", "–ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É–µ—Ç_–≤", "–ø—Ä–µ–±—ã–≤–∞–µ—Ç_–≤", "–ø–æ–≥—Ä—É–∂–∞–µ—Ç—Å—è_–≤",
            "–≤—ã—Ö–æ–¥–∏—Ç_–∏–∑", "–æ—Ç–æ–∂–¥–µ—Å—Ç–≤–ª—è–µ—Ç—Å—è_—Å", "—Ä–∞–∑–æ—Ç–æ–∂–¥–µ—Å—Ç–≤–ª—è–µ—Ç—Å—è_—Å", "–∏–¥–µ–Ω—Ç–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç—Å—è_—Å",
            "–¥–∏—Å—Ç–∞–Ω—Ü–∏—Ä—É–µ—Ç—Å—è_–æ—Ç", "–ø—Ä–∏–Ω–∏–º–∞–µ—Ç", "–æ—Ç–≤–µ—Ä–≥–∞–µ—Ç", "—Å–æ–ø—Ä–æ—Ç–∏–≤–ª—è–µ—Ç—Å—è", "—Å–¥–∞–µ—Ç—Å—è", "–æ—Ç–ø—É—Å–∫–∞–µ—Ç",
            "—É–¥–µ—Ä–∂–∏–≤–∞–µ—Ç", "—Ü–µ–ø–ª—è–µ—Ç—Å—è_–∑–∞", "–ø–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç–Ω–æ_—Å–≤—è–∑–∞–Ω–æ", "–≥–ª—É–±–æ–∫–æ_—Å–≤—è–∑–∞–Ω–æ", "–ø–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç–Ω–æ–µ",
            "–≥–ª—É–±–∏–Ω–Ω–æ–µ", "—Å—É—â–Ω–æ—Å—Ç–Ω–æ–µ", "–ø–µ—Ä–∏—Ñ–µ—Ä–∏–π–Ω–æ–µ", "—Ü–µ–Ω—Ç—Ä–∞–ª—å–Ω–æ–µ", "—è–¥–µ—Ä–Ω–æ–µ", "–±–∞–∑–æ–≤–æ–µ",
            "—Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω–æ–µ", "–æ—Å–Ω–æ–≤–æ–ø–æ–ª–∞–≥–∞—é—â–µ–µ", "–ø–µ—Ä–≤–∏—á–Ω–æ–µ", "–≤—Ç–æ—Ä–∏—á–Ω–æ–µ", "—Ç–µ—á–µ—Ç_–≤",
            "–ø—Ä–æ—Ç–µ–∫–∞–µ—Ç_—á–µ—Ä–µ–∑", "–¥–≤–∏–∂–µ—Ç—Å—è_–∫", "–Ω–∞–ø—Ä–∞–≤–ª—è–µ—Ç—Å—è_–Ω–∞", "—Å–ø–æ–Ω—Ç–∞–Ω–Ω–æ_–≤–æ–∑–Ω–∏–∫–∞–µ—Ç",
            "–µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ_—Ç–µ—á–µ—Ç", "–æ—Ä–≥–∞–Ω–∏—á–Ω–æ_—Ä–∞–∑–≤–∏–≤–∞–µ—Ç—Å—è", "—Ñ–æ—Ä—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ_–Ω–∞–≤—è–∑—ã–≤–∞–µ—Ç—Å—è",
            "–∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ_—Å–æ–∑–¥–∞–µ—Ç—Å—è", "–º–µ—Ö–∞–Ω–∏—á–µ—Å–∫–∏_–≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç—Å—è"
        }

        # üîÑ –°–õ–û–í–ê–†–¨ –°–ò–ù–û–ù–ò–ú–û–í –¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏
        self.synonym_map = {
            # –û—Å–Ω–æ–≤–Ω—ã–µ –∫–æ–Ω—Ü–µ–ø—Ç—ã –Ω–µ–π—Ä–æ—Å—Ç–∞–ª–∫–∏–Ω–≥–∞
            "–æ—Å–æ–∑–Ω–∞–Ω–Ω–æ—Å—Ç—å": "–æ—Å–æ–∑–Ω–∞–Ω–∏–µ", "–Ω–∞–±–ª—é–¥–µ–Ω–∏–µ –∑–∞ —É–º–æ–º": "–º–µ—Ç–∞–Ω–∞–±–ª—é–¥–µ–Ω–∏–µ",
            "–≤–Ω–∏–º–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å": "–æ—Å–æ–∑–Ω–∞–Ω–∏–µ", "—Ü–µ–Ω—Ç—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ –≤–Ω–∏–º–∞–Ω–∏—è": "—Ü–µ–Ω—Ç—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ",
            "–ø–æ–ª–µ –≤–Ω–∏–º–∞–Ω–∏—è": "–≤–Ω–∏–º–∞–Ω–∏–µ", "–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–µ—Ä–µ–∂–∏–≤–∞–Ω–∏–π": "–ø–µ—Ä–µ–∂–∏–≤–∞–Ω–∏—è",
            "–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ —Ä–µ–∞–∫—Ü–∏–∏": "–∞–≤—Ç–æ–º–∞—Ç–∏–∑–º—ã", "–ø–∞—Ç—Ç–µ—Ä–Ω—ã –ø–æ–≤–µ–¥–µ–Ω–∏—è": "–ø–∞—Ç—Ç–µ—Ä–Ω—ã",
            "—Ç—Ä–∏–≥–≥–µ—Ä–Ω—ã–µ —Å–∏—Ç—É–∞—Ü–∏–∏": "—Ç—Ä–∏–≥–≥–µ—Ä—ã", "–º–µ–¥–∏—Ç–∞—Ç–∏–≤–Ω—ã–µ –ø—Ä–∞–∫—Ç–∏–∫–∏": "–º–µ–¥–∏—Ç–∞—Ü–∏—è",
            "–¥—ã—Ö–∞—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–∞–∫—Ç–∏–∫–∏": "–¥—ã—Ö–∞–Ω–∏–µ", "—Ç–µ–ª–µ—Å–Ω—ã–µ –æ—â—É—â–µ–Ω–∏—è": "—Ç–µ–ª–æ",
            "—ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è": "—ç–º–æ—Ü–∏–∏", "–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ –ø—Ä–æ—Ü–µ—Å—Å—ã": "—É–º",
            "–¥—É—Ö–æ–≤–Ω—ã–µ –ø—Ä–∞–∫—Ç–∏–∫–∏": "–¥—É—Ö–æ–≤–Ω–æ—Å—Ç—å", "—Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã–µ –ø—Ä–æ—Ü–µ—Å—Å—ã": "—Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—è",
            
            # –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ —Å–∏–Ω–æ–Ω–∏–º—ã –∏–∑ –≥—Ä–∞—Ñ-–∫–æ–ª–ª–µ–∫—Ü–∏–∏
            "–Ω–µ–æ—Å—Ç–∞–ª–∫–∏–Ω–≥": "–Ω–µ–π—Ä–æ—Å—Ç–∞–ª–∫–∏–Ω–≥", "—Å—Ç–∞–ª–∫–∏–Ω–≥ —É–º–∞": "–º–µ—Ç–∞–Ω–∞–±–ª—é–¥–µ–Ω–∏–µ",
            "—á–∏—Å—Ç–æ–µ –æ—Å–æ–∑–Ω–∞–≤–∞–Ω–∏–µ": "–æ—Å–æ–∑–Ω–∞–Ω–∏–µ", "—Å–∞–º–æ-–æ—Å–æ–∑–Ω–∞–≤–∞–Ω–∏–µ": "—Å–∞–º–æ–æ—Å–æ–∑–Ω–∞–Ω–∏–µ",
            "–∂–∏–≤–æ–µ –ø–µ—Ä–µ–∂–∏–≤–∞–Ω–∏–µ": "–ø–µ—Ä–µ–∂–∏–≤–∞–Ω–∏–µ", "–∂–∏–≤–æ–µ –∑–Ω–∞–Ω–∏–µ": "–∑–Ω–∞–Ω–∏–µ",
            "–Ω–µ–ø–æ—Å—Ä–µ–¥—Å—Ç–≤–µ–Ω–Ω–æ–µ –≤–æ—Å–ø—Ä–∏—è—Ç–∏–µ": "–≤–æ—Å–ø—Ä–∏—è—Ç–∏–µ", "–∫–≤–∞–Ω—Ç–æ–≤—ã–π —Å–∫–∞—á–æ–∫": "–ø—Ä–æ–±—É–∂–¥–µ–Ω–∏–µ",
            "–∏–Ω—Ç–µ–≥—Ä–∞—Ç–∏–≤–Ω–∞—è —Å–∞–º–æ—Ä–µ–≥—É–ª—è—Ü–∏—è": "—Å–∞–º–æ—Ä–µ–≥—É–ª—è—Ü–∏—è", "—Å–∞–º–æ–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ": "—Å–∞–º–æ–ø–æ–∑–Ω–∞–Ω–∏–µ",
            "—Ä–∞–∑–æ—Ç–æ–∂–¥–µ—Å—Ç–≤–ª–µ–Ω–∏–µ": "–æ—Ç–æ–∂–¥–µ—Å—Ç–≤–ª–µ–Ω–∏–µ", "–≤–Ω–∏–º–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å": "–≤–Ω–∏–º–∞–Ω–∏–µ",
            "—Å–∞—Ç–∏–ø–∞—Ç—Ç—Ö–∞–Ω–∞": "–æ—Å–æ–∑–Ω–∞–Ω–Ω–æ—Å—Ç—å", "—Å–æ–∑–µ—Ä—Ü–∞–Ω–∏–µ": "–º–µ–¥–∏—Ç–∞—Ü–∏—è",
            "—Å–∞–º–æ—Å–æ–∑–µ—Ä—Ü–∞–Ω–∏–µ": "—Å–∞–º–æ–Ω–∞–±–ª—é–¥–µ–Ω–∏–µ", "–ø—Ä–æ–±—É–∂–¥–µ–Ω–∏–µ": "–æ—Å–æ–∑–Ω–∞–Ω–∏–µ",
            "–ø—Ä–æ—Å–≤–µ—Ç–ª–µ–Ω–∏–µ": "–ø—Ä–æ–±—É–∂–¥–µ–Ω–∏–µ", "–æ—Å–≤–æ–±–æ–∂–¥–µ–Ω–∏–µ": "–æ—Å–≤–æ–±–æ–∂–¥–µ–Ω–∏–µ",
            "–Ω–µ–¥—É–∞–ª—å–Ω–æ—Å—Ç—å": "–µ–¥–∏–Ω—Å—Ç–≤–æ", "–∞–¥–≤–∞–π—Ç–∞": "–Ω–µ–¥—É–∞–ª—å–Ω–æ—Å—Ç—å",
            "—á–∏—Å—Ç–æ–µ —Å–æ–∑–Ω–∞–Ω–∏–µ": "—Å–æ–∑–Ω–∞–Ω–∏–µ", "–µ—Å—Ç—å-–Ω–æ—Å—Ç—å": "–±—ã—Ç–∏–µ",
            "–ø—Ä–∏—Å—É—Ç—Å—Ç–≤–∏–µ –±—ã—Ç–∏—è": "–ø—Ä–∏—Å—É—Ç—Å—Ç–≤–∏–µ", "–ª–æ–∂–Ω–æ–µ —è": "—ç–≥–æ",
            "–∏—Å—Ç–∏–Ω–Ω–æ–µ —è": "—Å–∞–º–æ—Å—Ç—å", "–≤—ã—Å—à–µ–µ —è": "–¥—É—à–∞",
            "–∑–∞—â–∏—Ç–Ω—ã–µ –º–µ—Ö–∞–Ω–∏–∑–º—ã": "–∑–∞—â–∏—Ç–∞", "–∫–æ–ø–∏–Ω–≥-—Å—Ç—Ä–∞—Ç–µ–≥–∏–∏": "—Å—Ç—Ä–∞—Ç–µ–≥–∏–∏",
            
            # –ü—Å–∏—Ö–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è
            "—Å—Ç—Ä–∞–¥–∞–Ω–∏–µ": "–±–æ–ª—å", "–≥–æ—Ä–µ": "–ø–µ—á–∞–ª—å", "—Ç—Ä–µ–≤–æ–≥–∞": "—Å—Ç—Ä–∞—Ö",
            "—É–∂–∞—Å": "—Å—Ç—Ä–∞—Ö", "—è—Ä–æ—Å—Ç—å": "–≥–Ω–µ–≤", "–Ω–µ–Ω–∞–≤–∏—Å—Ç—å": "–≥–Ω–µ–≤",
            "–∑–∞–≤–∏—Å—Ç—å": "—Ä–µ–≤–Ω–æ—Å—Ç—å", "—Å–º—É—â–µ–Ω–∏–µ": "—Å—Ç—ã–¥", "–æ—Ç–≤—Ä–∞—â–µ–Ω–∏–µ": "–ø—Ä–µ–∑—Ä–µ–Ω–∏–µ",
            "–∏–∑–æ–ª—è—Ü–∏—è": "–æ–¥–∏–Ω–æ—á–µ—Å—Ç–≤–æ", "–æ—Ç—á—É–∂–¥–µ–Ω–∏–µ": "–æ–¥–∏–Ω–æ—á–µ—Å—Ç–≤–æ",
            "–æ–ø—É—Å—Ç–æ—à–µ–Ω–Ω–æ—Å—Ç—å": "–±–µ–∑–Ω–∞–¥–µ–∂–Ω–æ—Å—Ç—å", "–±–µ—Å—Å–º—ã—Å–ª–µ–Ω–Ω–æ—Å—Ç—å": "—Å–∫—É–∫–∞",
            "–∞–ø–∞—Ç–∏—è": "—Å–∫—É–∫–∞", "–∏—Å—Ç–æ—â–µ–Ω–∏–µ": "—É—Å—Ç–∞–ª–æ—Å—Ç—å", "–≤—ã–≥–æ—Ä–∞–Ω–∏–µ": "—É—Å—Ç–∞–ª–æ—Å—Ç—å",
            "–±–ª–∞–∂–µ–Ω—Å—Ç–≤–æ": "—Å—á–∞—Å—Ç—å–µ", "–≤–æ—Å—Ç–æ—Ä–≥": "—Ä–∞–¥–æ—Å—Ç—å", "–≤–æ—Å—Ö–∏—â–µ–Ω–∏–µ": "—Ä–∞–¥–æ—Å—Ç—å",
            "—Å–æ—Å—Ç—Ä–∞–¥–∞–Ω–∏–µ": "–ª—é–±–æ–≤—å", "—ç–º–ø–∞—Ç–∏—è": "—Å–æ—Å—Ç—Ä–∞–¥–∞–Ω–∏–µ",
            "–±–ª–∞–≥–æ–¥–∞—Ä–Ω–æ—Å—Ç—å": "–ª—é–±–æ–≤—å", "–ø—Ä–æ—â–µ–Ω–∏–µ": "–ø—Ä–∏–Ω—è—Ç–∏–µ",
            "–¥–æ–≤–µ—Ä–∏–µ": "–≤–µ—Ä–∞", "–Ω–∞–¥–µ–∂–¥–∞": "–æ–ø—Ç–∏–º–∏–∑–º", "—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å": "–æ–ø—Ç–∏–º–∏–∑–º",
            "–ø–æ–∫–æ–π": "—É–º–∏—Ä–æ—Ç–≤–æ—Ä–µ–Ω–∏–µ", "–≥–∞—Ä–º–æ–Ω–∏—è": "–±–∞–ª–∞–Ω—Å",
            "—Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç—å": "–µ–¥–∏–Ω–µ–Ω–∏–µ", "–µ–¥–∏–Ω–µ–Ω–∏–µ": "–≥–∞—Ä–º–æ–Ω–∏—è",
            "–ª–µ–≥–∫–æ—Å—Ç—å": "—Å–≤–æ–±–æ–¥–∞", "—Å–ø–æ–Ω—Ç–∞–Ω–Ω–æ—Å—Ç—å": "—Å–≤–æ–±–æ–¥–∞",
            "–∏–≥—Ä–∏–≤–æ—Å—Ç—å": "—Ç–≤–æ—Ä—á–µ—Å—Ç–≤–æ", "—Ç–≤–æ—Ä—á–µ—Å—Ç–≤–æ": "—Å–ø–æ–Ω—Ç–∞–Ω–Ω–æ—Å—Ç—å",
            
            # –ü—Ä–æ—Ü–µ—Å—Å—ã —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏
            "–ø–æ–∑–Ω–∞–Ω–∏–µ": "–∏–∑—É—á–µ–Ω–∏–µ", "–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ": "–∞–Ω–∞–ª–∏–∑",
            "—Å–∏–Ω—Ç–µ–∑": "–∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è", "–¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏–∞—Ü–∏—è": "—Ä–∞–∑–ª–∏—á–µ–Ω–∏–µ",
            "—Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ": "—É–∑–Ω–∞–≤–∞–Ω–∏–µ", "–æ–ø–æ–∑–Ω–∞–Ω–∏–µ": "—É–∑–Ω–∞–≤–∞–Ω–∏–µ",
            "–ø–æ—Å—Ç–∏–∂–µ–Ω–∏–µ": "–ø–æ–Ω–∏–º–∞–Ω–∏–µ", "–æ—Å–º—ã—Å–ª–µ–Ω–∏–µ": "–ø–æ–Ω–∏–º–∞–Ω–∏–µ",
            "–∏–Ω—Ç—É–∏—Ü–∏—è": "–ø—Ä–µ–¥—á—É–≤—Å—Ç–≤–∏–µ", "–ø—Ä–µ–¥–≤–∏–¥–µ–Ω–∏–µ": "–ø—Ä–µ–¥—á—É–≤—Å—Ç–≤–∏–µ",
            "—è—Å–Ω–æ–≤–∏–¥–µ–Ω–∏–µ": "–∏–Ω—Ç—É–∏—Ü–∏—è", "—ç–≤–æ–ª—é—Ü–∏—è": "—Ä–∞–∑–≤–∏—Ç–∏–µ",
            "—Ä–æ—Å—Ç": "—Ä–∞–∑–≤–∏—Ç–∏–µ", "—Å–æ–∑—Ä–µ–≤–∞–Ω–∏–µ": "—Ä–∞–∑–≤–∏—Ç–∏–µ",
            "–ø—Ä–æ–≥—Ä–µ—Å—Å": "—Ä–∞–∑–≤–∏—Ç–∏–µ", "—Ä–µ–≥—Ä–µ—Å—Å": "–¥–µ–≥—Ä–∞–¥–∞—Ü–∏—è",
            "—Ä–∞—Å–ø–∞–¥": "—Ä–∞–∑—Ä—É—à–µ–Ω–∏–µ", "—Å–º–µ—Ä—Ç—å": "—Ä–∞–∑—Ä—É—à–µ–Ω–∏–µ",
            "–≤–æ–∑—Ä–æ–∂–¥–µ–Ω–∏–µ": "–ø–µ—Ä–µ—Ä–æ–∂–¥–µ–Ω–∏–µ", "–≤–æ—Å–∫—Ä–µ—à–µ–Ω–∏–µ": "–ø–µ—Ä–µ—Ä–æ–∂–¥–µ–Ω–∏–µ",
            "—Ä–µ–∏–Ω–∫–∞—Ä–Ω–∞—Ü–∏—è": "–ø–µ—Ä–µ—Ä–æ–∂–¥–µ–Ω–∏–µ", "–º–µ—Ç–∞–º–æ—Ä—Ñ–æ–∑–∞": "—Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—è",
            "–∞–ª—Ö–∏–º–∏—è": "—Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—è", "—Ç—Ä–∞–Ω—Å–º—É—Ç–∞—Ü–∏—è": "—Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—è",
            "—Å—É–±–ª–∏–º–∞—Ü–∏—è": "–æ—á–∏—â–µ–Ω–∏–µ", "–∫–∞—Ç–∞—Ä—Å–∏—Å": "–æ—á–∏—â–µ–Ω–∏–µ",
            "–∏—Å—Ü–µ–ª–µ–Ω–∏–µ": "–æ—á–∏—â–µ–Ω–∏–µ", "–æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ": "—Å–ª–∏—è–Ω–∏–µ",
            "–µ–¥–∏–Ω–µ–Ω–∏–µ": "—Å–ª–∏—è–Ω–∏–µ", "–≥–∞—Ä–º–æ–Ω–∏–∑–∞—Ü–∏—è": "–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞",
            "—Å—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏—è": "–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞", "—Ü–µ–Ω—Ç—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ": "–∑–∞–∑–µ–º–ª–µ–Ω–∏–µ",
            "—É–∫–æ—Ä–µ–Ω–µ–Ω–∏–µ": "–∑–∞–∑–µ–º–ª–µ–Ω–∏–µ", "–æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ": "—É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ",
            "–ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ": "—É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ", "–≤–∞–ª–∏–¥–∞—Ü–∏—è": "—É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ",
            "–ø—Ä–∏–∑–Ω–∞–Ω–∏–µ": "–ø—Ä–∏–Ω—è—Ç–∏–µ", "–æ–¥–æ–±—Ä–µ–Ω–∏–µ": "–ø—Ä–∏–Ω—è—Ç–∏–µ",
            "–ø–æ–¥–¥–µ—Ä–∂–∫–∞": "–ø—Ä–∏–Ω—è—Ç–∏–µ", "–ø–æ–æ—â—Ä–µ–Ω–∏–µ": "–ø–æ–¥–¥–µ—Ä–∂–∫–∞",
            "–ø–æ—Ö–≤–∞–ª–∞": "–ø–æ–æ—â—Ä–µ–Ω–∏–µ", "–æ—Ç–ø—É—Å–∫–∞–Ω–∏–µ": "–æ—Ç–¥–∞—á–∞",
            "—Å–¥–∞—á–∞": "–æ—Ç–¥–∞—á–∞", "–∫–∞–ø–∏—Ç—É–ª—è—Ü–∏—è": "—Å–¥–∞—á–∞",
            "—Å–º–∏—Ä–µ–Ω–∏–µ": "–∫–∞–ø–∏—Ç—É–ª—è—Ü–∏—è", "–ø–æ–∫–æ—Ä–Ω–æ—Å—Ç—å": "—Å–º–∏—Ä–µ–Ω–∏–µ",
            "–ø–æ—Å–ª—É—à–∞–Ω–∏–µ": "–ø–æ–∫–æ—Ä–Ω–æ—Å—Ç—å", "–¥–æ–≤–µ—Ä–∏–µ": "–≤–µ—Ä–∞",
            "–ø—Ä–µ–¥–∞–Ω–Ω–æ—Å—Ç—å": "–≤–µ—Ä–∞", "—Å–ª—É–∂–µ–Ω–∏–µ": "–ø—Ä–µ–¥–∞–Ω–Ω–æ—Å—Ç—å",
            "–∂–µ—Ä—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å": "—Å–ª—É–∂–µ–Ω–∏–µ", "–∞–ª—å—Ç—Ä—É–∏–∑–º": "–∂–µ—Ä—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å",
            "–±–µ—Å–∫–æ—Ä—ã—Å—Ç–∏–µ": "–∞–ª—å—Ç—Ä—É–∏–∑–º", "–±–µ–∑—É—Å–ª–æ–≤–Ω–æ—Å—Ç—å": "–±–µ—Å–∫–æ—Ä—ã—Å—Ç–∏–µ",
            "–Ω–µ–ø—Ä–∏–≤—è–∑–∞–Ω–Ω–æ—Å—Ç—å": "–±–µ–∑—É—Å–ª–æ–≤–Ω–æ—Å—Ç—å", "–æ—Ç—Ä–µ—à–µ–Ω–Ω–æ—Å—Ç—å": "–Ω–µ–ø—Ä–∏–≤—è–∑–∞–Ω–Ω–æ—Å—Ç—å",
            "–±–µ–∑—Ä–∞–∑–ª–∏—á–∏–µ": "–æ—Ç—Ä–µ—à–µ–Ω–Ω–æ—Å—Ç—å", "—Ä–∞–≤–Ω–æ–¥—É—à–∏–µ": "–±–µ–∑—Ä–∞–∑–ª–∏—á–∏–µ",
            "–Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ—Å—Ç—å": "—Ä–∞–≤–Ω–æ–¥—É—à–∏–µ", "–æ–±—ä–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å": "–Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ—Å—Ç—å"
        }

        # üö´ –°–¢–û–ü-–°–õ–û–í–ê (—Å–ª–∏—à–∫–æ–º –æ–±—â–∏–µ –¥–ª—è –≥—Ä–∞—Ñ-—Å–∏—Å—Ç–µ–º—ã)
        self.stop_words = {
            "—Ä–æ—Å—Ç", "–±–æ–ª—å", "–æ—â—É—â–µ–Ω–∏–µ", "–º—ã—Å–ª—å", "–∏–¥–µ—è", "—Å–æ—Å—Ç–æ—è–Ω–∏–µ", "–ø—Ä–æ—Ü–µ—Å—Å", "–æ–ø—ã—Ç",
            "—á—É–≤—Å—Ç–≤–æ", "–º–æ–º–µ–Ω—Ç", "–≤—Ä–µ–º—è", "–∂–∏–∑–Ω—å", "—á–µ–ª–æ–≤–µ–∫", "–ª—é–¥–∏", "–º–∏—Ä", "–ø—É—Ç—å",
            "—Å–ø–æ—Å–æ–±", "–º–µ—Ç–æ–¥", "–≤–µ—â—å", "–¥–µ–ª–æ", "—Ä–∞–±–æ—Ç–∞", "–∑–∞–¥–∞—á–∞", "–ø—Ä–æ–±–ª–µ–º–∞", "–≤–æ–ø—Ä–æ—Å",
            "–æ—Ç–≤–µ—Ç", "—Ä–µ—à–µ–Ω–∏–µ", "—Ä–µ–∑—É–ª—å—Ç–∞—Ç", "—ç—Ñ—Ñ–µ–∫—Ç", "–≤–ª–∏—è–Ω–∏–µ", "–≤–æ–∑–¥–µ–π—Å—Ç–≤–∏–µ", "–∏–∑–º–µ–Ω–µ–Ω–∏–µ",
            "—Ä–∞–∑–≤–∏—Ç–∏–µ", "—É–ª—É—á—à–µ–Ω–∏–µ", "—É—Ö—É–¥—à–µ–Ω–∏–µ", "—É–≤–µ–ª–∏—á–µ–Ω–∏–µ", "—É–º–µ–Ω—å—à–µ–Ω–∏–µ", "–ø–æ—è–≤–ª–µ–Ω–∏–µ",
            "–∏—Å—á–µ–∑–Ω–æ–≤–µ–Ω–∏–µ", "—Å–æ–∑–¥–∞–Ω–∏–µ", "—É–Ω–∏—á—Ç–æ–∂–µ–Ω–∏–µ", "–Ω–∞—á–∞–ª–æ", "–∫–æ–Ω–µ—Ü", "—Å–µ—Ä–µ–¥–∏–Ω–∞",
            "—á–∞—Å—Ç—å", "—Ü–µ–ª–æ–µ", "—Å–∏—Å—Ç–µ–º–∞", "—Å—Ç—Ä—É–∫—Ç—É—Ä–∞", "–æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è", "—É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ",
            "–∫–æ–Ω—Ç—Ä–æ–ª—å", "–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ", "–¥–≤–∏–∂–µ–Ω–∏–µ", "—Å–∫–æ—Ä–æ—Å—Ç—å", "—Å–∏–ª–∞", "—ç–Ω–µ—Ä–≥–∏—è",
            "–º–∞—Ç–µ—Ä–∏—è", "—Ñ–æ—Ä–º–∞", "—Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ", "–∫–∞—á–µ—Å—Ç–≤–æ", "–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ", "—Ä–∞–∑–º–µ—Ä",
            "–≤–µ—Å", "–æ–±—ä–µ–º", "–ø–ª–æ—â–∞–¥—å", "–¥–ª–∏–Ω–∞", "—à–∏—Ä–∏–Ω–∞", "–≤—ã—Å–æ—Ç–∞", "–≥–ª—É–±–∏–Ω–∞",
            "–ø–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç—å", "–≤–Ω—É—Ç—Ä–µ–Ω–Ω–æ—Å—Ç—å", "–≤–Ω–µ—à–Ω–æ—Å—Ç—å", "–≤–µ—Ä—Ö", "–Ω–∏–∑", "–ª–µ–≤–æ", "–ø—Ä–∞–≤–æ",
            "—Ü–µ–Ω—Ç—Ä", "–∫—Ä–∞–π", "–≥—Ä–∞–Ω–∏—Ü–∞", "–ª–∏–Ω–∏—è", "—Ç–æ—á–∫–∞", "–ø–ª–æ—Å–∫–æ—Å—Ç—å", "–ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ"
        }

    # ---------------------- Internal helpers ---------------------- #
    def _tok(self, text: str) -> int:
        return len(self.encoding.encode(text))

    def _ask(self, model: str, prompt: str, max_tokens: int = 2200, temperature: float = 0.3) -> str:
        # –ó–∞–¥–µ—Ä–∂–∫–∞ –ø–µ—Ä–µ–¥ –∑–∞–ø—Ä–æ—Å–æ–º –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è rate limit (429 –æ—à–∏–±–æ–∫)
        # OpenAI API –∏–º–µ–µ—Ç –ª–∏–º–∏—Ç—ã: 3000 RPM –¥–ª—è gpt-4o-mini
        # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç—Å—è —á–µ—Ä–µ–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –æ–∫—Ä—É–∂–µ–Ω–∏—è OPENAI_API_DELAY (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 1.0 —Å–µ–∫—É–Ω–¥–∞)
        time.sleep(self.api_delay)
        
        r = self.client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": "–¢—ã —Ä–µ–¥–∞–∫—Ç–æ—Ä –ª–µ–∫—Ü–∏–π –°–∞–ª–∞–º–∞—Ç–∞ –°–∞—Ä—Å–µ–∫–µ–Ω–æ–≤–∞. –°–æ—Ö—Ä–∞–Ω—è–π —Ç–µ—Ä–º–∏–Ω–æ–ª–æ–≥–∏—é –Ω–µ–π—Ä–æ—Å—Ç–∞–ª–∫–∏–Ω–≥–∞/–Ω–µ–æ—Å—Ç–∞–ª–∫–∏–Ω–≥–∞ –∏ –∞–≤—Ç–æ—Ä—Å–∫–∏–π —Å—Ç–∏–ª—å."},
                {"role": "user", "content": prompt},
            ],
            temperature=temperature,
            max_tokens=max_tokens,
        )
        return r.choices[0].message.content

    # –£–ª—É—á—à–µ–Ω–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –∞–≤—Ç–æ—Ä—Å–∫–æ–≥–æ —Å—Ç–∏–ª—è
    def _light_clean(self, text: str) -> str:
        """
        –£–ª—É—á—à–µ–Ω–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –∞–≤—Ç–æ—Ä—Å–∫–æ–≥–æ —Å—Ç–∏–ª—è
        –£–±–∏—Ä–∞–µ–º: —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π —à—É–º, –º–µ–∂–¥–æ–º–µ—Ç–∏—è, –≥—Ä–∞–º–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –æ—à–∏–±–∫–∏, —Ä–µ—á–µ–≤—ã–µ —Å–±–æ–∏
        """
        # –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π —à—É–º (–±–∞–∑–æ–≤—ã–π)
        replacements = [
            ("[–º—É–∑—ã–∫–∞]", ""), ("[Music]", ""), (">>", ""), ("&gt;&gt;", ""),
        ]
        
        # –ù–û–í–û–ï: –ú–µ–∂–¥–æ–º–µ—Ç–∏—è –∏ —Ä–µ—á–µ–≤—ã–µ —Å–±–æ–∏
        speech_fixes = [
            # –ú–µ–∂–¥–æ–º–µ—Ç–∏—è (—Å –ø—Ä–æ–±–µ–ª–∞–º–∏ –¥–ª—è —Ç–æ—á–Ω–æ—Å—Ç–∏)
            (" —ç—ç ", " "), (" —ç-—ç ", " "), (" —ç—ç—ç ", " "), (" —ç—ç—ç—ç ", " "),
            (" –∞—Ö ", " "), (" –æ—Ö ", " "), (" —É—Ö ", " "), (" —ç—Ö ", " "),
            (" –º–º ", " "), (" —Ö–º ", " "), (" –Ω—É ", " "),
            
            # –ù–∞—á–∞–ª–æ –∏ –∫–æ–Ω–µ—Ü –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
            ("–≠—ç, ", ""), ("–≠—Ö, ", ""), ("–û—Ö, ", ""), ("–ê—Ö, ", ""),
            (" —ç—ç,", ","), (" –æ—Ö,", ","), (" –∞—Ö,", ","),
            
            # –Ø–≤–Ω—ã–µ –≥—Ä–∞–º–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –æ—à–∏–±–∫–∏
            ("–ø–æ—Ç–æ —á—Ç–æ", "–ø–æ—Ç–æ–º—É —á—Ç–æ"),
            ("–Ω–æ–≤–æ—Å—Ç–µ", "–Ω–æ–≤–æ—Å—Ç–∏"), 
            ("–±–æ–≥–æ–≤–µ—Å—Ç—å", "–±–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å"),
            ("–ø–æ—Ç–º—É —á—Ç–æ", "–ø–æ—Ç–æ–º—É —á—Ç–æ"),
            ("—Ç–æ–µ—Å—Ç—å", "—Ç–æ –µ—Å—Ç—å"),
            ("–≤–æ–±—â–µ", "–≤–æ–æ–±—â–µ"),
            
            # –ò–∑–±—ã—Ç–æ—á–Ω—ã–µ –ø–æ–≤—Ç–æ—Ä—ã —Å–ª–æ–≤
            (" –≤–æ—Ç –≤–æ—Ç ", " –≤–æ—Ç "), (" —ç—Ç–æ —ç—Ç–æ ", " —ç—Ç–æ "),
            (" –¥–∞ –¥–∞ ", " –¥–∞ "), (" –Ω—É –Ω—É ", " –Ω—É "),
            
            # –†–µ—á–µ–≤—ã–µ —Å–±–æ–∏ (–æ—Å—Ç–æ—Ä–æ–∂–Ω–æ, —Ç–æ–ª—å–∫–æ —è–≤–Ω—ã–µ)
            ("–º–∞–ª–æ –ª–∏ —á—Ç–æ", ""), ("—è –Ω–µ –∑–Ω–∞—é, ", ""),
            ("–∫–∞–∫ –±—ã ", " "), ("—Ç–∏–ø–∞ ", " "),
            ("–∫–æ—Ä–æ—á–µ –≥–æ–≤–æ—Ä—è", "–∫–æ—Ä–æ—á–µ"),
        ]
        
        cleaned = text
        for old, new in replacements + speech_fixes:
            cleaned = cleaned.replace(old, new)
        
        # –£–±–∏—Ä–∞–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã –∏ –ª–∏—à–Ω–∏–µ –∑–Ω–∞–∫–∏
        cleaned = re.sub(r'\s+', ' ', cleaned)
        cleaned = re.sub(r'\s*,\s*,', ',', cleaned)  # –¥–≤–æ–π–Ω—ã–µ –∑–∞–ø—è—Ç—ã–µ
        cleaned = re.sub(r'\s*\.\s*\.', '.', cleaned)  # –¥–≤–æ–π–Ω—ã–µ —Ç–æ—á–∫–∏
        
        return cleaned.strip()

    def _final_polish(self, text: str) -> str:
        """
        –§–∏–Ω–∞–ª—å–Ω–∞—è –ø–æ–ª–∏—Ä–æ–≤–∫–∞ —Ç–µ–∫—Å—Ç–∞ - —É–º–Ω–æ–µ —É–¥–∞–ª–µ–Ω–∏–µ –∏–∑–±—ã—Ç–æ—á–Ω—ã—Ö —Å–ª–æ–≤-–ø–∞—Ä–∞–∑–∏—Ç–æ–≤
        –£–±–∏—Ä–∞–µ–º –∫–∞–∂–¥–æ–µ –≤—Ç–æ—Ä–æ–µ "–≤–æ—Ç", "—ç—Ç–æ" –µ—Å–ª–∏ –∏—Ö —Å–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ –≤ –∞–±–∑–∞—Ü–µ
        """
        paragraphs = text.split('\n')
        polished_paragraphs = []
        
        for para in paragraphs:
            if not para.strip():
                polished_paragraphs.append(para)
                continue
                
            words = para.split()
            
            # –£–±–∏—Ä–∞–µ–º –∫–∞–∂–¥–æ–µ –≤—Ç–æ—Ä–æ–µ "–≤–æ—Ç" –µ—Å–ª–∏ –∏—Ö –±–æ–ª—å—à–µ 3 –≤ –∞–±–∑–∞—Ü–µ
            vot_count = words.count('–≤–æ—Ç')
            if vot_count > 3:
                vot_removed = 0
                for i, word in enumerate(words):
                    if word == '–≤–æ—Ç' and vot_removed < vot_count // 2:
                        words[i] = ''
                        vot_removed += 1
            
            # –£–±–∏—Ä–∞–µ–º –∫–∞–∂–¥–æ–µ —Ç—Ä–µ—Ç—å–µ "—ç—Ç–æ" –µ—Å–ª–∏ –∏—Ö –±–æ–ª—å—à–µ 5 –≤ –∞–±–∑–∞—Ü–µ
            eto_count = words.count('—ç—Ç–æ')
            if eto_count > 5:
                eto_removed = 0
                for i, word in enumerate(words):
                    if word == '—ç—Ç–æ' and eto_removed < eto_count // 3:
                        words[i] = ''
                        eto_removed += 1
            
            # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ "–Ω—É" –≤ –Ω–∞—á–∞–ª–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
            nu_pattern = []
            for i, word in enumerate(words):
                if word == '–Ω—É' and (i == 0 or words[i-1].endswith('.') or words[i-1].endswith('!')):
                    # –û—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –∫–∞–∂–¥–æ–µ –≤—Ç–æ—Ä–æ–µ "–Ω—É" –≤ –Ω–∞—á–∞–ª–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
                    if len(nu_pattern) % 2 == 1:
                        words[i] = ''
                    nu_pattern.append(i)
            
            polished_paragraphs.append(' '.join(w for w in words if w))
        
        return '\n'.join(polished_paragraphs)

    # ---------------------- SAG v2.0 CLASSIFICATION FUNCTIONS ---------------------- #
    
    def detect_speaker(self, content: str) -> str:
        """
        –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Å–ø–∏–∫–µ—Ä–∞ –±–ª–æ–∫–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—è
        """
        # –ï—Å–ª–∏ –≤ —Ç–µ–∫—Å—Ç–µ –µ—Å—Ç—å >> - —ç—Ç–æ –¥–∏–∞–ª–æ–≥
        if ">>" in content:
            return "mixed"  # –°–º–µ—à–∞–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç
        
        # –ú–∞—Ä–∫–µ—Ä—ã —Ä–µ—á–∏ –°–∞—Ä—Å–µ–∫–µ–Ω–æ–≤–∞
        sarsekenov_markers = [
            "–ø–æ–Ω–∏–º–∞–µ—Ç–µ", "–∑–∞–º–µ—Ç–∏–ª–∏", "—á—É–≤—Å—Ç–≤—É–µ—Ç–µ", "–ø—Ä—è–º–æ —Å–µ–π—á–∞—Å", 
            "–ø–æ–≤—Ç–æ—Ä—è—é—Å—å", "–≤–æ—Ç —Å–º–æ—Ç—Ä–∏—Ç–µ", "—à—Ç—Ä–∏—Ö-–∫–æ–¥ –±—Ä–æ—Å–∞—é",
            "–∫ –ø—Ä–∏–º–µ—Ä—É", "–º–∞–ª–æ —Ç–æ–≥–æ", "–æ–ø—è—Ç—å –∂–µ", "–≤–∏–¥–∏—Ç–µ –ª–∏"
        ]
        
        # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º –º–∞—Ä–∫–µ—Ä—ã
        content_lower = content.lower()
        marker_count = sum(1 for marker in sarsekenov_markers if marker in content_lower)
        
        # –ï—Å–ª–∏ –º–Ω–æ–≥–æ –º–∞—Ä–∫–µ—Ä–æ–≤ –°–∞—Ä—Å–µ–∫–µ–Ω–æ–≤–∞ –∏–ª–∏ –¥–ª–∏–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç - —ç—Ç–æ –æ–Ω
        if marker_count >= 2 or len(content) > 500:
            return "sarsekenov"
        
        # –ò–Ω–∞—á–µ - –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ –∏–ª–∏ —É—á–∞—Å—Ç–Ω–∏–∫
        return "unknown"

    def detect_block_type(self, content: str) -> str:
        """
        –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ç–∏–ø –±–ª–æ–∫–∞ (–º–æ–Ω–æ–ª–æ–≥/–¥–∏–∞–ª–æ–≥/–ø—Ä–∞–∫—Ç–∏–∫–∞)
        """
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –¥–∏–∞–ª–æ–≥–æ–≤—ã—Ö –º–∞—Ä–∫–µ—Ä–æ–≤
        if ">>" in content:
            return "dialogue"
        
        # –ú–∞—Ä–∫–µ—Ä—ã –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏—Ö —É–ø—Ä–∞–∂–Ω–µ–Ω–∏–π
        practice_keywords = [
            "—É–ø—Ä–∞–∂–Ω–µ–Ω–∏–µ", "–ø—Ä–∞–∫—Ç–∏–∫–∞", "–º–µ–¥–∏—Ç–∞—Ü–∏—è", "–ø—Ä–æ—á—É–≤—Å—Ç–≤—É–π—Ç–µ",
            "–æ—â—É—Ç–∏—Ç–µ", "–ø–æ–ø—Ä–æ–±—É–π—Ç–µ", "—Å–æ—Å—Ä–µ–¥–æ—Ç–æ—á—å—Ç–µ—Å—å", "–Ω–∞–±–ª—é–¥–∞–π—Ç–µ",
            "–ø–æ—á—É–≤—Å—Ç–≤—É–π—Ç–µ", "–∏—Å—Å–ª–µ–¥—É–π—Ç–µ"
        ]
        
        content_lower = content.lower()
        if any(keyword in content_lower for keyword in practice_keywords):
            return "practice"
        
        # –ú–∞—Ä–∫–µ—Ä—ã –≤–æ–ø—Ä–æ—Å–æ–≤
        if content.strip().endswith("?") or "–≤–æ–ø—Ä–æ—Å" in content_lower:
            return "question"
        
        # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é - –º–æ–Ω–æ–ª–æ–≥
        return "monologue"

    def detect_emotional_tone(self, content: str, title: str) -> str:
        """
        –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π —Ç–æ–Ω –±–ª–æ–∫–∞
        """
        # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ç–æ–Ω–æ–≤
        tones = {
            "contemplative": ["—Ä–∞–∑–º—ã—à–ª–µ–Ω–∏—è", "–æ—Å–æ–∑–Ω–∞–Ω–∏–µ", "–ø–æ–Ω–∏–º–∞–Ω–∏–µ", "–≥–ª—É–±–æ–∫–æ–µ", "—Å—É—Ç—å", "—Å—É—â–Ω–æ—Å—Ç—å"],
            "explanatory": ["–æ–±—ä—è—Å–Ω–µ–Ω–∏–µ", "–∫ –ø—Ä–∏–º–µ—Ä—É", "—Å–º–æ—Ç—Ä–∏—Ç–µ", "–ø–æ–Ω–∏–º–∞–µ—Ç–µ", "–∑–∞–º–µ—Ç–∏–ª–∏", "–≤–∏–¥–∏—Ç–µ"],
            "intense": ["–∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏", "–≤–∞–∂–Ω–æ", "—Å–µ—Ä—å–µ–∑–Ω–æ", "–¥—Ä–∞–º–∞—Ç–∏—á–µ—Å–∫–∏", "—Å–∏–ª—å–Ω–æ", "—Å—Ç—Ä–∞–¥–∞–Ω–∏–µ"],
            "light": ["—Å–º–µ—Ö", "–∏–≥—Ä–∞", "–ª–µ–≥–∫–æ", "–ø—Ä–æ—Å—Ç–æ", "—É–ª—ã–±–∫–∞", "–∫–ª–∞—Å—Å–Ω–æ", "–∑–∞–±–∞–≤–Ω–æ"]
        }
        
        # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è
        text_to_analyze = (content + " " + title).lower()
        scores = {}
        for tone, keywords in tones.items():
            score = sum(1 for kw in keywords if kw in text_to_analyze)
            scores[tone] = score
        
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ–Ω —Å –Ω–∞–∏–±–æ–ª—å—à–∏–º —Å—á–µ—Ç–æ–º –∏–ª–∏ neutral
        if max(scores.values()) > 0:
            return max(scores.keys(), key=lambda k: scores[k])
        return "neutral"

    def detect_conceptual_depth(self, content: str, keywords: list) -> str:
        """
        –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –∫–æ–Ω—Ü–µ–ø—Ç—É–∞–ª—å–Ω—É—é –≥–ª—É–±–∏–Ω—É –±–ª–æ–∫–∞
        """
        # –ú–∞—Ä–∫–µ—Ä—ã –≤—ã—Å–æ–∫–æ–π –≥–ª—É–±–∏–Ω—ã
        deep_concepts = [
            "—Å—É—â–Ω–æ—Å—Ç—å", "–ø—Ä–∏—Ä–æ–¥–∞", "–∞–±—Å–æ–ª—é—Ç", "–±—ã—Ç–∏–µ", "—Å–æ–∑–Ω–∞–Ω–∏–µ",
            "–ø—Ä–æ—Å–≤–µ—Ç–ª–µ–Ω–∏–µ", "–ø—Ä–æ–±—É–∂–¥–µ–Ω–∏–µ", "–±–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å", "–∏—Å—Ç–∏–Ω–∞",
            "—Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—è", "–∏—Å—Ü–µ–ª–µ–Ω–∏–µ"
        ]
        
        # –ú–∞—Ä–∫–µ—Ä—ã —Å—Ä–µ–¥–Ω–µ–π –≥–ª—É–±–∏–Ω—ã  
        medium_concepts = [
            "–ø–æ–Ω–∏–º–∞–Ω–∏–µ", "–æ—Å–æ–∑–Ω–∞–Ω–∏–µ", "–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ", "–Ω–∞–±–ª—é–¥–µ–Ω–∏–µ",
            "–≤–Ω–∏–º–∞–Ω–∏–µ", "–ø–µ—Ä–µ–∂–∏–≤–∞–Ω–∏–µ", "–æ–ø—ã—Ç", "–º—É–¥—Ä–æ—Å—Ç—å", "–ø—Ä–∞–∫—Ç–∏–∫–∞"
        ]
        
        # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º
        content_lower = content.lower()
        keywords_str = " ".join(keywords).lower()
        
        deep_count = sum(1 for concept in deep_concepts 
                        if concept in content_lower or concept in keywords_str)
        medium_count = sum(1 for concept in medium_concepts 
                          if concept in content_lower or concept in keywords_str)
        
        if deep_count >= 2:
            return "high"
        elif medium_count >= 2 or deep_count >= 1:
            return "medium"
        else:
            return "low"

    def calculate_complexity_score(self, content: str, keywords: list, conceptual_depth: str) -> float:
        """
        –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç —á–∏—Å–ª–æ–≤—É—é –æ—Ü–µ–Ω–∫—É —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ (1-10)
        """
        base_score = 5.0
        
        # –ö–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ –≥–ª—É–±–∏–Ω–µ
        depth_modifier = {"low": -1.5, "medium": 0, "high": 2.0}
        base_score += depth_modifier.get(conceptual_depth, 0)
        
        # –ö–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
        if len(keywords) > 5:
            base_score += 1.0
        elif len(keywords) < 3:
            base_score -= 0.5
            
        # –ö–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ –¥–ª–∏–Ω–µ —Ç–µ–∫—Å—Ç–∞ (—Å–ª–æ–∂–Ω–æ—Å—Ç—å —Ä–∞—Å—Ç–µ—Ç —Å –æ–±—ä–µ–º–æ–º)
        if len(content) > 1500:
            base_score += 1.0
        elif len(content) < 500:
            base_score -= 0.5
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–∏–∞–ø–∞–∑–æ–Ω 1-10
        return max(1.0, min(10.0, round(base_score, 1)))

    def extract_graph_entities(self, content: str, keywords: list) -> list:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–ª—é—á–µ–≤—ã–µ —Å—É—â–Ω–æ—Å—Ç–∏ –¥–ª—è –≥—Ä–∞—Ñ-—Å–∏—Å—Ç–µ–º—ã –∑–Ω–∞–Ω–∏–π –Ω–µ–π—Ä–æ—Å—Ç–∞–ª–∫–∏–Ω–≥–∞
        –û—Å–Ω–æ–≤–∞–Ω–æ –Ω–∞ –∫–æ–ª–ª–µ–∫—Ü–∏–∏ –∏–∑ 442 —É–∑–ª–æ–≤ –≥—Ä–∞—Ñ-—Å–∏—Å—Ç–µ–º—ã
        """
        # –ö–û–ù–¶–ï–ü–¢–´ –ù–ï–ô–†–û–°–¢–ê–õ–ö–ò–ù–ì–ê (74 —É–∑–ª–∞) - –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã–µ
        neurostalking_concepts = [
            # –ö–ª—é—á–µ–≤—ã–µ —Ç–µ—Ä–º–∏–Ω—ã –°–∞—Ä—Å–µ–∫–µ–Ω–æ–≤–∞
            "–Ω–µ–π—Ä–æ—Å—Ç–∞–ª–∫–∏–Ω–≥", "–Ω–µ–æ—Å—Ç–∞–ª–∫–∏–Ω–≥", "—Å—Ç–∞–ª–∫–∏–Ω–≥ —É–º–∞",
            "–º–µ—Ç–∞–Ω–∞–±–ª—é–¥–µ–Ω–∏–µ", "–ø–æ–ª–µ –≤–Ω–∏–º–∞–Ω–∏—è", "–ø–æ–ª–µ –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è",
            "—Å–≤–æ–±–æ–¥–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ", "—á–∏—Å—Ç–æ–µ –æ—Å–æ–∑–Ω–∞–≤–∞–Ω–∏–µ", "—Å–∞–º–æ-–æ—Å–æ–∑–Ω–∞–≤–∞–Ω–∏–µ",
            "—Å–∞–º–æ–æ—Å–æ–∑–Ω–∞–Ω–∏–µ", "–ø—Ä–∏—Å—É—Ç—Å—Ç–≤–∏–µ", "–∑–¥–µ—Å—å –∏ —Å–µ–π—á–∞—Å",
            "–∂–∏–≤–æ–µ –ø–µ—Ä–µ–∂–∏–≤–∞–Ω–∏–µ", "–∂–∏–≤–æ–µ –∑–Ω–∞–Ω–∏–µ", "–Ω–µ–ø–æ—Å—Ä–µ–¥—Å—Ç–≤–µ–Ω–Ω–æ–µ –≤–æ—Å–ø—Ä–∏—è—Ç–∏–µ",
            
            # –ü—Ä–æ—Ü–µ—Å—Å—ã –ø–æ–∑–Ω–∞–Ω–∏—è
            "—Å–∞–º–æ–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ", "—Å–∞–º–æ–ø–æ–∑–Ω–∞–Ω–∏–µ", "—Å—Ç–∞–ª–∫–∏–Ω–≥", "–Ω–∞–±–ª—é–¥–µ–Ω–∏–µ –∑–∞ —É–º–æ–º",
            "–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ —É–º–∞", "—Ä–∞–∑–æ—Ç–æ–∂–¥–µ—Å—Ç–≤–ª–µ–Ω–∏–µ", "—Ü–µ–Ω—Ç—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ", "–æ—Å–æ–∑–Ω–∞–≤–∞–Ω–∏–µ",
            "–≤–Ω–∏–º–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å", "–æ—Å–æ–∑–Ω–∞–Ω–Ω–æ—Å—Ç—å", "–≤–∏–ø–∞—à—å—è–Ω–∞", "—à–∞–º–∞—Ç—Ö–∞",
            
            # –î—É—Ö–æ–≤–Ω—ã–µ –∫–æ–Ω—Ü–µ–ø—Ç—ã
            "–ø—Ä–æ–±—É–∂–¥–µ–Ω–∏–µ", "–ø—Ä–æ—Å–≤–µ—Ç–ª–µ–Ω–∏–µ", "–æ—Å–≤–æ–±–æ–∂–¥–µ–Ω–∏–µ", "–µ–¥–∏–Ω—Å—Ç–≤–æ", "–Ω–µ–¥—É–∞–ª—å–Ω–æ—Å—Ç—å",
            "–±–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å", "–∞–±—Å–æ–ª—é—Ç", "–∏—Å—Ç–∏–Ω–∞", "—Å—É—â–Ω–æ—Å—Ç—å", "—Å–æ–∑–Ω–∞–Ω–∏–µ",
            "—á–∏—Å—Ç–æ–µ —Å–æ–∑–Ω–∞–Ω–∏–µ", "–±—ã—Ç–∏–µ", "–ø—Ä–∏—Å—É—Ç—Å—Ç–≤–∏–µ –±—ã—Ç–∏—è",
            
            # –ü—Å–∏—Ö–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ –∫–æ–Ω—Ü–µ–ø—Ç—ã
            "–ª–∏—á–Ω–æ—Å—Ç—å", "—ç–≥–æ", "–ª–æ–∂–Ω–æ–µ —è", "–∏—Å—Ç–∏–Ω–Ω–æ–µ —è", "–≤—ã—Å—à–µ–µ —è", "—Å–∞–º–æ—Å—Ç—å",
            "–∏–¥–µ–Ω—Ç–∏—á–Ω–æ—Å—Ç—å", "–ø—Ä–æ–µ–∫—Ü–∏—è", "–∏–Ω—Ç—Ä–æ–µ–∫—Ü–∏—è", "–∑–∞—â–∏—Ç–Ω—ã–µ –º–µ—Ö–∞–Ω–∏–∑–º—ã"
        ]
        
        # –ü–°–ò–•–û–õ–û–ì–ò–ß–ï–°–ö–ò–ï –°–û–°–¢–û–Ø–ù–ò–Ø (94 —É–∑–ª–∞)
        psychological_states = [
            # –ù–µ–≥–∞—Ç–∏–≤–Ω—ã–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è
            "—Å—Ç—Ä–∞–¥–∞–Ω–∏–µ", "–±–æ–ª—å", "–ø–µ—á–∞–ª—å", "–≥–æ—Ä–µ", "–¥–µ–ø—Ä–µ—Å—Å–∏—è", "—Ç—Ä–µ–≤–æ–≥–∞", "—Å—Ç—Ä–∞—Ö",
            "–æ–±–∏–¥–∞", "–≥–Ω–µ–≤", "—è—Ä–æ—Å—Ç—å", "–≤–∏–Ω–∞", "—Å—Ç—ã–¥", "–æ–¥–∏–Ω–æ—á–µ—Å—Ç–≤–æ", "–æ—Ç—á—É–∂–¥–µ–Ω–∏–µ",
            
            # –ü–æ–∑–∏—Ç–∏–≤–Ω—ã–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è
            "—Ä–∞–¥–æ—Å—Ç—å", "—Å—á–∞—Å—Ç—å–µ", "–±–ª–∞–∂–µ–Ω—Å—Ç–≤–æ", "–ª—é–±–æ–≤—å", "—Å–æ—Å—Ç—Ä–∞–¥–∞–Ω–∏–µ", "—ç–º–ø–∞—Ç–∏—è",
            "–±–ª–∞–≥–æ–¥–∞—Ä–Ω–æ—Å—Ç—å", "–ø—Ä–æ—â–µ–Ω–∏–µ", "–ø—Ä–∏–Ω—è—Ç–∏–µ", "–¥–æ–≤–µ—Ä–∏–µ", "–≤–µ—Ä–∞", "–ø–æ–∫–æ–π",
            "—É–º–∏—Ä–æ—Ç–≤–æ—Ä–µ–Ω–∏–µ", "–≥–∞—Ä–º–æ–Ω–∏—è", "–±–∞–ª–∞–Ω—Å", "—Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç—å", "—Å–≤–æ–±–æ–¥–∞",
            
            # –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è
            "–∏–Ω—Å–∞–π—Ç", "–ø—Ä–æ–∑—Ä–µ–Ω–∏–µ", "–æ–∑–∞—Ä–µ–Ω–∏–µ", "–æ—Ç–∫—Ä–æ–≤–µ–Ω–∏–µ", "–ø–æ–Ω–∏–º–∞–Ω–∏–µ", "—è—Å–Ω–æ—Å—Ç—å",
            "—Ç—Ä–∞–Ω—Å—Ü–µ–Ω–¥–µ–Ω—Ü–∏—è", "—ç–∫—Å—Ç–∞–∑", "–ø–æ—Ç–æ–∫", "—Ü–µ–Ω—Ç—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ—Å—Ç—å", "–∑–∞–∑–µ–º–ª–µ–Ω–Ω–æ—Å—Ç—å",
            
            # –°–ø–µ—Ü–∏—Ñ–∏—á–µ—Å–∫–∏–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è –Ω–µ–π—Ä–æ—Å—Ç–∞–ª–∫–∏–Ω–≥–∞
            "–∂–∏–≤–æ—Å—Ç—å", "–ø–æ–¥–ª–∏–Ω–Ω–æ—Å—Ç—å", "–∏—Å–∫—Ä–µ–Ω–Ω–æ—Å—Ç—å", "—á–µ—Å—Ç–Ω–æ—Å—Ç—å", "–æ—Ç–∫—Ä—ã—Ç–æ—Å—Ç—å",
            "—É—è–∑–≤–∏–º–æ—Å—Ç—å", "—Å–º–µ–ª–æ—Å—Ç—å", "–º—É–∂–µ—Å—Ç–≤–æ", "–ø–æ–∑–µ—Ä—Å—Ç–≤–æ", "—Ñ–∞–ª—å—à—å", "—Å–æ–ø—Ä–æ—Ç–∏–≤–ª–µ–Ω–∏–µ"
        ]
        
        # –ü–†–û–¶–ï–°–°–´ –¢–†–ê–ù–°–§–û–†–ú–ê–¶–ò–ò (101 —É–∑–µ–ª)
        transformation_processes = [
            # –ü—Ä–æ—Ü–µ—Å—Å—ã –ø–æ–∑–Ω–∞–Ω–∏—è
            "–ø–æ–∑–Ω–∞–Ω–∏–µ", "–∏–∑—É—á–µ–Ω–∏–µ", "–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ", "–∞–Ω–∞–ª–∏–∑", "—Å–∏–Ω—Ç–µ–∑", "–∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è",
            "–ø–æ–Ω–∏–º–∞–Ω–∏–µ", "–ø–æ—Å—Ç–∏–∂–µ–Ω–∏–µ", "–æ—Å–º—ã—Å–ª–µ–Ω–∏–µ", "–ø—Ä–æ–∑—Ä–µ–Ω–∏–µ", "–∏–Ω—Å–∞–π—Ç", "–∏–Ω—Ç—É–∏—Ü–∏—è",
            
            # –ü—Ä–æ—Ü–µ—Å—Å—ã —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏
            "–∏–∑–º–µ–Ω–µ–Ω–∏–µ", "—Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—è", "—ç–≤–æ–ª—é—Ü–∏—è", "—Ä–∞–∑–≤–∏—Ç–∏–µ", "—Ä–æ—Å—Ç", "—Å–æ–∑—Ä–µ–≤–∞–Ω–∏–µ",
            "–≤–æ–∑—Ä–æ–∂–¥–µ–Ω–∏–µ", "–º–µ—Ç–∞–º–æ—Ä—Ñ–æ–∑–∞", "–∞–ª—Ö–∏–º–∏—è", "—Ç—Ä–∞–Ω—Å–º—É—Ç–∞—Ü–∏—è", "–æ—á–∏—â–µ–Ω–∏–µ", "–∏—Å—Ü–µ–ª–µ–Ω–∏–µ",
            
            # –ü—Ä–æ—Ü–µ—Å—Å—ã –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏
            "–∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –æ–ø—ã—Ç–∞", "–æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ", "—Å–ª–∏—è–Ω–∏–µ", "–µ–¥–∏–Ω–µ–Ω–∏–µ", "–≥–∞—Ä–º–æ–Ω–∏–∑–∞—Ü–∏—è",
            "–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞", "—Å—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏—è", "—Ü–µ–Ω—Ç—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ", "–∑–∞–∑–µ–º–ª–µ–Ω–∏–µ", "—É–∫–æ—Ä–µ–Ω–µ–Ω–∏–µ",
            "–ø—Ä–∏–∑–Ω–∞–Ω–∏–µ", "–ø—Ä–∏–Ω—è—Ç–∏–µ", "–æ–¥–æ–±—Ä–µ–Ω–∏–µ", "–ø–æ–¥–¥–µ—Ä–∂–∫–∞",
            
            # –ü—Ä–æ—Ü–µ—Å—Å—ã –æ—Å–≤–æ–±–æ–∂–¥–µ–Ω–∏—è
            "–æ—Å–≤–æ–±–æ–∂–¥–µ–Ω–∏–µ", "–æ—Ç–ø—É—Å–∫–∞–Ω–∏–µ", "–æ—Ç–¥–∞—á–∞", "—Å–¥–∞—á–∞", "–∫–∞–ø–∏—Ç—É–ª—è—Ü–∏—è", "—Å–º–∏—Ä–µ–Ω–∏–µ",
            "–¥–æ–≤–µ—Ä–∏–µ", "–ø—Ä–µ–¥–∞–Ω–Ω–æ—Å—Ç—å", "—Å–ª—É–∂–µ–Ω–∏–µ", "–Ω–µ–ø—Ä–∏–≤—è–∑–∞–Ω–Ω–æ—Å—Ç—å", "–æ—Ç—Ä–µ—à–µ–Ω–Ω–æ—Å—Ç—å"
        ]
        
        # –ü–†–ê–ö–¢–ò–ö–ò –ò –¢–ï–•–ù–ò–ö–ò (77 —É–∑–ª–æ–≤)
        practices = [
            # –û—Å–Ω–æ–≤–Ω—ã–µ –ø—Ä–∞–∫—Ç–∏–∫–∏ –Ω–µ–π—Ä–æ—Å—Ç–∞–ª–∫–∏–Ω–≥–∞
            "–Ω–∞–±–ª—é–¥–µ–Ω–∏–µ –∑–∞ –Ω–∞–±–ª—é–¥–∞—é—â–∏–º", "–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–ª—è –≤–Ω–∏–º–∞–Ω–∏—è",
            "—Ä–∞–±–æ—Ç–∞ —Å –ø—Ä–∏—Å—É—Ç—Å—Ç–≤–∏–µ–º", "–¥—ã—Ö–∞—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–∞–∫—Ç–∏–∫–∏", "—Ç–µ–ª–µ—Å–Ω—ã–µ –ø—Ä–∞–∫—Ç–∏–∫–∏",
            
            # –ú–µ–¥–∏—Ç–∞—Ç–∏–≤–Ω—ã–µ –ø—Ä–∞–∫—Ç–∏–∫–∏
            "–º–µ–¥–∏—Ç–∞—Ü–∏—è", "–¥–∑–µ–Ω", "–¥–∑–∞–¥–∑–µ–Ω", "–∫–æ–∞–Ω", "–º–∞–Ω—Ç—Ä–∞", "–ø—Ä–∞–Ω–∞—è–º–∞", "–∞—Å–∞–Ω–∞",
            "–≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è", "–≤–æ–æ–±—Ä–∞–∂–µ–Ω–∏–µ",
            
            # –ü—Ä–∞–∫—Ç–∏–∫–∏ —Å–∞–º–æ–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è
            "—Å–∞–º–æ–Ω–∞–±–ª—é–¥–µ–Ω–∏–µ", "—Å–∞–º–æ–∞–Ω–∞–ª–∏–∑", "—Å–∞–º–æ—Ä–µ—Ñ–ª–µ–∫—Å–∏—è", "—Å–∞–º–æ–ø–æ–∑–Ω–∞–Ω–∏–µ",
            "—Å–∞–º–æ–ø—Ä–∏–Ω—è—Ç–∏–µ", "—Å–∞–º–æ–ø—Ä–æ—â–µ–Ω–∏–µ", "—Å–∞–º–æ–ª—é–±–æ–≤—å", "—Å–∞–º–æ—Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è"
        ]
        
        # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –∫–æ–Ω—Ü–µ–ø—Ç—ã
        all_concepts = (neurostalking_concepts + psychological_states + 
                       transformation_processes + practices)
        
        entities = []
        content_lower = content.lower()
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ (–≤—ã—Å–æ–∫–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç)
        entities.extend(keywords)
        
        # –ò—â–µ–º –∫–æ–Ω—Ü–µ–ø—Ç—ã –≤ —Ç–µ–∫—Å—Ç–µ
        for concept in all_concepts:
            if concept in content_lower:
                entities.append(concept)
        
        # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –∏ —Å–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—É
        unique_entities = list(set(entities))
        
        # –ü—Ä–∏–æ—Ä–∏—Ç–∏–∑–∞—Ü–∏—è: –∫–æ–Ω—Ü–µ–ø—Ç—ã –Ω–µ–π—Ä–æ—Å—Ç–∞–ª–∫–∏–Ω–≥–∞ –≤ –Ω–∞—á–∞–ª–µ
        prioritized_entities = []
        
        # –°–Ω–∞—á–∞–ª–∞ –¥–æ–±–∞–≤–ª—è–µ–º –∫–æ–Ω—Ü–µ–ø—Ç—ã –Ω–µ–π—Ä–æ—Å—Ç–∞–ª–∫–∏–Ω–≥–∞
        for entity in unique_entities:
            if entity in neurostalking_concepts:
                prioritized_entities.append(entity)
        
        # –ó–∞—Ç–µ–º –æ—Å—Ç–∞–ª—å–Ω—ã–µ
        for entity in unique_entities:
            if entity not in neurostalking_concepts and entity not in prioritized_entities:
                prioritized_entities.append(entity)
        
        # üöÄ –†–ê–°–®–ò–†–ï–ù–ù–´–ô –ª–∏–º–∏—Ç: –º–∞–∫—Å–∏–º—É–º 30 —Å—É—â–Ω–æ—Å—Ç–µ–π –¥–ª—è SAG v2.0
        raw_entities = prioritized_entities[:40]  # –£–≤–µ–ª–∏—á–∏–ª–∏ —Å 15 –¥–æ 40
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—É—é –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—é
        normalized_entities = self._normalize_graph_entities(raw_entities)
        
        return normalized_entities[:30]  # –£–≤–µ–ª–∏—á–∏–ª–∏ —Å 12 –¥–æ 30

    def _normalize_graph_entities(self, entities: list) -> list:
        """
        üöÄ –†–ê–°–®–ò–†–ï–ù–ù–ê–Ø –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –≥—Ä–∞—Ñ-—Å—É—â–Ω–æ—Å—Ç–µ–π: –∏—Å–ø–æ–ª—å–∑—É–µ—Ç 442 —É–∑–ª–∞ + 259 –æ—Ç–Ω–æ—à–µ–Ω–∏–π
        –ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç: —Å–∏–Ω–æ–Ω–∏–º—ã, —Ñ–∏–ª—å—Ç—Ä –æ–±—â–∏—Ö —Å–ª–æ–≤, –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è, –ø—Ä–∏–æ—Ä–∏—Ç–∏–∑–∞—Ü–∏—è –ø–æ –≥—Ä–∞—Ñ-–∫–æ–ª–ª–µ–∫—Ü–∏–∏
        """
        if not entities:
            return []
        
        # –§–∏–ª—å—Ç—Ä –∫–æ—Ä–æ—Ç–∫–∏—Ö –∏ –æ–±—â–µ—è–∑—ã–∫–æ–≤—ã—Ö —Å–ª–æ–≤
        min_length = 3  # –£–º–µ–Ω—å—à–∏–ª–∏ —Å 4 –¥–æ 3 –¥–ª—è –∑–∞—Ö–≤–∞—Ç–∞ –∫–æ—Ä–æ—Ç–∫–∏—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤
        
        normalized = []
        seen = set()
        
        for entity in entities:
            if not entity or not isinstance(entity, str):
                continue
                
            # –û—á–∏—Å—Ç–∫–∞ –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
            clean_entity = entity.strip().lower()
            
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ
            if len(clean_entity) < min_length:
                continue
            
            # –ü—Ä–∏–º–µ–Ω—è–µ–º —Å–∏–Ω–æ–Ω–∏–º—ã –∏–∑ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–π –∫–æ–ª–ª–µ–∫—Ü–∏–∏
            canonical_form = self.synonym_map.get(clean_entity, clean_entity)
            
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å—Ç–æ–ø-—Å–ª–æ–≤–∞
            if canonical_form in self.stop_words:
                continue
            
            # –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è
            if canonical_form in seen:
                continue
                
            seen.add(canonical_form)
            
            # üéØ –ü–†–ò–û–†–ò–¢–ò–ó–ê–¶–ò–Ø: —Å—É—â–Ω–æ—Å—Ç–∏ –∏–∑ –≥—Ä–∞—Ñ-–∫–æ–ª–ª–µ–∫—Ü–∏–∏ –ø–æ–ª—É—á–∞—é—Ç –≤—ã—Å—à–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç
            if canonical_form in self.graph_nodes:
                normalized.insert(0, canonical_form)  # –í –Ω–∞—á–∞–ª–æ —Å–ø–∏—Å–∫–∞
            elif canonical_form in self.graph_relationships:
                normalized.append(canonical_form)      # –í –∫–æ–Ω–µ—Ü —Å–ø–∏—Å–∫–∞
            else:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á–∞—Å—Ç–∏—á–Ω—ã–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è —Å –≥—Ä–∞—Ñ-—É–∑–ª–∞–º–∏
                partial_match = False
                for graph_node in self.graph_nodes:
                    if canonical_form in graph_node or graph_node in canonical_form:
                        normalized.append(canonical_form)
                        partial_match = True
                        break
                
                if not partial_match:
                    normalized.append(canonical_form)
        
        # üîÑ –†–ê–°–®–ò–†–ï–ù–ù–ê–Ø –§–ò–õ–¨–¢–†–ê–¶–ò–Ø: —É–≤–µ–ª–∏—á–∏–≤–∞–µ–º –ª–∏–º–∏—Ç —Å 12 –¥–æ 25-30
        if len(normalized) > 30:
            # –ü—Ä–∏–æ—Ä–∏—Ç–∏–∑–∏—Ä—É–µ–º –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º:
            # 1. –ö–æ–Ω—Ü–µ–ø—Ç—ã –Ω–µ–π—Ä–æ—Å—Ç–∞–ª–∫–∏–Ω–≥–∞ (–≤—ã—Å—à–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç)
            # 2. –ü—Å–∏—Ö–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è
            # 3. –ü—Ä–∞–∫—Ç–∏–∫–∏ –∏ —Ç–µ—Ö–Ω–∏–∫–∏
            # 4. –û—Å—Ç–∞–ª—å–Ω—ã–µ
            
            priority_order = []
            
            # –ö–æ–Ω—Ü–µ–ø—Ç—ã –Ω–µ–π—Ä–æ—Å—Ç–∞–ª–∫–∏–Ω–≥–∞ (–ø–µ—Ä–≤—ã–µ 74 —É–∑–ª–∞)
            neurostalking_concepts = [
                "–Ω–µ–π—Ä–æ—Å—Ç–∞–ª–∫–∏–Ω–≥", "–Ω–µ–æ—Å—Ç–∞–ª–∫–∏–Ω–≥", "–º–µ—Ç–∞–Ω–∞–±–ª—é–¥–µ–Ω–∏–µ", "–ø–æ–ª–µ –≤–Ω–∏–º–∞–Ω–∏—è",
                "–æ—Å–æ–∑–Ω–∞–≤–∞–Ω–∏–µ", "–ø—Ä–∏—Å—É—Ç—Å—Ç–≤–∏–µ", "—Å–∞–º–æ–æ—Å–æ–∑–Ω–∞–Ω–∏–µ", "—Ü–µ–Ω—Ç—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ"
            ]
            
            for concept in neurostalking_concepts:
                if concept in normalized:
                    priority_order.append(concept)
            
            # –î–æ–±–∞–≤–ª—è–µ–º –æ—Å—Ç–∞–ª—å–Ω—ã–µ –≥—Ä–∞—Ñ-—É–∑–ª—ã
            for entity in normalized:
                if entity in self.graph_nodes and entity not in priority_order:
                    priority_order.append(entity)
            
            # –î–æ–±–∞–≤–ª—è–µ–º –≥—Ä–∞—Ñ-–æ—Ç–Ω–æ—à–µ–Ω–∏—è
            for entity in normalized:
                if entity in self.graph_relationships and entity not in priority_order:
                    priority_order.append(entity)
            
            # –î–æ–±–∞–≤–ª—è–µ–º –æ—Å—Ç–∞–ª—å–Ω—ã–µ (–µ—Å–ª–∏ –º–µ—Å—Ç–æ –æ—Å—Ç–∞–ª–æ—Å—å)
            for entity in normalized:
                if entity not in priority_order and len(priority_order) < 30:
                    priority_order.append(entity)
            
            normalized = priority_order[:30]
        
        return normalized[:30]  # –£–≤–µ–ª–∏—á–∏–ª–∏ –ª–∏–º–∏—Ç —Å 12 –¥–æ 30

    def analyze_semantic_relationships(self, entities: list) -> dict:
        """
        üöÄ –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ —Å–≤—è–∑–∏ –º–µ–∂–¥—É –≥—Ä–∞—Ñ-—Å—É—â–Ω–æ—Å—Ç—è–º–∏
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É —Å–≤—è–∑–µ–π –¥–ª—è –±—É–¥—É—â–µ–π –≥—Ä–∞—Ñ-—Å–∏—Å—Ç–µ–º—ã
        """
        if not entities or len(entities) < 2:
            return {}
        
        relationships = {
            "conceptual_links": [],
            "causal_links": [],
            "practical_links": [],
            "temporal_links": [],
            "structural_links": [],
            "quality_links": [],
            "neurostalking_specific": []
        }
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –ø–∞—Ä—ã —Å—É—â–Ω–æ—Å—Ç–µ–π –Ω–∞ –ø—Ä–µ–¥–º–µ—Ç —Å–≤—è–∑–µ–π
        for i, entity1 in enumerate(entities):
            for j, entity2 in enumerate(entities[i+1:], i+1):
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Ç–∏–ø—ã —Å–≤—è–∑–µ–π
                
                # –ö–æ–Ω—Ü–µ–ø—Ç—É–∞–ª—å–Ω—ã–µ —Å–≤—è–∑–∏
                if self._check_conceptual_relationship(entity1, entity2):
                    relationships["conceptual_links"].append({
                        "source": entity1,
                        "target": entity2,
                        "type": "conceptual",
                        "strength": self._calculate_relationship_strength(entity1, entity2)
                    })
                
                # –ö–∞—É–∑–∞–ª—å–Ω—ã–µ —Å–≤—è–∑–∏
                if self._check_causal_relationship(entity1, entity2):
                    relationships["causal_links"].append({
                        "source": entity1,
                        "target": entity2,
                        "type": "causal",
                        "strength": self._calculate_relationship_strength(entity1, entity2)
                    })
                
                # –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —Å–≤—è–∑–∏
                if self._check_practical_relationship(entity1, entity2):
                    relationships["practical_links"].append({
                        "source": entity1,
                        "target": entity2,
                        "type": "practical",
                        "strength": self._calculate_relationship_strength(entity1, entity2)
                    })
        
        return relationships
    
    def _check_conceptual_relationship(self, entity1: str, entity2: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –∫–æ–Ω—Ü–µ–ø—Ç—É–∞–ª—å–Ω—É—é —Å–≤—è–∑—å –º–µ–∂–¥—É —Å—É—â–Ω–æ—Å—Ç—è–º–∏"""
        # –ö–æ–Ω—Ü–µ–ø—Ç—É–∞–ª—å–Ω—ã–µ —Å–≤—è–∑–∏: –≤–∫–ª—é—á–µ–Ω–∏–µ, —Å—Ö–æ–¥—Å—Ç–≤–æ, –ø—Ä–æ—Ç–∏–≤–æ–ø–æ–ª–æ–∂–Ω–æ—Å—Ç—å
        conceptual_patterns = [
            (entity1, entity2, "–≤–∫–ª—é—á–∞–µ—Ç"),
            (entity1, entity2, "—Å–æ–¥–µ—Ä–∂–∏—Ç"),
            (entity1, entity2, "–ø–æ—Ö–æ–∂–µ"),
            (entity1, entity2, "–ø—Ä–æ—Ç–∏–≤–æ–ø–æ–ª–æ–∂–Ω–æ"),
            (entity1, entity2, "–∞–Ω–∞–ª–æ–≥–∏—á–Ω–æ")
        ]
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —Ç–∞–∫–∏–µ –æ—Ç–Ω–æ—à–µ–Ω–∏—è –≤ –≥—Ä–∞—Ñ-–∫–æ–ª–ª–µ–∫—Ü–∏–∏
        for pattern in conceptual_patterns:
            if any(rel in self.graph_relationships for rel in [pattern[2], f"{pattern[2]}_–Ω–∞", f"{pattern[2]}_—Å"]):
                return True
        
        return False
    
    def _check_causal_relationship(self, entity1: str, entity2: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –∫–∞—É–∑–∞–ª—å–Ω—É—é —Å–≤—è–∑—å –º–µ–∂–¥—É —Å—É—â–Ω–æ—Å—Ç—è–º–∏"""
        # –ö–∞—É–∑–∞–ª—å–Ω—ã–µ —Å–≤—è–∑–∏: –ø—Ä–∏—á–∏–Ω–∞-—Å–ª–µ–¥—Å—Ç–≤–∏–µ, –≤–µ–¥–µ—Ç –∫
        causal_patterns = [
            (entity1, entity2, "–≤–µ–¥–µ—Ç"),
            (entity1, entity2, "–ø—Ä–∏–≤–æ–¥–∏—Ç"),
            (entity1, entity2, "–≤—ã–∑—ã–≤–∞–µ—Ç"),
            (entity1, entity2, "–ø–æ—Ä–æ–∂–¥–∞–µ—Ç"),
            (entity1, entity2, "—Å–æ–∑–¥–∞–µ—Ç")
        ]
        
        for pattern in causal_patterns:
            if any(rel in self.graph_relationships for rel in [pattern[2], f"{pattern[2]}_–∫", f"{pattern[2]}_–≤"]):
                return True
        
        return False
    
    def _check_practical_relationship(self, entity1: str, entity2: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫—É—é —Å–≤—è–∑—å –º–µ–∂–¥—É —Å—É—â–Ω–æ—Å—Ç—è–º–∏"""
        # –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —Å–≤—è–∑–∏: –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è, –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è –≤
        practical_patterns = [
            (entity1, entity2, "–∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è"),
            (entity1, entity2, "–ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è"),
            (entity1, entity2, "—Å–ª—É–∂–∏—Ç"),
            (entity1, entity2, "–ø—Ä–µ–¥–Ω–∞–∑–Ω–∞—á–µ–Ω–æ"),
            (entity1, entity2, "–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–æ")
        ]
        
        for pattern in practical_patterns:
            if any(rel in self.graph_relationships for rel in [pattern[2], f"{pattern[2]}_–¥–ª—è", f"{pattern[2]}_–≤", f"{pattern[2]}_–Ω–∞"]):
                return True
        
        return False
    
    def _calculate_relationship_strength(self, entity1: str, entity2: str) -> float:
        """–í—ã—á–∏—Å–ª—è–µ—Ç —Å–∏–ª—É —Å–≤—è–∑–∏ –º–µ–∂–¥—É —Å—É—â–Ω–æ—Å—Ç—è–º–∏ (0.0 - 1.0)"""
        strength = 0.0
        
        # –ë–∞–∑–æ–≤–∞—è —Å–∏–ª–∞: –æ–±–∞ –≤ –≥—Ä–∞—Ñ-–∫–æ–ª–ª–µ–∫—Ü–∏–∏
        if entity1 in self.graph_nodes and entity2 in self.graph_nodes:
            strength += 0.3
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è —Å–∏–ª–∞: –∫–æ–Ω—Ü–µ–ø—Ç—É–∞–ª—å–Ω–∞—è –±–ª–∏–∑–æ—Å—Ç—å
        if self._check_conceptual_relationship(entity1, entity2):
            strength += 0.4
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è —Å–∏–ª–∞: –∫–∞—É–∑–∞–ª—å–Ω–∞—è —Å–≤—è–∑—å
        if self._check_causal_relationship(entity1, entity2):
            strength += 0.3
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è —Å–∏–ª–∞: –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∞—è —Å–≤—è–∑—å
        if self._check_practical_relationship(entity1, entity2):
            strength += 0.2
        
        return min(strength, 1.0)



    def detect_has_dialogue(self, blocks: list, full_text: str = None) -> bool:
        """
        –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –µ—Å—Ç—å –ª–∏ –¥–∏–∞–ª–æ–≥–∏ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ (–∞–Ω–∞–ª–∏–∑ –±–ª–æ–∫–æ–≤ + full_text)
        """
        
        # 1. –ü–†–ò–û–†–ò–¢–ï–¢: –ê–Ω–∞–ª–∏–∑ full_text (–±–æ–ª–µ–µ –Ω–∞–¥–µ–∂–Ω—ã–π)
        if full_text:
            # –î–∏–∞–ª–æ–≥–æ–≤—ã–µ –º–∞—Ä–∫–µ—Ä—ã –≤ –ø–æ–ª–Ω–æ–º —Ç–µ–∫—Å—Ç–µ
            if ">>" in full_text:
                return True
            
            # –í–æ–ø—Ä–æ—Å–∏—Ç–µ–ª—å–Ω—ã–µ –∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –≤ –∫–æ–Ω—Ü–µ —Å—Ç—Ä–æ–∫
            import re
            if re.search(r'\?\s*$', full_text, re.MULTILINE):
                return True
            
            # –ß–∞—Å—Ç—ã–µ –¥–∏–∞–ª–æ–≥–æ–≤—ã–µ —Ñ—Ä–∞–∑—ã
            dialogue_markers = [
                "–º–æ–∂–µ—Ç–µ —Å–ø—Ä–æ—Å–∏—Ç—å", "–¥–∞, –≤–æ–ø—Ä–æ—Å", "–≤–æ–ø—Ä–æ—Å –µ—Å—Ç—å", 
                "—Å–ø—Ä–∞—à–∏–≤–∞", "–æ—Ç–≤–µ—á–∞", "–∞ –µ—Å–ª–∏", "–º–æ–∂–Ω–æ –ª–∏",
                "–∫–∞–∫ –∂–µ", "—á—Ç–æ –¥–µ–ª–∞—Ç—å", "–ø–æ—á–µ–º—É —Ç–∞–∫"
            ]
            
            full_text_lower = full_text.lower()
            for marker in dialogue_markers:
                if marker in full_text_lower:
                    return True
        
        # 2. –ê–Ω–∞–ª–∏–∑ –±–ª–æ–∫–æ–≤ (—Ä–µ–∑–µ—Ä–≤–Ω—ã–π)
        for block in blocks:
            content = block.get("content", "")
            block_type = block.get("block_type", "")
            
            # –î–∏–∞–ª–æ–≥–æ–≤—ã–µ –º–∞—Ä–∫–µ—Ä—ã –≤ –±–ª–æ–∫–∞—Ö
            if ">>" in content:
                return True
            if block_type in ["dialogue", "question"]:
                return True
            
            # –í–æ–ø—Ä–æ—Å–∏—Ç–µ–ª—å–Ω—ã–µ –∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏
            if content.strip().endswith("?"):
                return True
            if any(marker in content.lower() for marker in [
                "–º–æ–∂–µ—Ç–µ —Å–ø—Ä–æ—Å–∏—Ç—å", "–≤–æ–ø—Ä–æ—Å", "—Å–ø—Ä–∞—à–∏–≤–∞", "–æ—Ç–≤–µ—á–∞"
            ]):
                return True
        
        return False

    def structure_dialogue_block(self, content: str) -> dict:
        """
        –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä—É–µ—Ç –¥–∏–∞–ª–æ–≥–æ–≤—ã–π –±–ª–æ–∫ –¥–ª—è SAG-—Å–∏—Å—Ç–µ–º—ã v2.1 —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–µ–π
        """
        if ">>" not in content:
            return {}
        
        # –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ —Ä–µ–ø–ª–∏–∫–∏ –ø–æ –º–∞—Ä–∫–µ—Ä—É >>
        parts = content.split(">>")
        dialogue_structure = []
        dialogue_turn = 0
        
        for i, part in enumerate(parts):
            part = part.strip()
            if not part:
                continue
            
            dialogue_turn += 1
            
            # –£–ª—É—á—à–µ–Ω–Ω–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–ø–∏–∫–µ—Ä–∞ –∏ —Ç–∏–ø–∞ —Ä–µ–ø–ª–∏–∫–∏
            is_question = part.endswith("?") or any(marker in part.lower() for marker in [
                "–º–æ–∂–µ—Ç–µ —Å–ø—Ä–æ—Å–∏—Ç—å", "–≤–æ–ø—Ä–æ—Å", "–∫–∞–∫", "—á—Ç–æ", "–ø–æ—á–µ–º—É", "–∑–∞—á–µ–º", "–≥–¥–µ", "–∫–æ–≥–¥–∞"
            ])
            
            # –≠–≤—Ä–∏—Å—Ç–∏–∫–∞ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Å–ø–∏–∫–µ—Ä–∞
            if i == 0:  # –ü–µ—Ä–≤–∞—è —á–∞—Å—Ç—å –æ–±—ã—á–Ω–æ –°–∞—Ä—Å–µ–∫–µ–Ω–æ–≤
                speaker = "sarsekenov"
                question_type = None
                answer_completeness = None
            elif is_question and len(part) < 200:  # –ö–æ—Ä–æ—Ç–∫–∏–µ –≤–æ–ø—Ä–æ—Å—ã - —É—á–∞—Å—Ç–Ω–∏–∫–∏
                speaker = "participant"
                # –£–ª—É—á—à–µ–Ω–Ω–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Ç–∏–ø–∞ –≤–æ–ø—Ä–æ—Å–∞
                part_lower = part.lower()
                if any(kw in part_lower for kw in ["–∫–∞–∫", "–º–æ–∂–Ω–æ –ª–∏", "—á—Ç–æ –¥–µ–ª–∞—Ç—å", "–∫–∞–∫ –ø—Ä–∏–º–µ–Ω–∏—Ç—å"]):
                    question_type = "practical"
                elif any(kw in part_lower for kw in ["—á—Ç–æ —Ç–∞–∫–æ–µ", "–ø–æ—á–µ–º—É", "–≤ —á–µ–º", "–∑–∞—á–µ–º"]):
                    question_type = "theoretical"  
                elif any(kw in part_lower for kw in ["—É –º–µ–Ω—è", "—Å–æ –º–Ω–æ–π", "–º–Ω–µ", "—è"]):
                    question_type = "personal"
                else:
                    question_type = "clarification"
                answer_completeness = None
            else:  # –î–ª–∏–Ω–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã - –°–∞—Ä—Å–µ–∫–µ–Ω–æ–≤
                speaker = "sarsekenov"
                question_type = None
                # –û—Ü–µ–Ω–∫–∞ –ø–æ–ª–Ω–æ—Ç—ã –æ—Ç–≤–µ—Ç–∞
                if len(part) > 500:
                    answer_completeness = "full"
                elif len(part) > 200:
                    answer_completeness = "partial"
                elif any(redirect in part.lower() for redirect in [
                    "—ç—Ç–æ –æ—Ç–¥–µ–ª—å–Ω–∞—è —Ç–µ–º–∞", "–æ–± —ç—Ç–æ–º –ø–æ–∑–∂–µ", "–Ω–µ —Å–µ–π—á–∞—Å", "–ø–æ—Ç–æ–º"
                ]):
                    answer_completeness = "redirected"
                else:
                    answer_completeness = "partial"
            
            dialogue_structure.append({
                "turn_id": dialogue_turn,
                "speaker": speaker,
                "content": part,
                "question_type": question_type,
                "answer_completeness": answer_completeness,
                "word_count": len(part.split()),
                "char_length": len(part)
            })
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤
        participant_turns = [turn for turn in dialogue_structure if turn["speaker"] == "participant"]
        sarsekenov_turns = [turn for turn in dialogue_structure if turn["speaker"] == "sarsekenov"]
        
        # –£–ª—É—á—à–µ–Ω–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è
        quality_factors = []
        
        # –§–∞–∫—Ç–æ—Ä 1: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–±–º–µ–Ω–æ–≤ (–±–æ–ª—å—à–µ = –ª—É—á—à–µ, –Ω–æ –Ω–µ –ª–∏–Ω–µ–π–Ω–æ)
        exchange_count = min(len(participant_turns), len(sarsekenov_turns))
        turn_score = min(4.0, exchange_count * 1.5)
        quality_factors.append(turn_score)
        
        # –§–∞–∫—Ç–æ—Ä 2: –†–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ —Ç–∏–ø–æ–≤ –≤–æ–ø—Ä–æ—Å–æ–≤
        question_types = set(turn["question_type"] for turn in participant_turns if turn["question_type"])
        diversity_score = len(question_types) * 1.2
        quality_factors.append(diversity_score)
        
        # –§–∞–∫—Ç–æ—Ä 3: –ü–æ–ª–Ω–æ—Ç–∞ –æ—Ç–≤–µ—Ç–æ–≤
        full_answers = len([t for t in sarsekenov_turns if t["answer_completeness"] == "full"])
        completeness_score = full_answers * 1.8
        quality_factors.append(completeness_score)
        
        # –§–∞–∫—Ç–æ—Ä 4: –†–∞–∑–≤–µ—Ä–Ω—É—Ç–æ—Å—Ç—å –æ—Ç–≤–µ—Ç–æ–≤
        avg_answer_length = sum(t["word_count"] for t in sarsekenov_turns) / len(sarsekenov_turns) if sarsekenov_turns else 0
        if avg_answer_length > 100:
            length_score = 2.0
        elif avg_answer_length > 50:
            length_score = 1.0
        else:
            length_score = 0.5
        quality_factors.append(length_score)
        
        # –ò—Ç–æ–≥–æ–≤–∞—è –æ—Ü–µ–Ω–∫–∞ (1-10)
        interaction_quality = min(10.0, max(3.0, sum(quality_factors)))
        
        return {
            "dialogue_structure": dialogue_structure,
            "interaction_quality": round(interaction_quality, 1),
            "total_turns": dialogue_turn,
            "participant_turns": len(participant_turns),
            "sarsekenov_turns": len(sarsekenov_turns),
            "total_exchanges": exchange_count,
            "question_types": list(question_types),
            "question_count": len([t for t in dialogue_structure if t["question_type"]]),
            "answer_count": len([t for t in dialogue_structure if t["answer_completeness"]]),
            "full_answers": full_answers,
            "avg_answer_length": round(avg_answer_length, 1)
        }

    def determine_collection_target(self, main_topics: list, keywords: list, has_dialogue: bool, 
                                   blocks: list) -> tuple:
        """
        –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ü–µ–ª–µ–≤—É—é –∫–æ–ª–ª–µ–∫—Ü–∏—é –¥–ª—è –±–ª–æ–∫–∞ –≤ –≥—Ä–∞—Ñ-—Å–∏—Å—Ç–µ–º–µ SAG
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç: (collection_target, routing_confidence)
        """
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
        all_keywords = main_topics + keywords
        keywords_str = " ".join(all_keywords).lower()
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ –±–ª–æ–∫–æ–≤ –¥–ª—è –±–æ–ª–µ–µ —Ç–æ—á–Ω–æ–≥–æ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
        all_content = " ".join([block.get("content", "") for block in blocks]).lower()
        
        # –ï—Å–ª–∏ –µ—Å—Ç—å –¥–∏–∞–ª–æ–≥ - –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç dialogue_sessions
        if has_dialogue:
            return "dialogue_sessions"
        
        # –ú–∞—Ä–∫–µ—Ä—ã –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –∫–æ–ª–ª–µ–∫—Ü–∏–π (–Ω–∞ –æ—Å–Ω–æ–≤–µ –≥—Ä–∞—Ñ-—Å–∏—Å—Ç–µ–º—ã)
        collection_markers = {
            "neurostalking_basics": [
                "–Ω–µ–π—Ä–æ—Å—Ç–∞–ª–∫–∏–Ω–≥", "–Ω–µ–æ—Å—Ç–∞–ª–∫–∏–Ω–≥", "–æ—Å–Ω–æ–≤—ã", "–∫–æ–Ω—Ü–µ–ø—Ü–∏—è", 
                "–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ", "—Ç–µ—Ä–º–∏–Ω–æ–ª–æ–≥–∏—è", "—Å–∏—Å—Ç–µ–º–∞", "–º–µ—Ç–∞–Ω–∞–±–ª—é–¥–µ–Ω–∏–µ",
                "–ø–æ–ª–µ –≤–Ω–∏–º–∞–Ω–∏—è", "—Å–≤–æ–±–æ–¥–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ", "—Å—Ç–∞–ª–∫–∏–Ω–≥ —É–º–∞"
            ],
            "meditation_practices": [
                "–º–µ–¥–∏—Ç–∞—Ü–∏—è", "–ø—Ä–∞–∫—Ç–∏–∫–∞", "—É–ø—Ä–∞–∂–Ω–µ–Ω–∏–µ", "–¥—ã—Ö–∞–Ω–∏–µ", "–ø—Ä–∞–Ω–∞—è–º–∞",
                "—Ü–µ–Ω—Ç—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ", "–∑–∞–∑–µ–º–ª–µ–Ω–∏–µ", "—Ç–µ—Ö–Ω–∏–∫–∞", "–≤–∏–ø–∞—à—å—è–Ω–∞", "—à–∞–º–∞—Ç—Ö–∞",
                "–Ω–∞–±–ª—é–¥–µ–Ω–∏–µ –∑–∞ –Ω–∞–±–ª—é–¥–∞—é—â–∏–º", "–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–ª—è –≤–Ω–∏–º–∞–Ω–∏—è"
            ],
            "healing_transformation": [
                "–∏—Å—Ü–µ–ª–µ–Ω–∏–µ", "—Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—è", "–ø—Ä–µ–æ–¥–æ–ª–µ–Ω–∏–µ", "–±–æ–ª—å", "—Å—Ç—Ä–∞–¥–∞–Ω–∏–µ",
                "–æ–±–∏–¥–∞", "–ø—Ä–æ—â–µ–Ω–∏–µ", "–ø—Ä–∏–Ω—è—Ç–∏–µ", "–∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è", "–æ—Å–≤–æ–±–æ–∂–¥–µ–Ω–∏–µ",
                "–æ—Ç–ø—É—Å–∫–∞–Ω–∏–µ", "—Å–¥–∞—á–∞", "–∫–∞–ø–∏—Ç—É–ª—è—Ü–∏—è", "—Å–º–∏—Ä–µ–Ω–∏–µ"
            ],
            "advanced_concepts": [
                "–ø—Ä–æ–±—É–∂–¥–µ–Ω–∏–µ", "–ø—Ä–æ—Å–≤–µ—Ç–ª–µ–Ω–∏–µ", "—Å–æ–∑–Ω–∞–Ω–∏–µ", "–∞–±—Å–æ–ª—é—Ç", "–±—ã—Ç–∏–µ", 
                "—Å—É—â–Ω–æ—Å—Ç—å", "–±–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å", "–º—É–¥—Ä–æ—Å—Ç—å", "–µ–¥–∏–Ω—Å—Ç–≤–æ", "–Ω–µ–¥—É–∞–ª—å–Ω–æ—Å—Ç—å",
                "—á–∏—Å—Ç–æ–µ —Å–æ–∑–Ω–∞–Ω–∏–µ", "–ø—Ä–∏—Å—É—Ç—Å—Ç–≤–∏–µ –±—ã—Ç–∏—è", "–∏—Å—Ç–∏–Ω–∞"
            ],
            "psychological_states": [
                "—Å–æ—Å—Ç–æ—è–Ω–∏–µ", "—ç–º–æ—Ü–∏—è", "—á—É–≤—Å—Ç–≤–æ", "–ø–µ—Ä–µ–∂–∏–≤–∞–Ω–∏–µ", "–Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ",
                "—Ä–∞–¥–æ—Å—Ç—å", "—Å—á–∞—Å—Ç—å–µ", "–ø–µ—á–∞–ª—å", "–≥–Ω–µ–≤", "—Å—Ç—Ä–∞—Ö", "—Ç—Ä–µ–≤–æ–≥–∞",
                "–ø–æ–∫–æ–π", "—É–º–∏—Ä–æ—Ç–≤–æ—Ä–µ–Ω–∏–µ", "–±–ª–∞–∂–µ–Ω—Å—Ç–≤–æ", "—ç–∫—Å—Ç–∞–∑"
            ],
            "spiritual_development": [
                "—Ä–∞–∑–≤–∏—Ç–∏–µ", "—ç–≤–æ–ª—é—Ü–∏—è", "—Ä–æ—Å—Ç", "—Å–æ–∑—Ä–µ–≤–∞–Ω–∏–µ", "–ø—Ä–æ–≥—Ä–µ—Å—Å",
                "—Å–∞–º–æ–ø–æ–∑–Ω–∞–Ω–∏–µ", "—Å–∞–º–æ–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ", "—Å–∞–º–æ—Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è",
                "–¥—É—Ö–æ–≤–Ω–æ—Å—Ç—å", "–¥—É—Ö–æ–≤–Ω—ã–π –ø—É—Ç—å", "–ø—É—Ç—å", "–ø—Ä–∞–∫—Ç–∏–∫—É—é—â–∏–π"
            ]
        }
        
        # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –¥–ª—è keywords
        keyword_scores = {}
        for collection, markers in collection_markers.items():
            score = sum(1 for marker in markers if marker in keywords_str)
            keyword_scores[collection] = score
        
        # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –¥–ª—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞ (—Å –º–µ–Ω—å—à–∏–º –≤–µ—Å–æ–º)
        content_scores = {}
        for collection, markers in collection_markers.items():
            score = sum(1 for marker in markers if marker in all_content)
            content_scores[collection] = score * 0.5  # –ú–µ–Ω—å—à–∏–π –≤–µ—Å –¥–ª—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –æ—Ü–µ–Ω–∫–∏
        total_scores = {}
        for collection in collection_markers.keys():
            total_scores[collection] = keyword_scores[collection] + content_scores[collection]
        
        # –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∞–≤–∏–ª–∞ –¥–ª—è –±–æ–ª–µ–µ —Ç–æ—á–Ω–æ–≥–æ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
        
        # –ï—Å–ª–∏ –º–Ω–æ–≥–æ –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤ - –º–µ–¥–∏—Ç–∞—Ç–∏–≤–Ω—ã–µ –ø—Ä–∞–∫—Ç–∏–∫–∏
        practice_terms = ["—É–ø—Ä–∞–∂–Ω–µ–Ω–∏–µ", "—Ç–µ—Ö–Ω–∏–∫–∞", "–º–µ—Ç–æ–¥", "—Å–ø–æ—Å–æ–±", "–ø—Ä–∞–∫—Ç–∏–∫–∞"]
        practice_count = sum(1 for term in practice_terms if term in keywords_str)
        if practice_count >= 2:
            total_scores["meditation_practices"] += 2.0
        
        # –ï—Å–ª–∏ –µ—Å—Ç—å —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã - –ø—Å–∏—Ö–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è
        emotion_terms = ["—á—É–≤—Å—Ç–≤–æ", "—ç–º–æ—Ü–∏—è", "–ø–µ—Ä–µ–∂–∏–≤–∞–Ω–∏–µ", "—Å–æ—Å—Ç–æ—è–Ω–∏–µ"]
        emotion_count = sum(1 for term in emotion_terms if term in keywords_str)
        if emotion_count >= 2:
            total_scores["psychological_states"] += 2.0
        
        # –ï—Å–ª–∏ –µ—Å—Ç—å –≥–ª—É–±–æ–∫–∏–µ –∫–æ–Ω—Ü–µ–ø—Ç—ã - –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ –∫–æ–Ω—Ü–µ–ø—Ç—ã
        deep_terms = ["—Å—É—â–Ω–æ—Å—Ç—å", "–∞–±—Å–æ–ª—é—Ç", "–±—ã—Ç–∏–µ", "–∏—Å—Ç–∏–Ω–∞", "–±–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å"]
        deep_count = sum(1 for term in deep_terms if term in keywords_str)
        if deep_count >= 1:
            total_scores["advanced_concepts"] += 2.0
        
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –∫–æ–ª–ª–µ–∫—Ü–∏—é —Å –Ω–∞–∏–±–æ–ª—å—à–∏–º —Å—á–µ—Ç–æ–º + confidence
        if max(total_scores.values()) > 0:
            best_collection = max(total_scores.keys(), key=lambda k: total_scores[k])
            best_score = total_scores[best_collection]
            total_possible_score = sum(total_scores.values())
            
            # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å
            if has_dialogue:
                # –î–ª—è –¥–∏–∞–ª–æ–≥–æ–≤ –≤—ã—Å–æ–∫–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å
                confidence = 0.95
            elif total_possible_score == 0:
                confidence = 0.3  # –ù–∏–∑–∫–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –ø—Ä–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–∏ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π
            else:
                # –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å = –¥–æ–ª—è –ª—É—á—à–µ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –æ—Ç –æ–±—â–µ–≥–æ
                confidence = min(0.95, max(0.4, best_score / total_possible_score))
                
                # –ë–æ–Ω—É—Å –∑–∞ –≤—ã—Å–æ–∫–∏–µ –∞–±—Å–æ–ª—é—Ç–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
                if best_score >= 5:
                    confidence = min(0.95, confidence + 0.1)
                elif best_score >= 3:
                    confidence = min(0.9, confidence + 0.05)
            
            return best_collection, round(confidence, 2)
        
        # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é - –æ—Å–Ω–æ–≤—ã –Ω–µ–π—Ä–æ—Å—Ç–∞–ª–∫–∏–Ω–≥–∞ —Å –Ω–∏–∑–∫–æ–π —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å—é
        return "neurostalking_basics", 0.3

    # ---------------------- HELPER FUNCTIONS FOR METADATA ---------------------- #
    
    def _calculate_document_metadata(self, blocks: list, full_text: str) -> dict:
        """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç –≤—Å–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –æ–¥–Ω–∏–º –≤—ã–∑–æ–≤–æ–º –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏"""
        has_dialogue = self.detect_has_dialogue(blocks, full_text)
        main_topics = self._extract_main_topics(blocks)
        all_keywords = self._get_all_keywords(blocks)
        # –í–†–ï–ú–ï–ù–ù–û: –±–µ–∑ routing_confidence –∏–∑-–∑–∞ –æ—à–∏–±–∫–∏ —Ä–∞—Å–ø–∞–∫–æ–≤–∫–∏
        collection_result = self.determine_collection_target(main_topics, all_keywords, has_dialogue, blocks)
        if isinstance(collection_result, tuple):
            collection_target, routing_confidence = collection_result
        else:
            collection_target = collection_result
            routing_confidence = 0.7  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é
        
        return {
            "recording_date": None,
            "published_date": self._extract_published_date(full_text),  # –î–æ–±–∞–≤–ª–µ–Ω–æ –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å ChromaDB
            "lecture_type": "seminar" if has_dialogue else "lecture",
            "has_dialogue": has_dialogue,
            "main_topics": main_topics,
            "difficulty_level": self._calculate_difficulty_level(blocks),
            "collection_target": collection_target,
            "routing_confidence": routing_confidence
        }
    
    def _extract_main_topics(self, blocks: list) -> list:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –≥–ª–∞–≤–Ω—ã–µ —Ç–µ–º—ã –∏–∑ –≤—Å–µ—Ö –±–ª–æ–∫–æ–≤"""
        all_keywords = []
        for block in blocks:
            all_keywords.extend(block.get("keywords", []))
        
        # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º —á–∞—Å—Ç–æ—Ç—É
        keyword_counts = {}
        for keyword in all_keywords:
            keyword_counts[keyword] = keyword_counts.get(keyword, 0) + 1
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —á–∞—Å—Ç–æ—Ç–µ –∏ –±–µ—Ä–µ–º —Ç–æ–ø-5
        sorted_keywords = sorted(keyword_counts.items(), key=lambda x: x[1], reverse=True)
        return [kw[0] for kw in sorted_keywords[:5]]
    
    def _get_all_keywords(self, blocks: list) -> list:
        """–ü–æ–ª—É—á–∞–µ—Ç –≤—Å–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏–∑ –±–ª–æ–∫–æ–≤"""
        all_keywords = []
        for block in blocks:
            all_keywords.extend(block.get("keywords", []))
        return list(set(all_keywords))  # –£–Ω–∏–∫–∞–ª—å–Ω—ã–µ
    
    def _calculate_difficulty_level(self, blocks: list) -> str:
        """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç —Å—Ä–µ–¥–Ω–∏–π —É—Ä–æ–≤–µ–Ω—å —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        complexity_scores = [block.get("complexity_score", 5.0) for block in blocks]
        avg_complexity = sum(complexity_scores) / len(complexity_scores) if complexity_scores else 5.0
        
        if avg_complexity < 4.0:
            return "beginner"
        elif avg_complexity < 7.0:
            return "intermediate"
        else:
            return "advanced"
    
    def _estimate_participant_count(self, blocks: list) -> int:
        """–ü—Ä–∏–º–µ—Ä–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤ –ø–æ –¥–∏–∞–ª–æ–≥–∞–º"""
        dialogue_blocks = [b for b in blocks if b.get("block_type") == "dialogue"]
        if not dialogue_blocks:
            return None
        
        # –ü—Ä–∏–º–µ—Ä–Ω–∞—è –æ—Ü–µ–Ω–∫–∞: 1-2 —É—á–∞—Å—Ç–Ω–∏–∫–∞ –Ω–∞ –∫–∞–∂–¥—ã–µ 2-3 –¥–∏–∞–ª–æ–≥–æ–≤—ã—Ö –±–ª–æ–∫–∞
        return max(1, len(dialogue_blocks) // 2)
    
    def _calculate_duration_minutes(self, blocks: list) -> int:
        """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∏–∑ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –º–µ—Ç–æ–∫"""
        if not blocks:
            return None
        
        try:
            last_block = blocks[-1]
            end_time = last_block.get("end", "00:00:00")
            # –ü–∞—Ä—Å–∏–º –≤—Ä–µ–º—è –≤ —Ñ–æ—Ä–º–∞—Ç–µ HH:MM:SS
            h, m, s = map(int, end_time.split(":"))
            return h * 60 + m + (1 if s > 0 else 0)
        except:
            return None

    def _extract_published_date(self, full_text: str) -> Optional[str]:
        """–ü—ã—Ç–∞–µ—Ç—Å—è –∏–∑–≤–ª–µ—á—å –¥–∞—Ç—É –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ –∏–∑ –ø–æ–ª–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –∏–ª–∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–µ–∫—É—â—É—é"""
        import datetime
        # –í –±—É–¥—É—â–µ–º –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –ª–æ–≥–∏–∫—É –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –¥–∞—Ç—ã –∏–∑ —Ç–µ–∫—Å—Ç–∞
        # –ü–æ–∫–∞ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–µ–∫—É—â—É—é –¥–∞—Ç—É –∫–∞–∫ –∑–∞–≥–ª—É—à–∫—É, –µ—Å–ª–∏ –¥–∞—Ç–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞
        return datetime.datetime.now().strftime("%Y-%m-%dT%H:%M:%SZ")

    # ---------------------- IO ---------------------- #
    def _load_config(self) -> dict:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –∏–∑ config.yaml"""
        config_path = PROJECT_ROOT / "config.yaml"
        if config_path.exists():
            try:
                with open(config_path, 'r', encoding='utf-8') as f:
                    return yaml.safe_load(f) or {}
            except Exception:
                # –ï—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç–æ–π —Å–ª–æ–≤–∞—Ä—å —Å –¥–µ—Ñ–æ–ª—Ç–Ω—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏
                return {
                    'pipeline': {
                        'sag_v2': {
                            'use_safety_extractor': True,
                            'use_causal_chain_extractor': True,
                            'use_concept_hierarchy_extractor': True,
                            'use_case_study_extractor': True,
                            'use_prerequisite_extractor': True,
                            'models': {
                                'safety': 'gpt-4o-mini',
                                'causal': 'gpt-4o-mini',
                                'hierarchy': 'gpt-4o-mini',
                                'case_study': 'gpt-4o-mini',
                                'prerequisite': 'gpt-4o-mini'
                            }
                        }
                    }
                }
        else:
            # –ï—Å–ª–∏ —Ñ–∞–π–ª –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –¥–µ—Ñ–æ–ª—Ç–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
            return {
                'pipeline': {
                    'sag_v2': {
                        'use_safety_extractor': True,
                        'use_causal_chain_extractor': True,
                        'use_concept_hierarchy_extractor': True,
                        'use_case_study_extractor': True,
                        'use_prerequisite_extractor': True,
                        'models': {
                            'safety': 'gpt-4o-mini',
                            'causal': 'gpt-4o-mini',
                            'hierarchy': 'gpt-4o-mini',
                            'case_study': 'gpt-4o-mini',
                            'prerequisite': 'gpt-4o-mini'
                        }
                    }
                }
            }

    def load_input(self, file_path: Path) -> Dict[str, Any]:
        """–ü–æ–¥–¥–µ—Ä–∂–∫–∞ –≤—Ö–æ–¥–∞ –∏–∑ get_subtitles.json —Ñ–æ—Ä–º–∞—Ç–∞ –∏ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã—Ö —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç–æ–≤."""
        with open(file_path, "r", encoding="utf-8") as f:
            data = json.load(f)

        if "subtitles" in data:
            raw_segments = data["subtitles"]
        elif "transcript" in data and isinstance(data["transcript"], dict):
            raw_segments = data["transcript"].get("segments", [])
        else:
            raw_segments = data.get("segments", [])

        segments: List[Dict[str, Any]] = []
        for i, s in enumerate(raw_segments):
            start = s.get("start", 0.0)
            dur = s.get("duration", max(0.0, s.get("end", 0.0) - s.get("start", 0.0)))
            end = start + dur
            segments.append({
                "id": i,
                "start": start,
                "end": end,
                "duration": dur,
                "text": s.get("text", ""),
            })

        return {
            "segments": segments,
            "metadata": data.get("metadata", data.get("file_info", {})),
            "full_text": " ".join(s.get("text", "") for s in segments),
        }

    # ---------------------- Chunking ---------------------- #
    def chunk_segments(self, segments: List[Dict[str, Any]], *, max_tokens: int = 4800, overlap_tokens: int = 200) -> List[Dict[str, Any]]:
        """–£–º–Ω–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ: —É—á–∏—Ç—ã–≤–∞–µ–º –º–∞—Ä–∫–µ—Ä—ã —Å–º–µ–Ω—ã —Ç–µ–º—ã –∏ –¥–ª–∏–Ω–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã."""
        chunks: List[Dict[str, Any]] = []
        current: List[Dict[str, Any]] = []
        cur_tokens = 0

        def flush_chunk():
            nonlocal current, cur_tokens
            if not current:
                return
            chunks.append({
                "text": " ".join(s.get("text", "") for s in current),
                "start_time": current[0]["start"],
                "end_time": current[-1]["end"],
                "segments": list(current),
            })
            # overlap –ø–æ –ø–æ—Å–ª–µ–¥–Ω–∏–º 5 —Å–µ–≥–º–µ–Ω—Ç–∞–º, –µ—Å–ª–∏ –Ω–µ –ø—Ä–µ–≤—ã—à–∞–µ—Ç overlap_tokens
            tail = current[-5:]
            tail_tok = sum(self._tok(s.get("text", "")) for s in tail)
            if tail_tok <= overlap_tokens:
                current, cur_tokens = tail, tail_tok
            else:
                current, cur_tokens = [], 0

        for s in segments:
            t = s.get("text", "")
            n = self._tok(t)

            is_boundary = any(marker in t for marker in [
                "–ò—Ç–∞–∫", "–ü–æ–¥–≤–µ–¥—ë–º –∏—Ç–æ–≥", "–ü–æ–¥–≤–æ–¥—è –∏—Ç–æ–≥", "–¢–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º",
                "–î–∞–≤–∞–π—Ç–µ", "–¢–µ–ø–µ—Ä—å", "–°–ª–µ–¥—É—é—â–∏–π", "–í–æ–ø—Ä–æ—Å –∏–∑ –∑–∞–ª–∞", "–ü—Ä–∞–∫—Ç–∏–∫–∞", "–£–ø—Ä–∞–∂–Ω–µ–Ω–∏–µ",
            ])

            # –î–µ–ª–∞–µ–º –±–ª–æ–∫–∏ –∫—Ä—É–ø–Ω–µ–µ: –º–∏–Ω–∏–º—É–º 2000 —Ç–æ–∫–µ–Ω–æ–≤, –º–∞–∫—Å–∏–º—É–º 4800
            if current and (cur_tokens + n > max_tokens or (is_boundary and cur_tokens > 2000)):
                flush_chunk()

            current.append(s)
            cur_tokens += n

        flush_chunk()
        return chunks

    # ---------------------- LLM prompts ---------------------- #
    def process_chunk(self, chunk: Dict[str, Any]) -> List[Dict[str, Any]]:
        source_text = self._light_clean(chunk['text'])
        prompt = f"""{self.domain_context}

–ó–ê–î–ê–ß–ê: –†–∞–∑–±–µ–π —Ñ—Ä–∞–≥–º–µ–Ω—Ç –ª–µ–∫—Ü–∏–∏ –Ω–∞ –ö–†–£–ü–ù–´–ï –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–µ –±–ª–æ–∫–∏ (4‚Äì8 –º–∏–Ω—É—Ç –∫–∞–∂–¥—ã–π, –º–∏–Ω–∏–º—É–º 300-500 —Å–ª–æ–≤). –°–¢–†–û–ì–û —Å–æ—Ö—Ä–∞–Ω—è–π –∞–≤—Ç–æ—Ä—Å–∫–∏–µ —Ç–µ—Ä–º–∏–Ω—ã –∏ —Å—Ç–∏–ª—å —Ä–µ—á–∏.

–¢–†–ï–ë–û–í–ê–ù–ò–Ø:
1) –ö–†–£–ü–ù–´–ï –ë–õ–û–ö–ò: –ö–∞–∂–¥—ã–π –±–ª–æ–∫ –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å 300-500+ —Å–ª–æ–≤. –ù–ï –¥–µ–ª–∏ –Ω–∞ –º–µ–ª–∫–∏–µ —á–∞—Å—Ç–∏.
2) –¢–µ—Ä–º–∏–Ω—ã –Ω–µ —Ç—Ä–æ–≥–∞—Ç—å: –Ω–µ–π—Ä–æ—Å—Ç–∞–ª–∫–∏–Ω–≥, –Ω–µ–æ—Å—Ç–∞–ª–∫–∏–Ω–≥, –ø–æ–ª–µ –≤–Ω–∏–º–∞–Ω–∏—è, –Ω–∞–±–ª—é–¥–µ–Ω–∏–µ –∑–∞ —É–º–æ–º, –º–µ—Ç–∞–Ω–∞–±–ª—é–¥–µ–Ω–∏–µ, –∞–≤—Ç–æ–º–∞—Ç–∏–∑–º—ã, –ø–∞—Ç—Ç–µ—Ä–Ω—ã, —Ç—Ä–∏–≥–≥–µ—Ä—ã, –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–µ—Ä–µ–∂–∏–≤–∞–Ω–∏–π, –ø—Ä–∞–∫—Ç–∏–∫–∞, —É–ø—Ä–∞–∂–Ω–µ–Ω–∏–µ, —Ä–∞–∑–±–æ—Ä –∏ –¥—Ä.
3) –í–ï–°–¨ –¢–ï–ö–°–¢ –≤–∫–ª—é—á–∞—Ç—å: –ù–ï –ø–µ—Ä–µ—Ñ—Ä–∞–∑–∏—Ä—É–π –∏ –ù–ï –ø–µ—Ä–µ—Å–∫–∞–∑—ã–≤–∞–π. –í–∫–ª—é—á–∏ –í–°–Å —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ –≤ –±–ª–æ–∫–∏. –£–¥–∞–ª—è–π —Ç–æ–ª—å–∫–æ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π –º—É—Å–æ—Ä.
4) –£–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏: –∫–∞–∂–¥—ã–π title –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —É–Ω–∏–∫–∞–ª—å–Ω—ã–º –∏ –æ—Ç—Ä–∞–∂–∞—Ç—å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—É—é —Ç–µ–º—É –±–ª–æ–∫–∞.
5) –§–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞ ‚Äî JSON-–º–∞—Å—Å–∏–≤:
[
  {{
    "start": "HH:MM:SS",
    "end": "HH:MM:SS",
    "title": "–£–Ω–∏–∫–∞–ª—å–Ω—ã–π –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π –∑–∞–≥–æ–ª–æ–≤–æ–∫ (5‚Äì12 —Å–ª–æ–≤)",
    "summary": "–ö—Ä–∞—Ç–∫–æ–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ (2‚Äì3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è) —Å —Ç–µ—Ä–º–∏–Ω–∞–º–∏ –∏–∑ –ª–µ–∫—Ü–∏–∏",
    "keywords": ["—Ç–µ—Ä–º–∏–Ω1", "—Ç–µ—Ä–º–∏–Ω2", "–∫–æ–Ω—Ü–µ–ø—Ç3"],
    "content": "–ü–û–õ–ù–´–ô –æ—á–∏—â–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –±–ª–æ–∫–∞ (300-500+ —Å–ª–æ–≤, –≤–µ—Å—å –∫–æ–Ω—Ç–µ–Ω—Ç —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞)"
  }}
]

–§–†–ê–ì–ú–ï–ù–¢ –¢–ï–ö–°–¢–ê (–Ω–µ –ø–µ—Ä–µ—Å–∫–∞–∑—ã–≤–∞—Ç—å, –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∫–∞–∫ –µ—Å—Ç—å; —Ç–æ–ª—å–∫–æ –º–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —á–∏—Å—Ç–∫–∞):
{source_text}
"""

        try:
            resp = self._ask(self.primary_model, prompt, max_tokens=4200, temperature=0.2)
            txt = resp.strip()
            if txt.startswith("```json"):
                txt = txt[7:]
            elif txt.startswith("```"):
                txt = txt[3:]
            if txt.endswith("```"):
                txt = txt[:-3]
            blocks = orjson.loads(txt)

            st, et = _hms(chunk.get("start_time")), _hms(chunk.get("end_time"))
            for b in blocks:
                b.setdefault("start", st)
                b.setdefault("end", et)
                # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—É—é –ø–æ–ª–∏—Ä–æ–≤–∫—É –∫ —Å–æ–¥–µ—Ä–∂–∏–º–æ–º—É –±–ª–æ–∫–∞
                if 'content' in b:
                    b['content'] = self._final_polish(b['content'])
            return blocks
        except Exception:
            # –ë–µ–∑–æ–ø–∞—Å–Ω—ã–π —Ä–µ–∑–µ—Ä–≤: –≤–µ—Ä–Ω—É—Ç—å –∏—Å—Ö–æ–¥–Ω—ã–π —Ñ—Ä–∞–≥–º–µ–Ω—Ç –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π —Å–º—ã—Å–ª–∞
            return [{
                "start": _hms(chunk.get("start_time")),
                "end": _hms(chunk.get("end_time")),
                "title": "–§—Ä–∞–≥–º–µ–Ω—Ç –ª–µ–∫—Ü–∏–∏ (–±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π)",
                "summary": "–ë–ª–æ–∫ —Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω –∏–∑ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ (–º–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞).",
                "keywords": ["–Ω–µ–π—Ä–æ—Å—Ç–∞–ª–∫–∏–Ω–≥"],
                "content": self._final_polish(source_text)
            }]

    def refine_blocks(self, blocks: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        if not self.refine_model:
            return blocks

        prompt = f"""{self.domain_context}

–ü–û–õ–ò–†–û–í–ö–ê –ë–õ–û–ö–û–í: —É–ª—É—á—à–∏ —á–∏—Ç–∞–µ–º–æ—Å—Ç—å, –Ω–æ —Å—Ç—Ä–æ–≥–æ —Å–æ—Ö—Ä–∞–Ω–∏ —Ç–µ—Ä–º–∏–Ω—ã –∏ –∞–≤—Ç–æ—Ä—Å–∫–∏–µ —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∏. 
–ú–æ–∂–Ω–æ: –ø—Ä–∞–≤–∏—Ç—å –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é, –∞–±–∑–∞—Ü—ã, –¥–µ–ª–∞—Ç—å –∑–∞–≥–æ–ª–æ–≤–∫–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω–µ–µ, —Ä–∞—Å—à–∏—Ä—è—Ç—å keywords. –ù–µ–ª—å–∑—è: –ø–µ—Ä–µ–ø–∏—Å—ã–≤–∞—Ç—å content —Å–≤–æ–∏–º–∏ —Å–ª–æ–≤–∞–º–∏.

–í–µ—Ä–Ω–∏ JSON-–º–∞—Å—Å–∏–≤ –≤ —Ç–æ–º –∂–µ —Ñ–æ—Ä–º–∞—Ç–µ:
{orjson.dumps(blocks, option=orjson.OPT_INDENT_2).decode()}
"""

        try:
            resp = self._ask(self.refine_model, prompt, max_tokens=5200, temperature=0.2)
            if resp.startswith("```json"):
                resp = resp[7:]
            elif resp.startswith("```"):
                resp = resp[3:]
            if resp.endswith("```"):
                resp = resp[:-3]
            refined_blocks = orjson.loads(resp)
            # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—É—é –ø–æ–ª–∏—Ä–æ–≤–∫—É –∫ –æ—Ç—Ä–µ—Ñ–∞–π–Ω–µ–Ω–Ω—ã–º –±–ª–æ–∫–∞–º
            for b in refined_blocks:
                if 'content' in b:
                    b['content'] = self._final_polish(b['content'])
            return refined_blocks
        except Exception:
            return blocks

    # ---------------------- Knowledge Graph Builder ---------------------- #
    
    def _build_knowledge_graph_from_blocks(
        self, 
        blocks: List[Dict[str, Any]], 
        video_id: str,
        document_entities: List[str]
    ) -> Dict[str, Any]:
        """
        üöÄ –ü–æ—Å—Ç—Ä–æ–∏—Ç—å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –ø–æ–ª–Ω—ã–π Knowledge Graph –∏–∑ –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä–æ–≤.
        
        –û–±—ä–µ–¥–∏–Ω—è–µ—Ç:
        - graph_entities (rule-based –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ)
        - concept_hierarchy (–∏–µ—Ä–∞—Ä—Ö–∏—è –∫–æ–Ω—Ü–µ–ø—Ç–æ–≤)
        - causal_chains (–ø—Ä–∏—á–∏–Ω–Ω–æ-—Å–ª–µ–¥—Å—Ç–≤–µ–Ω–Ω—ã–µ —Ü–µ–ø–æ—á–∫–∏)
        - prerequisites (–ø—Ä–µ–¥–ø–æ—Å—ã–ª–∫–∏ –∏ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏)
        - case_studies (–∫–µ–π—Å—ã –∏ –ø—Ä–∏–º–µ—Ä—ã)
        - semantic_relationships (—Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ —Å–≤—è–∑–∏)
        
        Args:
            blocks: –°–ø–∏—Å–æ–∫ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –±–ª–æ–∫–æ–≤
            video_id: ID –≤–∏–¥–µ–æ
            document_entities: –í—Å–µ –≥—Ä–∞—Ñ-—Å—É—â–Ω–æ—Å—Ç–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞
            
        Returns:
            Dict —Å –ø–æ–ª–Ω—ã–º knowledge graph
        """
        from .orchestrator.graph_weight_calculator import GraphWeightCalculator
        
        nodes = {}  # name -> node_data
        edges = []  # —Å–ø–∏—Å–æ–∫ —Å–≤—è–∑–µ–π
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–∞–ª—å–∫—É–ª—è—Ç–æ—Ä–∞ –≤–µ—Å–æ–≤
        weight_calculator = GraphWeightCalculator()
        
        # –ê–Ω–∞–ª–∏–∑ –≤—Å–µ—Ö –±–ª–æ–∫–æ–≤ –¥–ª—è —Å–±–æ—Ä–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ–∑–∏—Ü–∏–π –∏ co-occurrence
        for idx, block in enumerate(blocks):
            block_content = block.get('content', '')
            block_entities = block.get('graph_entities', [])
            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º entities –≤ —Å–ø–∏—Å–æ–∫ —Å—Ç—Ä–æ–∫ –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
            entities_list = []
            for entity in block_entities:
                if isinstance(entity, (tuple, list)) and len(entity) >= 1:
                    entities_list.append(str(entity[0]))
                elif isinstance(entity, str):
                    entities_list.append(entity)
            
            if block_content and entities_list:
                weight_calculator.analyze_block(block_content, entities_list, idx)
        
        total_blocks = len(blocks)
        
        # –¢–∏–ø—ã —É–∑–ª–æ–≤
        NODE_TYPES = {
            "concept": "CONCEPT",
            "domain": "CONCEPT",
            "practice": "PRACTICE",
            "technique": "TECHNIQUE",
            "exercise": "EXERCISE",
            "pattern": "PATTERN",
            "process_stage": "PROCESS_STAGE",
            "symptom": "CONCEPT",
            "state": "CONCEPT"
        }
        
        # –¢–∏–ø—ã —Å–≤—è–∑–µ–π
        EDGE_TYPES = {
            "is_core_component_of": "IS_CORE_COMPONENT_OF",
            "is_practice_for": "IS_PRACTICE_FOR",
            "is_technique_for": "IS_TECHNIQUE_FOR",
            "is_exercise_for": "IS_EXERCISE_FOR",
            "enables": "ENABLES",
            "requires": "REQUIRES",
            "leads_to": "LEADS_TO",
            "transforms_into": "TRANSFORMS_INTO",
            "emerges_from": "EMERGES_FROM",
            "related_to": "RELATED_TO",
            "prerequisite_for": "REQUIRES",
            "part_of": "IS_CORE_COMPONENT_OF"
        }
        
        node_id_counter = 0
        
        def get_or_create_node(name: str, node_type: str = "CONCEPT", description: str = "", 
                               metadata: Dict = None) -> str:
            """–ü–æ–ª—É—á–∏—Ç—å –∏–ª–∏ —Å–æ–∑–¥–∞—Ç—å —É–∑–µ–ª, –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç node_id"""
            nonlocal node_id_counter
            name_lower = name.lower().strip()
            
            if name_lower not in nodes:
                node_id_counter += 1
                node_id = f"node_{node_id_counter:04d}"
                nodes[name_lower] = {
                    "id": node_id,
                    "name": name,
                    "node_type": node_type,
                    "description": description,
                    "metadata": metadata or {},
                    "sources": []  # –û—Ç–∫—É–¥–∞ –∏–∑–≤–ª–µ—á–µ–Ω —É–∑–µ–ª
                }
            else:
                # –û–±–Ω–æ–≤–ª—è–µ–º –æ–ø–∏—Å–∞–Ω–∏–µ –µ—Å–ª–∏ –æ–Ω–æ –ø—É—Å—Ç–æ–µ
                if not nodes[name_lower]["description"] and description:
                    nodes[name_lower]["description"] = description
                # –û–±—ä–µ–¥–∏–Ω—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
                if metadata:
                    nodes[name_lower]["metadata"].update(metadata)
            
            return nodes[name_lower]["id"]
        
        def add_edge(from_name: str, to_name: str, edge_type: str, explanation: str = "", 
                    confidence: float = 1.0, metadata: Dict = None):
            """–î–æ–±–∞–≤–∏—Ç—å —Å–≤—è–∑—å –º–µ–∂–¥—É —É–∑–ª–∞–º–∏"""
            from_id = get_or_create_node(from_name)
            to_id = get_or_create_node(to_name)
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
            for edge in edges:
                if (edge["from_id"] == from_id and edge["to_id"] == to_id and 
                    edge["edge_type"] == edge_type):
                    return
            
            edges.append({
                "from_id": from_id,
                "to_id": to_id,
                "from_name": from_name,
                "to_name": to_name,
                "edge_type": edge_type,
                "explanation": explanation,
                "confidence": confidence,
                "metadata": metadata or {}
            })
        
        # 1. –ò–ó–í–õ–ï–ß–ï–ù–ò–ï –£–ó–õ–û–í –ò–ó GRAPH_ENTITIES (rule-based)
        for entity in document_entities:
            # –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–æ—Ä—Ç–µ–∂–µ–π (name, count) –∏–ª–∏ –ø—Ä–æ—Å—Ç–æ —Å—Ç—Ä–æ–∫
            if isinstance(entity, tuple) and len(entity) == 2:
                entity_name, count = entity
            elif isinstance(entity, list) and len(entity) == 2:
                entity_name, count = entity[0], entity[1]
            else:
                entity_name = str(entity)
                count = 1
            
            if not entity_name or not isinstance(entity_name, str):
                continue
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø —É–∑–ª–∞ –ø–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É
            node_type = "CONCEPT"
            entity_lower = entity_name.lower()
            if any(word in entity_lower for word in ["–ø—Ä–∞–∫—Ç–∏–∫–∞", "—É–ø—Ä–∞–∂–Ω–µ–Ω–∏–µ", "—Ç–µ—Ö–Ω–∏–∫–∞"]):
                if "—É–ø—Ä–∞–∂–Ω–µ–Ω–∏–µ" in entity_lower:
                    node_type = "EXERCISE"
                elif "—Ç–µ—Ö–Ω–∏–∫–∞" in entity_lower:
                    node_type = "TECHNIQUE"
                else:
                    node_type = "PRACTICE"
            
            get_or_create_node(
                entity_name,
                node_type=node_type,
                metadata={"frequency": count, "source": "graph_entities"}
            )
        
        # 2. –ò–ó–í–õ–ï–ß–ï–ù–ò–ï –ò–ó CONCEPT_HIERARCHY (–∏–µ—Ä–∞—Ä—Ö–∏—è –∫–æ–Ω—Ü–µ–ø—Ç–æ–≤)
        for block in blocks:
            hierarchy = block.get("concept_hierarchy", [])
            if not hierarchy:
                continue
            
            for concept in hierarchy:
                concept_name = concept.get("name", "").strip()
                if not concept_name:
                    continue
                
                level = concept.get("level", "concept")
                node_type = NODE_TYPES.get(level, "CONCEPT")
                description = concept.get("description", "")
                parent = concept.get("parent", "")
                relationship = concept.get("relationship", "")
                
                # –°–æ–∑–¥–∞–µ–º —É–∑–µ–ª –∫–æ–Ω—Ü–µ–ø—Ç–∞
                concept_id = get_or_create_node(
                    concept_name,
                    node_type=node_type,
                    description=description,
                    metadata={
                        "level": level,
                        "source": "concept_hierarchy",
                        "block_id": block.get("block_id", "")
                    }
                )
                
                # –î–æ–±–∞–≤–ª—è–µ–º —Å–≤—è–∑—å —Å —Ä–æ–¥–∏—Ç–µ–ª–µ–º
                if parent and parent.strip():
                    edge_type = EDGE_TYPES.get(relationship, "IS_CORE_COMPONENT_OF")
                    # –í—ã—á–∏—Å–ª—è–µ–º –≤–µ—Å –¥–ª—è –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏—Ö —Å–≤—è–∑–µ–π
                    calculated_weight = weight_calculator.calculate_combined_weight(
                        concept_name, parent, total_blocks
                    )
                    add_edge(
                        concept_name,
                        parent,
                        edge_type=edge_type,
                        explanation=f"{concept_name} —è–≤–ª—è–µ—Ç—Å—è —á–∞—Å—Ç—å—é {parent}",
                        confidence=calculated_weight,
                        metadata={"source": "concept_hierarchy"}
                    )
        
        # 3. –ò–ó–í–õ–ï–ß–ï–ù–ò–ï –ò–ó CAUSAL_CHAINS (–ø—Ä–∏—á–∏–Ω–Ω–æ-—Å–ª–µ–¥—Å—Ç–≤–µ–Ω–Ω—ã–µ —Ü–µ–ø–æ—á–∫–∏)
        for block in blocks:
            chains = block.get("causal_chains", [])
            if not chains:
                continue
            
            for chain in chains:
                chain_name = chain.get("name", "")
                steps = chain.get("steps", [])
                
                # –°–æ–∑–¥–∞–µ–º —É–∑–ª—ã –¥–ª—è —ç—Ç–∞–ø–æ–≤ –ø—Ä–æ—Ü–µ—Å—Å–∞
                prev_step_name = None
                for step in steps:
                    step_name = step.get("event", "").strip() or step.get("stage_name", "").strip()
                    if not step_name:
                        continue
                    
                    step_id = get_or_create_node(
                        step_name,
                        node_type="PROCESS_STAGE",
                        description=step.get("description", ""),
                        metadata={
                            "process": chain_name,
                            "step_number": step.get("step", 0),
                            "source": "causal_chains",
                            "block_id": block.get("block_id", "")
                        }
                    )
                    
                    # –°–≤—è–∑—å –º–µ–∂–¥—É —ç—Ç–∞–ø–∞–º–∏
                    if prev_step_name:
                        # –í—ã—á–∏—Å–ª—è–µ–º –≤–µ—Å –¥–ª—è –ø—Ä–∏—á–∏–Ω–Ω–æ-—Å–ª–µ–¥—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Å–≤—è–∑–µ–π
                        calculated_weight = weight_calculator.calculate_combined_weight(
                            prev_step_name, step_name, total_blocks
                        )
                        add_edge(
                            prev_step_name,
                            step_name,
                            edge_type="LEADS_TO",
                            explanation=f"{prev_step_name} –≤–µ–¥–µ—Ç –∫ {step_name}",
                            confidence=calculated_weight,
                            metadata={"source": "causal_chains", "process": chain_name}
                        )
                    
                    prev_step_name = step_name
                
                # Intervention points (—Ç–æ—á–∫–∏ –≤–º–µ—à–∞—Ç–µ–ª—å—Å—Ç–≤–∞)
                interventions = chain.get("intervention_points", [])
                for intervention in interventions:
                    practice = intervention.get("practice", "").strip()
                    after_step = intervention.get("after_step", "")
                    
                    if practice:
                        practice_id = get_or_create_node(
                            practice,
                            node_type="PRACTICE",
                            description=intervention.get("effect", ""),
                            metadata={"source": "causal_chains_intervention"}
                        )
                        
                        if after_step and prev_step_name:
                            # –í—ã—á–∏—Å–ª—è–µ–º –≤–µ—Å –¥–ª—è —Å–≤—è–∑–µ–π –ø—Ä–∞–∫—Ç–∏–∫ —Å —ç—Ç–∞–ø–∞–º–∏
                            calculated_weight = weight_calculator.calculate_combined_weight(
                                practice, prev_step_name, total_blocks
                            )
                            add_edge(
                                practice,
                                prev_step_name,
                                edge_type="ENABLES",
                                explanation=intervention.get("effect", ""),
                                confidence=calculated_weight,
                                metadata={"source": "causal_chains_intervention"}
                            )
        
        # 4. –ò–ó–í–õ–ï–ß–ï–ù–ò–ï –ò–ó PREREQUISITES (–ø—Ä–µ–¥–ø–æ—Å—ã–ª–∫–∏)
        for block in blocks:
            prereq = block.get("prerequisites", {})
            if not prereq:
                continue
            
            prerequisites = prereq.get("prerequisites", [])
            sequence = prereq.get("recommended_sequence", [])
            
            # –ü—Ä–µ–¥–ø–æ—Å—ã–ª–∫–∏
            for prereq_item in prerequisites:
                prereq_name = prereq_item if isinstance(prereq_item, str) else prereq_item.get("concept", "")
                if prereq_name:
                    get_or_create_node(
                        prereq_name,
                        node_type="CONCEPT",
                        metadata={"source": "prerequisites"}
                    )
            
            # –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è
            for i, item in enumerate(sequence):
                if i > 0:
                    prev_item = sequence[i-1]
                    curr_item = item if isinstance(item, str) else item.get("concept", "")
                    prev_item_name = prev_item if isinstance(prev_item, str) else prev_item.get("concept", "")
                    
                    if curr_item and prev_item_name:
                        # –í—ã—á–∏—Å–ª—è–µ–º –≤–µ—Å –¥–ª—è —Å–≤—è–∑–µ–π –ø—Ä–µ–¥–ø–æ—Å—ã–ª–æ–∫
                        calculated_weight = weight_calculator.calculate_combined_weight(
                            prev_item_name, curr_item, total_blocks
                        )
                        add_edge(
                            prev_item_name,
                            curr_item,
                            edge_type="REQUIRES",
                            explanation=f"{prev_item_name} —è–≤–ª—è–µ—Ç—Å—è –ø—Ä–µ–¥–ø–æ—Å—ã–ª–∫–æ–π –¥–ª—è {curr_item}",
                            confidence=calculated_weight,
                            metadata={"source": "prerequisites_sequence"}
                        )
        
        # 5. –ò–ó–í–õ–ï–ß–ï–ù–ò–ï –ò–ó CASE_STUDIES (–∫–µ–π—Å—ã)
        for block in blocks:
            case_studies = block.get("case_studies", [])
            if not case_studies:
                continue
            
            for case in case_studies:
                situation = case.get("situation", "").strip()
                concepts = case.get("related_concepts", [])
                
                if situation:
                    case_id = get_or_create_node(
                        situation,
                        node_type="CONCEPT",
                        description=case.get("analysis", ""),
                        metadata={"source": "case_study", "case_id": case.get("id", "")}
                    )
                    
                    # –°–≤—è–∑–∏ —Å –∫–æ–Ω—Ü–µ–ø—Ç–∞–º–∏
                    for concept in concepts:
                        if isinstance(concept, str):
                            # –í—ã—á–∏—Å–ª—è–µ–º –≤–µ—Å –¥–ª—è —Å–≤—è–∑–µ–π –∫–µ–π—Å–æ–≤ —Å –∫–æ–Ω—Ü–µ–ø—Ç–∞–º–∏
                            calculated_weight = weight_calculator.calculate_combined_weight(
                                situation, concept, total_blocks
                            )
                            add_edge(
                                situation,
                                concept,
                                edge_type="RELATED_TO",
                                explanation=f"–ö–µ–π—Å —Å–≤—è–∑–∞–Ω —Å –∫–æ–Ω—Ü–µ–ø—Ç–æ–º {concept}",
                                confidence=calculated_weight,
                                metadata={"source": "case_study"}
                            )
        
        # 6. –ò–ó–í–õ–ï–ß–ï–ù–ò–ï –ò–ó SEMANTIC_RELATIONSHIPS (—Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ —Å–≤—è–∑–∏)
        for block in blocks:
            relationships = block.get("semantic_relationships", {})
            
            for rel_type, rel_list in relationships.items():
                for rel in rel_list:
                    source = rel.get("source", "")
                    target = rel.get("target", "")
                    
                    if not source or not target:
                        continue
                    
                    # –ú–∞–ø–ø–∏–Ω–≥ —Ç–∏–ø–æ–≤ —Å–≤—è–∑–µ–π
                    edge_type_map = {
                        "conceptual": "RELATED_TO",
                        "causal": "LEADS_TO",
                        "practical": "ENABLES",
                        "temporal": "LEADS_TO",
                        "structural": "IS_CORE_COMPONENT_OF"
                    }
                    
                    edge_type = edge_type_map.get(rel_type, "RELATED_TO")
                    # –í—ã—á–∏—Å–ª—è–µ–º –≤–µ—Å —Å–≤—è–∑–∏ –≤–º–µ—Å—Ç–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ strength
                    calculated_weight = weight_calculator.calculate_combined_weight(
                        source, target, total_blocks
                    )
                    
                    add_edge(
                        source,
                        target,
                        edge_type=edge_type,
                        explanation=f"–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è —Å–≤—è–∑—å: {rel_type}",
                        confidence=calculated_weight,
                        metadata={"source": "semantic_relationships", "rel_type": rel_type}
                    )
        
        # 7. –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–´–ï –°–í–Ø–ó–ò: –ø–µ—Ä–µ–∫—Ä–µ—Å—Ç–Ω—ã–µ —Å–≤—è–∑–∏ –º–µ–∂–¥—É –±–ª–æ–∫–∞–º–∏
        # –°–≤—è–∑—ã–≤–∞–µ–º –∫–æ–Ω—Ü–µ–ø—Ç—ã, –∫–æ—Ç–æ—Ä—ã–µ –≤—Å—Ç—Ä–µ—á–∞—é—Ç—Å—è –≤–º–µ—Å—Ç–µ –≤ —Ä–∞–∑–Ω—ã—Ö –±–ª–æ–∫–∞—Ö
        concept_cooccurrence = {}
        for block in blocks:
            block_entities = block.get("graph_entities", [])
            for i, entity1 in enumerate(block_entities):
                for entity2 in block_entities[i+1:]:
                    key = tuple(sorted([entity1, entity2]))
                    concept_cooccurrence[key] = concept_cooccurrence.get(key, 0) + 1
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Å–≤—è–∑–∏ –¥–ª—è —á–∞—Å—Ç–æ –≤—Å—Ç—Ä–µ—á–∞—é—â–∏—Ö—Å—è –ø–∞—Ä
        for (entity1, entity2), count in concept_cooccurrence.items():
            if count >= 2:  # –ú–∏–Ω–∏–º—É–º 2 —Å–æ–≤–º–µ—Å—Ç–Ω—ã—Ö –ø–æ—è–≤–ª–µ–Ω–∏—è
                # –í—ã—á–∏—Å–ª—è–µ–º –≤–µ—Å –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
                calculated_weight = weight_calculator.calculate_combined_weight(
                    entity1, entity2, total_blocks
                )
                add_edge(
                    entity1,
                    entity2,
                    edge_type="RELATED_TO",
                    explanation=f"–ß–∞—Å—Ç–æ –≤—Å—Ç—Ä–µ—á–∞—é—Ç—Å—è –≤–º–µ—Å—Ç–µ ({count} —Ä–∞–∑)",
                    confidence=calculated_weight,
                    metadata={"source": "cooccurrence", "frequency": count}
                )
        
        # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –≤–µ—Å–æ–≤ —Å–≤—è–∑–µ–π
        edge_weights = [edge.get("confidence", 1.0) for edge in edges]
        weight_statistics = {}
        if edge_weights:
            sorted_weights = sorted(edge_weights)
            weight_statistics = {
                "min_weight": min(edge_weights),
                "max_weight": max(edge_weights),
                "avg_weight": sum(edge_weights) / len(edge_weights),
                "median_weight": sorted_weights[len(sorted_weights) // 2] if sorted_weights else 0
            }
        else:
            weight_statistics = {
                "min_weight": 0,
                "max_weight": 0,
                "avg_weight": 0,
                "median_weight": 0
            }
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É
        return {
            "nodes": list(nodes.values()),
            "edges": edges,
            "metadata": {
                "video_id": video_id,
                "total_nodes": len(nodes),
                "total_edges": len(edges),
                "node_types": self._count_node_types_in_graph(nodes),
                "edge_types": self._count_edge_types_in_graph(edges),
                "weight_statistics": weight_statistics,
                "sources": {
                    "graph_entities": sum(1 for n in nodes.values() if "graph_entities" in n.get("metadata", {}).get("source", "")),
                    "concept_hierarchy": sum(1 for n in nodes.values() if "concept_hierarchy" in n.get("metadata", {}).get("source", "")),
                    "causal_chains": sum(1 for n in nodes.values() if "causal_chains" in n.get("metadata", {}).get("source", "")),
                    "prerequisites": sum(1 for n in nodes.values() if "prerequisites" in n.get("metadata", {}).get("source", "")),
                    "case_studies": sum(1 for n in nodes.values() if "case_study" in n.get("metadata", {}).get("source", "")),
                    "semantic_relationships": sum(1 for e in edges if e.get("metadata", {}).get("source") == "semantic_relationships")
                }
            }
        }
    
    def _count_node_types_in_graph(self, nodes: Dict) -> Dict[str, int]:
        """–ü–æ–¥—Å—á–∏—Ç–∞—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–∑–ª–æ–≤ –∫–∞–∂–¥–æ–≥–æ —Ç–∏–ø–∞"""
        counts = {}
        for node in nodes.values():
            node_type = node.get("node_type", "CONCEPT")
            counts[node_type] = counts.get(node_type, 0) + 1
        return counts
    
    def _count_edge_types_in_graph(self, edges: List[Dict]) -> Dict[str, int]:
        """–ü–æ–¥—Å—á–∏—Ç–∞—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–±–µ—Ä –∫–∞–∂–¥–æ–≥–æ —Ç–∏–ø–∞"""
        counts = {}
        for edge in edges:
            edge_type = edge.get("edge_type", "RELATED_TO")
            counts[edge_type] = counts.get(edge_type, 0) + 1
        return counts
    
    # ---------------------- AI Bot Helper Methods ---------------------- #
    
    def find_practices_for_concept(self, concept: str, knowledge_graph: Dict = None) -> List[Dict]:
        """
        AI-–±–æ—Ç API: –ù–∞–π—Ç–∏ –ø—Ä–∞–∫—Ç–∏–∫–∏ –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –∫–æ–Ω—Ü–µ–ø—Ç–æ–º/—Å–∏–º–ø—Ç–æ–º–æ–º.
        
        Args:
            concept: –ù–∞–∑–≤–∞–Ω–∏–µ –∫–æ–Ω—Ü–µ–ø—Ç–∞ –∏–ª–∏ —Å–∏–º–ø—Ç–æ–º–∞
            knowledge_graph: Knowledge graph (–µ—Å–ª–∏ None, –Ω—É–∂–Ω–æ –∑–∞–≥—Ä—É–∑–∏—Ç—å –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞)
            
        Returns:
            –°–ø–∏—Å–æ–∫ –ø—Ä–∞–∫—Ç–∏–∫ —Å reasoning chains
        """
        if not knowledge_graph:
            return []
        
        practices = []
        nodes = {node["name"].lower(): node for node in knowledge_graph.get("nodes", [])}
        edges = knowledge_graph.get("edges", [])
        
        concept_lower = concept.lower()
        concept_node = nodes.get(concept_lower)
        
        if not concept_node:
            # –ü–æ–∏—Å–∫ —á–∞—Å—Ç–∏—á–Ω–æ–≥–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è
            for node_name, node in nodes.items():
                if concept_lower in node_name or node_name in concept_lower:
                    concept_node = node
                    break
        
        if not concept_node:
            return []
        
        concept_id = concept_node["id"]
        
        # –ò—â–µ–º –ø—Ä–∞–∫—Ç–∏–∫–∏ —á–µ—Ä–µ–∑ —Å–≤—è–∑–∏
        for edge in edges:
            if edge["to_id"] == concept_id:
                # –ù–∞—Ö–æ–¥–∏–º —É–∑–ª—ã —Ç–∏–ø–∞ PRACTICE, –∫–æ—Ç–æ—Ä—ã–µ —Å–≤—è–∑–∞–Ω—ã —Å –∫–æ–Ω—Ü–µ–ø—Ç–æ–º
                source_node = next((n for n in nodes.values() if n["id"] == edge["from_id"]), None)
                if source_node and source_node["node_type"] == "PRACTICE":
                    practices.append({
                        "practice": source_node["name"],
                        "relation": edge["edge_type"],
                        "explanation": edge.get("explanation", ""),
                        "confidence": edge.get("confidence", 1.0),
                        "description": source_node.get("description", "")
                    })
        
        return practices
    
    def get_concept_chain(self, from_concept: str, to_concept: str, knowledge_graph: Dict = None) -> Optional[List[Dict]]:
        """
        AI-–±–æ—Ç API: –ü–æ–ª—É—á–∏—Ç—å —Ü–µ–ø–æ—á–∫—É —Å–≤—è–∑–µ–π –º–µ–∂–¥—É –¥–≤—É–º—è –∫–æ–Ω—Ü–µ–ø—Ç–∞–º–∏.
        
        Args:
            from_concept: –ù–∞—á–∞–ª—å–Ω—ã–π –∫–æ–Ω—Ü–µ–ø—Ç
            to_concept: –ö–æ–Ω–µ—á–Ω—ã–π –∫–æ–Ω—Ü–µ–ø—Ç
            knowledge_graph: Knowledge graph
            
        Returns:
            –°–ø–∏—Å–æ–∫ —à–∞–≥–æ–≤ —Ü–µ–ø–æ—á–∫–∏ –∏–ª–∏ None
        """
        if not knowledge_graph:
            return None
        
        nodes = {node["id"]: node for node in knowledge_graph.get("nodes", [])}
        nodes_by_name = {node["name"].lower(): node for node in nodes.values()}
        edges_by_from = {}
        
        for edge in knowledge_graph.get("edges", []):
            if edge["from_id"] not in edges_by_from:
                edges_by_from[edge["from_id"]] = []
            edges_by_from[edge["from_id"]].append(edge)
        
        # BFS –ø–æ–∏—Å–∫ –ø—É—Ç–∏
        from_node = nodes_by_name.get(from_concept.lower())
        to_node = nodes_by_name.get(to_concept.lower())
        
        if not from_node or not to_node:
            return None
        
        if from_node["id"] == to_node["id"]:
            return [{"from": from_concept, "to": to_concept, "relation": "same"}]
        
        queue = [(from_node["id"], [from_node["id"]])]
        visited = {from_node["id"]}
        
        while queue:
            current_id, path = queue.pop(0)
            
            if len(path) > 5:  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –≥–ª—É–±–∏–Ω–∞
                continue
            
            for edge in edges_by_from.get(current_id, []):
                next_id = edge["to_id"]
                
                if next_id == to_node["id"]:
                    # –ù–∞–π–¥–µ–Ω –ø—É—Ç—å
                    chain = []
                    full_path = path + [next_id]
                    for i in range(len(full_path) - 1):
                        current_step_id = full_path[i]
                        next_step_id = full_path[i + 1]
                        
                        # –ù–∞–π—Ç–∏ —Å–≤—è–∑—å
                        edge_found = next(
                            (e for e in edges_by_from.get(current_step_id, []) if e["to_id"] == next_step_id),
                            None
                        )
                        
                        chain.append({
                            "from": nodes[current_step_id]["name"],
                            "to": nodes[next_step_id]["name"],
                            "relation": edge_found["edge_type"] if edge_found else "unknown",
                            "explanation": edge_found.get("explanation", "") if edge_found else ""
                        })
                    
                    return chain
                
                if next_id not in visited:
                    visited.add(next_id)
                    queue.append((next_id, path + [next_id]))
        
        return None
    
    def recommend_exercise_for_practice(self, practice: str, knowledge_graph: Dict = None, 
                                       duration: Optional[str] = None) -> Optional[Dict]:
        """
        AI-–±–æ—Ç API: –†–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞—Ç—å —É–ø—Ä–∞–∂–Ω–µ–Ω–∏–µ –¥–ª—è –ø—Ä–∞–∫—Ç–∏–∫–∏.
        
        Args:
            practice: –ù–∞–∑–≤–∞–Ω–∏–µ –ø—Ä–∞–∫—Ç–∏–∫–∏
            knowledge_graph: Knowledge graph
            duration: –ñ–µ–ª–∞–µ–º–∞—è –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
            
        Returns:
            Dict —Å —É–ø—Ä–∞–∂–Ω–µ–Ω–∏–µ–º –∏–ª–∏ None
        """
        if not knowledge_graph:
            return None
        
        nodes = {node["name"].lower(): node for node in knowledge_graph.get("nodes", [])}
        edges = knowledge_graph.get("edges", [])
        
        practice_lower = practice.lower()
        practice_node = nodes.get(practice_lower)
        
        if not practice_node:
            return None
        
        practice_id = practice_node["id"]
        
        # –ò—â–µ–º —É–ø—Ä–∞–∂–Ω–µ–Ω–∏—è —á–µ—Ä–µ–∑ —Å–≤—è–∑–∏ IS_EXERCISE_FOR –∏–ª–∏ IS_TECHNIQUE_FOR
        for edge in edges:
            if edge["to_id"] == practice_id and edge["edge_type"] in ["IS_EXERCISE_FOR", "IS_TECHNIQUE_FOR"]:
                source_node = next((n for n in nodes.values() if n["id"] == edge["from_id"]), None)
                
                if source_node and source_node["node_type"] in ["EXERCISE", "TECHNIQUE"]:
                    exercise_metadata = source_node.get("metadata", {})
                    
                    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω–∞
                    if duration:
                        exercise_duration = exercise_metadata.get("duration", "")
                        if exercise_duration and duration.lower() not in exercise_duration.lower():
                            continue
                    
                    return {
                        "exercise": source_node["name"],
                        "practice": practice,
                        "description": source_node.get("description", ""),
                        "duration": exercise_metadata.get("duration", ""),
                        "frequency": exercise_metadata.get("frequency", ""),
                        "instructions": exercise_metadata.get("instructions", []),
                        "confidence": edge.get("confidence", 1.0)
                    }
        
        return None
    
    # ---------------------- Save ---------------------- #
    def save_markdown(self, md_path: Path, data: Dict[str, Any], base_name: str) -> None:
        lines: List[str] = [
            "# üß≠ –õ–µ–∫—Ü–∏–∏ –°. –°–∞—Ä—Å–µ–∫–µ–Ω–æ–≤–∞ ‚Äî –∫–æ–Ω—Å–ø–µ–∫—Ç",
            f"## {data.get('document_title', base_name)}",
            f"*{data.get('document_summary', '–û–ø–∏—Å–∞–Ω–∏–µ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–æ')}*",
            "",
            # –ö–æ—Ä–æ—Ç–∫–æ–µ –æ–≥–ª–∞–≤–ª–µ–Ω–∏–µ –∏ –æ–±–æ–±—â—ë–Ω–Ω–æ–µ —Ä–µ–∑—é–º–µ —Å–≤–µ—Ä—Ö—É
            ("**–ö–æ—Ä–æ—Ç–∫–æ–µ –æ–≥–ª–∞–≤–ª–µ–Ω–∏–µ:** " + data.get('overview_toc', '')).strip(),
            ("**–û–±—â–µ–µ —Ä–µ–∑—é–º–µ:** " + data.get('overview_summary', '')).strip(),
            "",
            "---",
            "## üìë –û–≥–ª–∞–≤–ª–µ–Ω–∏–µ",
        ]

        for i, b in enumerate(data.get("blocks", [])):
            lines.append(f"{i+1}. [{b.get('title','–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è')}](#{i+1}-block)")

        lines.append("\n---\n")

        for i, b in enumerate(data.get("blocks", [])):
            lines.extend([
                f"## <a id='{i+1}-block'></a> {i+1}. {b.get('title','–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è')}",
                f"**‚è± –í—Ä–µ–º—è:** [{b.get('start','N/A')} - {b.get('end','N/A')}]",
                f"**üìù –ö—Ä–∞—Ç–∫–æ–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ:** {b.get('summary','')}",
                f"**üè∑ –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞:** {', '.join(b.get('keywords', []))}",
                "",
                "### –°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ:",
                b.get("content", ""),
                "",
                "---\n",
            ])

        md_path.write_text("\n".join(lines), encoding="utf-8")

    # ---------------------- Main processing ---------------------- #
    def process_file(self, input_path: Path, output_dir: Path) -> Dict[str, Any]:
        data = self.load_input(input_path)
        segments = data.get("segments", [])
        if not segments:
            raise RuntimeError("No segments found in input")

        chunks = self.chunk_segments(segments)
        all_blocks: List[Dict[str, Any]] = []
        for i, ch in enumerate(chunks, 1):
            print(f"[INFO] Processing chunk {i}/{len(chunks)}")
            blocks = self.process_chunk(ch)
            all_blocks.extend(blocks)
            # –ó–∞–¥–µ—Ä–∂–∫–∞ —É–∂–µ –µ—Å—Ç—å –≤ –º–µ—Ç–æ–¥–µ _ask –ø–µ—Ä–µ–¥ –∫–∞–∂–¥—ã–º –∑–∞–ø—Ä–æ—Å–æ–º –∫ API
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –Ω–µ –Ω—É–∂–Ω–∞

        if self.refine_model:
            all_blocks = self.refine_blocks(all_blocks)

        base = Path(input_path).stem
        video_id = base
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Å—Ç–∞–±–∏–ª—å–Ω—ã–µ ID –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–ª—è RAG + –Ω–æ–≤—ã–µ –ø–æ–ª—è SAG v2.0
        for i, block in enumerate(all_blocks):
            # –°—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –ø–æ–ª—è
            block["block_id"] = f"{video_id}_{i:03d}"
            block["video_id"] = video_id
            block["source_url"] = f"https://youtube.com/watch?v={video_id}"
            # –í—Ä–µ–º–µ–Ω–Ω–∞—è —Å—Å—ã–ª–∫–∞ –Ω–∞ YouTube
            if block.get("start"):
                start_seconds = self._hms_to_seconds(block["start"])
                if start_seconds is not None:
                    block["youtube_link"] = f"https://youtube.com/watch?v={video_id}&t={start_seconds}s"
            
            # –ù–û–í–´–ï –ø–æ–ª—è –¥–ª—è SAG-—Å–∏—Å—Ç–µ–º—ã v2.0 - –ê–í–¢–û–ú–ê–¢–ò–ß–ï–°–ö–ê–Ø –ö–õ–ê–°–°–ò–§–ò–ö–ê–¶–ò–Ø
            content = block.get("content", "")
            title = block.get("title", "")
            keywords = block.get("keywords", [])
            
            # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ñ—É–Ω–∫—Ü–∏–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
            speaker = self.detect_speaker(content)
            block_type = self.detect_block_type(content)
            emotional_tone = self.detect_emotional_tone(content, title)
            conceptual_depth = self.detect_conceptual_depth(content, keywords)
            complexity_score = self.calculate_complexity_score(content, keywords, conceptual_depth)
            graph_entities = self.extract_graph_entities(content, keywords)
            
            # üöÄ –ù–û–í–û–ï: –ê–Ω–∞–ª–∏–∑ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö —Å–≤—è–∑–µ–π –º–µ–∂–¥—É –≥—Ä–∞—Ñ-—Å—É—â–Ω–æ—Å—Ç—è–º–∏
            semantic_relationships = self.analyze_semantic_relationships(graph_entities)
            
            # –ó–∞–ø–æ–ª–Ω—è–µ–º –≤—Å–µ –ø–æ–ª—è
            block["speaker"] = speaker
            block["block_type"] = block_type
            block["emotional_tone"] = emotional_tone
            block["conceptual_depth"] = conceptual_depth
            block["complexity_score"] = complexity_score
            block["graph_entities"] = graph_entities
            block["semantic_relationships"] = semantic_relationships  # üÜï –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ —Å–≤—è–∑–∏
            block["contains_practice"] = block_type == "practice"
            block["dialogue_turn"] = None  # –ë—É–¥–µ—Ç –æ–±–Ω–æ–≤–ª–µ–Ω–æ –¥–ª—è –¥–∏–∞–ª–æ–≥–æ–≤
            block["question_type"] = None  # –ë—É–¥–µ—Ç –æ–±–Ω–æ–≤–ª–µ–Ω–æ –¥–ª—è –¥–∏–∞–ª–æ–≥–æ–≤
            block["answer_completeness"] = None
            block["interaction_quality"] = None
            block["key_insights"] = min(len(keywords), 8)  # –û—Ü–µ–Ω–∫–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
            block["transformation_stage"] = "beginning"    # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é
            block["transcript_edited"] = True
            
            # –°–ø–µ—Ü–∏–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∏–∞–ª–æ–≥–æ–≤—ã—Ö –±–ª–æ–∫–æ–≤
            if block_type == "dialogue":
                dialogue_data = self.structure_dialogue_block(content)
                if dialogue_data:
                    block["interaction_quality"] = dialogue_data.get("interaction_quality")
                    block["dialogue_turn"] = 1  # –ü–µ—Ä–≤—ã–π —Ö–æ–¥ –¥–∏–∞–ª–æ–≥–∞
                    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –¥–∏–∞–ª–æ–≥–æ–≤—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏
            
            # üöÄ –ù–û–í–´–ï –≠–ö–°–¢–†–ê–ö–¢–û–†–´ SAG v2.0 (–µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω—ã –≤ –∫–æ–Ω—Ñ–∏–≥–µ)
            sag_config = self.config.get('pipeline', {}).get('sag_v2', {})
            
            if self.safety_extractor and sag_config.get('use_safety_extractor', True):
                try:
                    safety_info = self.safety_extractor.extract(
                        block_content=block["content"],
                        practice_or_concept_name=block.get("title", "–Ω–µ —É–∫–∞–∑–∞–Ω–æ")
                    )
                    block["safety"] = safety_info
                    if safety_info.get("contraindications") or safety_info.get("when_to_stop"):
                        print(f"[INFO] –ë–ª–æ–∫ {i+1}: –Ω–∞–π–¥–µ–Ω—ã safety-–ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è")
                except Exception as e:
                    print(f"[WARNING] –û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ safety-–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –¥–ª—è –±–ª–æ–∫–∞ {i+1}: {e}")
                    block["safety"] = {
                        "contraindications": [],
                        "limitations": [],
                        "when_to_stop": [],
                        "when_to_seek_professional_help": [],
                        "notes": []
                    }
            
            if self.causal_extractor and sag_config.get('use_causal_chain_extractor', True):
                try:
                    causal_data = self.causal_extractor.extract(block_content=block["content"])
                    block["causal_chains"] = causal_data.get("processes", [])
                    if block["causal_chains"]:
                        print(f"[INFO] –ë–ª–æ–∫ {i+1}: –Ω–∞–π–¥–µ–Ω–æ {len(block['causal_chains'])} –ø—Ä–∏—á–∏–Ω–Ω–æ-—Å–ª–µ–¥—Å—Ç–≤–µ–Ω–Ω—ã—Ö –ø—Ä–æ—Ü–µ—Å—Å–æ–≤")
                except Exception as e:
                    print(f"[WARNING] –û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ –ø—Ä–∏—á–∏–Ω–Ω–æ-—Å–ª–µ–¥—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Ü–µ–ø–æ—á–µ–∫ –¥–ª—è –±–ª–æ–∫–∞ {i+1}: {e}")
                    block["causal_chains"] = []
            
            if self.hierarchy_extractor and sag_config.get('use_concept_hierarchy_extractor', True):
                try:
                    hierarchy_data = self.hierarchy_extractor.extract(
                        block_content=block["content"],
                        known_entities=block.get("graph_entities", [])
                    )
                    block["concept_hierarchy"] = hierarchy_data.get("concepts", [])
                    if block["concept_hierarchy"]:
                        print(f"[INFO] –ë–ª–æ–∫ {i+1}: –Ω–∞–π–¥–µ–Ω–æ {len(block['concept_hierarchy'])} –∫–æ–Ω—Ü–µ–ø—Ç–æ–≤ –≤ –∏–µ—Ä–∞—Ä—Ö–∏–∏")
                except Exception as e:
                    print(f"[WARNING] –û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ –∏–µ—Ä–∞—Ä—Ö–∏–∏ –∫–æ–Ω—Ü–µ–ø—Ç–æ–≤ –¥–ª—è –±–ª–æ–∫–∞ {i+1}: {e}")
                    block["concept_hierarchy"] = []
            
            if self.case_extractor and sag_config.get('use_case_study_extractor', True):
                try:
                    case_data = self.case_extractor.extract(block_content=block["content"])
                    block["case_studies"] = case_data.get("case_studies", [])
                    if block["case_studies"]:
                        print(f"[INFO] –ë–ª–æ–∫ {i+1}: –Ω–∞–π–¥–µ–Ω–æ {len(block['case_studies'])} –∫–µ–π—Å–æ–≤")
                except Exception as e:
                    print(f"[WARNING] –û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ –∫–µ–π—Å–æ–≤ –¥–ª—è –±–ª–æ–∫–∞ {i+1}: {e}")
                    block["case_studies"] = []
            
            if self.prereq_extractor and sag_config.get('use_prerequisite_extractor', True):
                try:
                    prereq_data = self.prereq_extractor.extract(block_content=block["content"])
                    block["prerequisites"] = prereq_data
                    if prereq_data.get("prerequisites") or prereq_data.get("recommended_sequence"):
                        print(f"[INFO] –ë–ª–æ–∫ {i+1}: –Ω–∞–π–¥–µ–Ω—ã –ø—Ä–µ–¥–ø–æ—Å—ã–ª–∫–∏ –∏ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è")
                except Exception as e:
                    print(f"[WARNING] –û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ –ø—Ä–µ–¥–ø–æ—Å—ã–ª–æ–∫ –¥–ª—è –±–ª–æ–∫–∞ {i+1}: {e}")
                    block["prerequisites"] = {
                        "prerequisites": [],
                        "recommended_sequence": [],
                        "common_mistakes": []
                    }
        
        doc = {
            "document_title": f"–õ–µ–∫—Ü–∏—è –°–∞—Ä—Å–µ–∫–µ–Ω–æ–≤–∞: {base}",
            "document_summary": "–ö–æ–Ω—Å–ø–µ–∫—Ç –ª–µ–∫—Ü–∏–∏, —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–æ —Ç–µ–º–∞–º –Ω–µ–π—Ä–æ—Å—Ç–∞–ª–∫–∏–Ω–≥–∞/–Ω–µ–æ—Å—Ç–∞–ª–∫–∏–Ω–≥–∞.",
            "document_metadata": {
                # –°—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –ø–æ–ª—è
                "total_blocks": len(all_blocks),
                "language": "ru",
                "domain": "sarsekenov_neurostalking",
                "video_id": video_id,
                "source_url": f"https://youtube.com/watch?v={video_id}",
                "schema_version": "2.0",  # –û–ë–ù–û–í–õ–ï–ù–û –¥–æ v2.0
                
                # –ù–û–í–´–ï –ø–æ–ª—è –¥–ª—è SAG-—Å–∏—Å—Ç–µ–º—ã - –ê–í–¢–û–ú–ê–¢–ò–ß–ï–°–ö–û–ï –ó–ê–ü–û–õ–ù–ï–ù–ò–ï
                # –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—ã–µ —Ä–∞—Å—á–µ—Ç—ã –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
                **self._calculate_document_metadata(all_blocks, data.get("full_text", "")),
                "transcript_confidence": 0.85,       # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é 0.85
                "participant_count": self._estimate_participant_count(all_blocks),
                "duration_minutes": self._calculate_duration_minutes(all_blocks),
                "audio_quality": "good",             # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é good
                "processing_version": "v2.1"         # –í–µ—Ä—Å–∏—è –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞
            },
            "blocks": all_blocks,
            "full_text": data.get("full_text", ""),
        }

        # –°—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞—Ç—å ¬´–∫–æ—Ä–æ—Ç–∫–æ–µ –æ–≥–ª–∞–≤–ª–µ–Ω–∏–µ¬ª –∏ ¬´–æ–±—â–µ–µ —Ä–µ–∑—é–º–µ¬ª –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤—Å–µ—Ö –±–ª–æ–∫–æ–≤
        overview = self.generate_overview(doc)
        
        # üöÄ –†–ê–°–®–ò–†–ï–ù–ù–´–ô –∞–Ω–∞–ª–∏–∑ –≥—Ä–∞—Ñ-—Å—É—â–Ω–æ—Å—Ç–µ–π –Ω–∞ —É—Ä–æ–≤–Ω–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –¥–ª—è SAG-—Å–∏—Å—Ç–µ–º—ã
        all_document_entities = []
        all_semantic_relationships = []
        
        for block in all_blocks:
            all_document_entities.extend(block.get("graph_entities", []))
            # –°–æ–±–∏—Ä–∞–µ–º —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ —Å–≤—è–∑–∏ —Å–æ –≤—Å–µ—Ö –±–ª–æ–∫–æ–≤
            block_relationships = block.get("semantic_relationships", {})
            for rel_type, relationships in block_relationships.items():
                all_semantic_relationships.extend(relationships)
        
        # –°—á–∏—Ç–∞–µ–º —á–∞—Å—Ç–æ—Ç—É –∏ –±–µ—Ä–µ–º —Ç–æ–ø-—Å—É—â–Ω–æ—Å—Ç–∏ (—É–≤–µ–ª–∏—á–∏–ª–∏ —Å 12 –¥–æ 20)
        entity_counts = {}
        for entity in all_document_entities:
            entity_counts[entity] = entity_counts.get(entity, 0) + 1
        top_document_entities = sorted(entity_counts.items(), key=lambda x: x[1], reverse=True)[:20]
        
        # üÜï –ê–Ω–∞–ª–∏–∑ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö —Å–≤—è–∑–µ–π –Ω–∞ —É—Ä–æ–≤–Ω–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞
        document_semantic_analysis = self.analyze_semantic_relationships(all_document_entities)
        
        # üöÄ –ü–û–°–¢–†–û–ï–ù–ò–ï –ú–ê–ö–°–ò–ú–ê–õ–¨–ù–û –ü–û–õ–ù–û–ì–û KNOWLEDGE GRAPH
        print("[INFO] Building comprehensive Knowledge Graph from all extractors...")
        knowledge_graph_data = self._build_knowledge_graph_from_blocks(all_blocks, video_id, all_document_entities)
        print(f"[SUCCESS] Knowledge Graph built: {len(knowledge_graph_data.get('nodes', []))} nodes, "
              f"{len(knowledge_graph_data.get('edges', []))} edges")
        
        doc.update({
            "overview_toc": overview.get("overview_toc", ""),
            "overview_summary": overview.get("overview_summary", ""),
            "graph_entities": top_document_entities,  # –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –≥—Ä–∞—Ñ-—Å—É—â–Ω–æ—Å—Ç–∏
            "semantic_relationships": document_semantic_analysis,  # üÜï –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ —Å–≤—è–∑–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞
            "total_graph_nodes": len(self.graph_nodes),  # üÜï –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –≥—Ä–∞—Ñ-—Å–∏—Å—Ç–µ–º—ã
            "total_graph_relationships": len(self.graph_relationships),  # üÜï –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –≥—Ä–∞—Ñ-—Å–∏—Å—Ç–µ–º—ã
            
            # üöÄ –ù–û–í–û–ï: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –ø–æ–ª–Ω—ã–π Knowledge Graph —Å–æ –≤—Å–µ–º–∏ –≤–∑–∞–∏–º–æ—Å–≤—è–∑—è–º–∏
            "knowledge_graph": knowledge_graph_data,
        })

        output_dir.mkdir(parents=True, exist_ok=True)
        json_path = output_dir / f"{base}.for_vector.json"
        md_path = output_dir / f"{base}.for_review.md"
        with open(json_path, "wb") as f:
            f.write(orjson.dumps(doc, option=orjson.OPT_INDENT_2))
        self.save_markdown(md_path, doc, base)

        print(f"[SUCCESS] JSON: {json_path}")
        print(f"[SUCCESS] MD  : {md_path}")
        return {"json_output": str(json_path), "md_output": str(md_path), "blocks": len(all_blocks)}

    # –°–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å —Å –æ—Ä–∫–µ—Å—Ç—Ä–∞—Ç–æ—Ä–æ–º (—Ç–∞ –∂–µ —Å–∏–≥–Ω–∞—Ç—É—Ä–∞, —á—Ç–æ —É SubtitlesProcessor)
    def process_subtitles_file(self, transcript_path: Path, output_dir: Path) -> Dict[str, Any]:
        result = self.process_file(transcript_path, output_dir)
        return {
            "json_output": result["json_output"],
            "md_output": result["md_output"],
            "blocks_created": result.get("blocks", 0),
        }

    def _hms_to_seconds(self, hms_str: str) -> Optional[int]:
        """–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç HH:MM:SS –≤ —Å–µ–∫—É–Ω–¥—ã"""
        try:
            parts = hms_str.split(':')
            if len(parts) == 3:
                h, m, s = map(int, parts)
                return h * 3600 + m * 60 + s
        except:
            pass
        return None

    # ---------------------- SAG v2.0 OVERVIEW GENERATION ---------------------- #
    def generate_overview(self, doc: Dict[str, Any]) -> Dict[str, str]:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —É–ª—É—á—à–µ–Ω–Ω—ã–π –æ–±–∑–æ—Ä –¥–ª—è SAG-—Å–∏—Å—Ç–µ–º—ã v2.0
        """
        return self.generate_enhanced_overview(doc)
    
    def generate_enhanced_overview(self, doc: Dict[str, Any]) -> Dict[str, str]:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —É–ª—É—á—à–µ–Ω–Ω—ã–π –æ–±–∑–æ—Ä –¥–ª—è SAG-—Å–∏—Å—Ç–µ–º—ã v2.0
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∏ –≥—Ä–∞—Ñ-—Å—É—â–Ω–æ—Å—Ç–∏ –¥–ª—è –±–æ–ª–µ–µ —Ç–æ—á–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
        """
        blocks = doc.get("blocks", [])
        metadata = doc.get("document_metadata", {})
        
        # –°–æ–±–∏—Ä–∞–µ–º –∞–Ω–∞–ª–∏—Ç–∏–∫—É –¥–ª—è overview
        main_topics = metadata.get("main_topics", [])
        has_dialogue = metadata.get("has_dialogue", False)
        collection_target = metadata.get("collection_target", "neurostalking_basics")
        difficulty_level = metadata.get("difficulty_level", "intermediate")
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –≥—Ä–∞—Ñ-—Å—É—â–Ω–æ—Å—Ç–∏
        all_entities = []
        for block in blocks:
            all_entities.extend(block.get("graph_entities", []))
        
        # –¢–æ–ø-—Å—É—â–Ω–æ—Å—Ç–∏ –ø–æ —á–∞—Å—Ç–æ—Ç–µ
        entity_counts = {}
        for entity in all_entities:
            entity_counts[entity] = entity_counts.get(entity, 0) + 1
        top_entities = sorted(entity_counts.items(), key=lambda x: x[1], reverse=True)[:8]
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ —Ç–æ–Ω–∞
        tones = [block.get("emotional_tone", "neutral") for block in blocks]
        dominant_tone = max(set(tones), key=tones.count) if tones else "neutral"
        
        # –ë–∞–∑–æ–≤–æ–µ –æ–≥–ª–∞–≤–ª–µ–Ω–∏–µ
        titles = [b.get("title", "") for b in blocks if b.get("title")]
        toc_sentence_default = " ‚Ä¢ ".join(titles[:15])
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —É–ª—É—á—à–µ–Ω–Ω—ã–π overview —Å –ø–æ–º–æ—â—å—é LLM
        try:
            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è LLM
            context_data = {
                "main_topics": main_topics[:5],
                "top_entities": [entity[0] for entity in top_entities[:6]] if top_entities else [],
                "collection_target": collection_target,
                "difficulty_level": difficulty_level,
                "has_dialogue": has_dialogue,
                "dominant_tone": dominant_tone,
                "block_count": len(blocks),
                "titles": titles[:10]
            }
            
            prompt = f"""
{self.domain_context}

–ó–ê–î–ê–ß–ê: –°–æ–∑–¥–∞–π —É–ª—É—á—à–µ–Ω–Ω—ã–π –æ–±–∑–æ—Ä –ª–µ–∫—Ü–∏–∏ –¥–ª—è SAG-—Å–∏—Å—Ç–µ–º—ã v2.0.

–ö–û–ù–¢–ï–ö–°–¢ –õ–ï–ö–¶–ò–ò:
- –ì–ª–∞–≤–Ω—ã–µ —Ç–µ–º—ã: {', '.join(main_topics[:5])}
- –ö–ª—é—á–µ–≤—ã–µ –∫–æ–Ω—Ü–µ–ø—Ç—ã: {', '.join([e[0] for e in top_entities[:6]]) if top_entities else '–æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç'}
- –¶–µ–ª–µ–≤–∞—è –∫–æ–ª–ª–µ–∫—Ü–∏—è: {collection_target}
- –£—Ä–æ–≤–µ–Ω—å —Å–ª–æ–∂–Ω–æ—Å—Ç–∏: {difficulty_level}
- –§–æ—Ä–º–∞—Ç: {"—Å–µ–º–∏–Ω–∞—Ä —Å –¥–∏–∞–ª–æ–≥–∞–º–∏" if has_dialogue else "–ª–µ–∫—Ü–∏—è-–º–æ–Ω–æ–ª–æ–≥"}
- –≠–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π —Ç–æ–Ω: {dominant_tone}
- –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –±–ª–æ–∫–æ–≤: {len(blocks)}

–°–û–ó–î–ê–ô:
1) **–ö—Ä–∞—Ç–∫–æ–µ –æ–≥–ª–∞–≤–ª–µ–Ω–∏–µ** (–æ–¥–Ω–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ —Å –æ—Å–Ω–æ–≤–Ω—ã–º–∏ —Ç–µ–º–∞–º–∏ –ø–æ –ø–æ—Ä—è–¥–∫—É)
2) **–û–±–∑–æ—Ä –ª–µ–∫—Ü–∏–∏** (3-4 —Ä–∞–∑–≤–µ—Ä–Ω—É—Ç—ã—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –æ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–∏, –ø–æ–¥—Ö–æ–¥–µ –°–∞—Ä—Å–µ–∫–µ–Ω–æ–≤–∞, –∫–ª—é—á–µ–≤—ã—Ö –∏–Ω—Å–∞–π—Ç–∞—Ö)

–¢–†–ï–ë–û–í–ê–ù–ò–Ø:
- –°–æ—Ö—Ä–∞–Ω—è–π —Ç–µ—Ä–º–∏–Ω–æ–ª–æ–≥–∏—é –Ω–µ–π—Ä–æ—Å—Ç–∞–ª–∫–∏–Ω–≥–∞
- –£–∫–∞–∂–∏ —Å–ø–µ—Ü–∏—Ñ–∏–∫—É –ø–æ–¥—Ö–æ–¥–∞ –∞–≤—Ç–æ—Ä–∞
- –û—Ç—Ä–∞–∑–∏ —É—Ä–æ–≤–µ–Ω—å —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –∏ —Ñ–æ—Ä–º–∞—Ç
- **–ú–ò–ù–ò–ú–£–ú 200 —Å–∏–º–≤–æ–ª–æ–≤** –≤ –æ–±–∑–æ—Ä–µ (—ç—Ç–æ –∫—Ä–∏—Ç–∏—á–Ω–æ!)
- **–ò–°–ü–û–õ–¨–ó–£–ô –ü–†–ê–í–ò–õ–¨–ù–´–ï –ì–†–ê–ú–ú–ê–¢–ò–ß–ï–°–ö–ò–ï –§–û–†–ú–´ –¥–ª—è —Ä—É—Å—Å–∫–∏—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤:**
  * "–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –æ—Å–æ–∑–Ω–∞–Ω–∏—è" (–Ω–µ "–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –æ—Å–æ–∑–Ω–∞–Ω–∏–µ")
  * "–ø—Ä–∞–∫—Ç–∏–∫–∏ –º–µ–¥–∏—Ç–∞—Ü–∏–∏" (–Ω–µ "–ø—Ä–∞–∫—Ç–∏–∫–∏ –º–µ–¥–∏—Ç–∞—Ü–∏—è") 
  * "–ø—Ä–∏—Ä–æ–¥–∞ –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è" (–Ω–µ "–ø—Ä–∏—Ä–æ–¥–∞ –≤–æ—Å–ø—Ä–∏—è—Ç–∏–µ")
  * "–∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –æ–ø—ã—Ç–∞" (–Ω–µ "–∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –æ–ø—ã—Ç")
  * "—Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—è —Å–æ–∑–Ω–∞–Ω–∏—è" (–Ω–µ "—Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—è —Å–æ–∑–Ω–∞–Ω–∏–µ")
  
**–ö–û–ù–ö–†–ï–¢–ù–´–ï –ü–†–ò–ú–ï–†–´ –¥–ª—è –¥–∞–Ω–Ω–æ–π –ª–µ–∫—Ü–∏–∏:**
- –ï—Å–ª–∏ —Ç–µ–º–∞ "–∏—Å—Ü–µ–ª–µ–Ω–∏–µ" ‚Üí –∏—Å–ø–æ–ª—å–∑—É–π "–∏—Å—Ü–µ–ª–µ–Ω–∏—è" (—Ä–æ–¥–∏—Ç–µ–ª—å–Ω—ã–π –ø–∞–¥–µ–∂)
- –ï—Å–ª–∏ —Ç–µ–º–∞ "–æ—Å–æ–∑–Ω–∞–≤–∞–Ω–∏–µ" ‚Üí –∏—Å–ø–æ–ª—å–∑—É–π "–æ—Å–æ–∑–Ω–∞–≤–∞–Ω–∏—è" (—Ä–æ–¥–∏—Ç–µ–ª—å–Ω—ã–π –ø–∞–¥–µ–∂)
- –ï—Å–ª–∏ —Ç–µ–º–∞ "–±–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å" ‚Üí –∏—Å–ø–æ–ª—å–∑—É–π "–±–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç–∏" (—Ä–æ–¥–∏—Ç–µ–ª—å–Ω—ã–π –ø–∞–¥–µ–∂)
- –ï—Å–ª–∏ —Ç–µ–º–∞ "–∞–≤—Ç–æ–º–∞—Ç–∏–∑–º—ã" ‚Üí –∏—Å–ø–æ–ª—å–∑—É–π "–∞–≤—Ç–æ–º–∞—Ç–∏–∑–º–æ–≤" (—Ä–æ–¥–∏—Ç–µ–ª—å–Ω—ã–π –ø–∞–¥–µ–∂)
- –ï—Å–ª–∏ —Ç–µ–º–∞ "—Å–∞–º–æ–ø–æ–∑–Ω–∞–Ω–∏–µ" ‚Üí –∏—Å–ø–æ–ª—å–∑—É–π "—Å–∞–º–æ–ø–æ–∑–Ω–∞–Ω–∏—è" (—Ä–æ–¥–∏—Ç–µ–ª—å–Ω—ã–π –ø–∞–¥–µ–∂)
- –î–æ–±–∞–≤—å –¥–µ—Ç–∞–ª–∏ –æ –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏—Ö –∞—Å–ø–µ–∫—Ç–∞—Ö –∏ –∏–Ω—Å–∞–π—Ç–∞—Ö

–§–û–†–ú–ê–¢ –û–¢–í–ï–¢–ê:
–û–ì–õ–ê–í–õ–ï–ù–ò–ï: [–∫—Ä–∞—Ç–∫–æ–µ –æ–≥–ª–∞–≤–ª–µ–Ω–∏–µ –æ–¥–Ω–∏–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ–º]

–û–ë–ó–û–†: [—Ä–∞–∑–≤–µ—Ä–Ω—É—Ç—ã–π –æ–±–∑–æ—Ä –ª–µ–∫—Ü–∏–∏ 3-4 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è, –º–∏–Ω–∏–º—É–º 200 —Å–∏–º–≤–æ–ª–æ–≤]
"""
            
            resp = self._ask(self.refine_model or self.primary_model, prompt, max_tokens=400, temperature=0.2)
            
            # –ü–∞—Ä—Å–∏–º –æ—Ç–≤–µ—Ç
            lines = [l.strip() for l in resp.splitlines() if l.strip()]
            toc_line = ""
            summary_lines = []
            
            for line in lines:
                if line.upper().startswith("–û–ì–õ–ê–í–õ–ï–ù–ò–ï:"):
                    toc_line = line.split(":", 1)[-1].strip()
                elif line.upper().startswith("–û–ë–ó–û–†:"):
                    summary_lines.append(line.split(":", 1)[-1].strip())
                elif summary_lines and not line.upper().startswith(("–û–ì–õ–ê–í–õ–ï–ù–ò–ï", "–û–ë–ó–û–†")):
                    summary_lines.append(line)
            
            overview_summary = " ".join(summary_lines).strip()
            
            # –í–ê–õ–ò–î–ê–¶–ò–Ø: –ü—Ä–æ–≤–µ—Ä—è–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω—É—é –¥–ª–∏–Ω—É overview
            if len(overview_summary) < 200:
                print(f"[WARNING] LLM overview —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π ({len(overview_summary)} —Å–∏–º–≤–æ–ª–æ–≤), –∏—Å–ø–æ–ª—å–∑—É–µ–º fallback —Å –º–æ—Ä—Ñ–æ–ª–æ–≥–∏–µ–π")
                overview_summary = self._generate_fallback_summary(metadata, top_entities)
            
            return {
                "overview_toc": toc_line or toc_sentence_default,
                "overview_summary": overview_summary[:800],
            }
            
        except Exception as e:
            # Fallback: —Å–æ–∑–¥–∞–µ–º –æ–±–∑–æ—Ä –Ω–∞ –æ—Å–Ω–æ–≤–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
            return {
                "overview_toc": toc_sentence_default,
                "overview_summary": self._generate_fallback_summary(metadata, top_entities)
            }
    
    def _generate_fallback_summary(self, metadata: dict, top_entities: list) -> str:
        """–†–µ–∑–µ—Ä–≤–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –æ–±–∑–æ—Ä–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö —Å –º–æ—Ä—Ñ–æ–ª–æ–≥–∏–µ–π"""
        main_topics = metadata.get("main_topics", [])
        collection = metadata.get("collection_target", "neurostalking_basics")
        has_dialogue = metadata.get("has_dialogue", False)
        difficulty_level = metadata.get("difficulty_level", "intermediate")
        
        # –ú–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ —Ñ–æ—Ä–º—ã –¥–ª—è —á–∞—Å—Ç—ã—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤ (—Ä–æ–¥–∏—Ç–µ–ª—å–Ω—ã–π –ø–∞–¥–µ–∂)
        morph_dict = {
            "–æ—Å–æ–∑–Ω–∞–Ω–∏–µ": "–æ—Å–æ–∑–Ω–∞–Ω–∏—è",
            "–æ—Å–æ–∑–Ω–∞–≤–∞–Ω–∏–µ": "–æ—Å–æ–∑–Ω–∞–≤–∞–Ω–∏—è",
            "–∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è": "–∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏", 
            "–≤–æ—Å–ø—Ä–∏—è—Ç–∏–µ": "–≤–æ—Å–ø—Ä–∏—è—Ç–∏—è",
            "–∏—Å—Ü–µ–ª–µ–Ω–∏–µ": "–∏—Å—Ü–µ–ª–µ–Ω–∏—è",
            "–≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ": "–≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è",
            "–ø—Ä–∞–∫—Ç–∏–∫–∞": "–ø—Ä–∞–∫—Ç–∏–∫",
            "–º–µ–¥–∏—Ç–∞—Ü–∏—è": "–º–µ–¥–∏—Ç–∞—Ü–∏–∏",
            "–Ω–∞–±–ª—é–¥–µ–Ω–∏–µ": "–Ω–∞–±–ª—é–¥–µ–Ω–∏—è",
            "–º–µ—Ç–∞–Ω–∞–±–ª—é–¥–µ–Ω–∏–µ": "–º–µ—Ç–∞–Ω–∞–±–ª—é–¥–µ–Ω–∏—è",
            "–ø–æ–Ω–∏–º–∞–Ω–∏–µ": "–ø–æ–Ω–∏–º–∞–Ω–∏—è",
            "–ø–µ—Ä–µ–∂–∏–≤–∞–Ω–∏–µ": "–ø–µ—Ä–µ–∂–∏–≤–∞–Ω–∏–π",
            "—Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—è": "—Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏",
            "–ø—Ä–æ—Å–≤–µ—Ç–ª–µ–Ω–∏–µ": "–ø—Ä–æ—Å–≤–µ—Ç–ª–µ–Ω–∏—è",
            "–±–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å": "–±–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç–∏",
            "—Ü–µ–Ω—Ç—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ": "—Ü–µ–Ω—Ç—Ä–∏—Ä–æ–≤–∞–Ω–∏—è",
            "–≤–Ω–∏–º–∞–Ω–∏–µ": "–≤–Ω–∏–º–∞–Ω–∏—è",
            "–ø—Ä–∏—Å—É—Ç—Å—Ç–≤–∏–µ": "–ø—Ä–∏—Å—É—Ç—Å—Ç–≤–∏—è",
            "–ø–∞—Ç—Ç–µ—Ä–Ω": "–ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤",
            "–ø–∞—Ç—Ç–µ—Ä–Ω—ã": "–ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤",
            "–∞–≤—Ç–æ–º–∞—Ç–∏–∑–º": "–∞–≤—Ç–æ–º–∞—Ç–∏–∑–º–æ–≤",
            "–∞–≤—Ç–æ–º–∞—Ç–∏–∑–º—ã": "–∞–≤—Ç–æ–º–∞—Ç–∏–∑–º–æ–≤",
            "—Å–∞–º–æ–ø–æ–∑–Ω–∞–Ω–∏–µ": "—Å–∞–º–æ–ø–æ–∑–Ω–∞–Ω–∏—è",
            "—Å–æ–∑–Ω–∞–Ω–∏–µ": "—Å–æ–∑–Ω–∞–Ω–∏—è",
            "–∞–±—Å–æ–ª—é—Ç": "–∞–±—Å–æ–ª—é—Ç–∞",
            "–ø—Ä–∏—Ä–æ–¥–∞": "–ø—Ä–∏—Ä–æ–¥—ã",
            "–æ–ø—ã—Ç": "–æ–ø—ã—Ç–∞",
            "–º—É–¥—Ä–æ—Å—Ç—å": "–º—É–¥—Ä–æ—Å—Ç–∏",
            "—Ä–æ—Å—Ç": "—Ä–æ—Å—Ç–∞"
        }
        
        # –°–∏–Ω–æ–Ω–∏–º—ã –¥–ª—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è
        synonyms = {
            "–æ—Å–æ–∑–Ω–∞–Ω–∏–µ": "–æ—Å–æ–∑–Ω–∞–Ω–Ω–æ–≥–æ –ø—Ä–∏—Å—É—Ç—Å—Ç–≤–∏—è",
            "–∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è": "–∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ –æ–ø—ã—Ç–∞", 
            "–≤–æ—Å–ø—Ä–∏—è—Ç–∏–µ": "–ø—Ä–∏—Ä–æ–¥—ã –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è",
            "–∏—Å—Ü–µ–ª–µ–Ω–∏–µ": "–≥–ª—É–±–∏–Ω–Ω–æ–≥–æ –∏—Å—Ü–µ–ª–µ–Ω–∏—è",
            "–≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ": "–∂–∏–≤–æ–≥–æ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è"
        }
        
        summary_parts = []
        
        # –û—Å–Ω–æ–≤–Ω–∞—è —Ç–µ–º–∞ —Å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –≥—Ä–∞–º–º–∞—Ç–∏–∫–æ–π
        if main_topics:
            topics_genitive = []
            for topic in main_topics[:3]:
                # –ü—Ä–∏–º–µ–Ω—è–µ–º –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—é
                genitive_form = morph_dict.get(topic.lower(), topic)
                topics_genitive.append(genitive_form)
            
            if len(topics_genitive) == 1:
                summary_parts.append(f"–õ–µ–∫—Ü–∏—è –ø–æ—Å–≤—è—â–µ–Ω–∞ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—é {topics_genitive[0]}.")
            elif len(topics_genitive) == 2:
                summary_parts.append(f"–õ–µ–∫—Ü–∏—è –ø–æ—Å–≤—è—â–µ–Ω–∞ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—é {topics_genitive[0]} –∏ {topics_genitive[1]}.")
            else:
                summary_parts.append(f"–õ–µ–∫—Ü–∏—è –ø–æ—Å–≤—è—â–µ–Ω–∞ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—é {', '.join(topics_genitive[:-1])} –∏ {topics_genitive[-1]}.")
        else:
            summary_parts.append("–õ–µ–∫—Ü–∏—è –ø–æ—Å–≤—è—â–µ–Ω–∞ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—é –ø—Ä–∞–∫—Ç–∏–∫ –Ω–µ–π—Ä–æ—Å—Ç–∞–ª–∫–∏–Ω–≥–∞.")
        
        # –ü–æ–¥—Ö–æ–¥ –∞–≤—Ç–æ—Ä–∞ —Å —Å–∏–Ω–æ–Ω–∏–º–∞–º–∏
        if main_topics:
            author_concepts = []
            for topic in main_topics[:2]:
                synonym = synonyms.get(topic.lower(), morph_dict.get(topic.lower(), topic))
                author_concepts.append(synonym)
            
            if author_concepts:
                summary_parts.append(f"–°–∞–ª–∞–º–∞—Ç –°–∞—Ä—Å–µ–∫–µ–Ω–æ–≤ –ø–æ–¥—á—ë—Ä–∫–∏–≤–∞–µ—Ç –≤–∞–∂–Ω–æ—Å—Ç—å {' –∏ '.join(author_concepts)}.")
        
        # –§–æ—Ä–º–∞—Ç
        if has_dialogue:
            summary_parts.append("–§–æ—Ä–º–∞—Ç: –ª–µ–∫—Ü–∏—è —Å –¥–∏–∞–ª–æ–≥–∞–º–∏ –∏ –≤–æ–ø—Ä–æ—Å–∞–º–∏ —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤.")
        else:
            summary_parts.append("–§–æ—Ä–º–∞—Ç: –ª–µ–∫—Ü–∏—è-–º–æ–Ω–æ–ª–æ–≥ —Å –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–º–∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è–º–∏.")
        
        # –°–æ–µ–¥–∏–Ω—è–µ–º –∏ —á–∏—Å—Ç–∏–º
        text = " ".join(summary_parts)
        
        # –£–±–∏—Ä–∞–µ–º –¥–≤–æ–π–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã –∏ –ª–∏—à–Ω–∏–µ –∑–∞–ø—è—Ç—ã–µ
        import re
        text = re.sub(r'\s+', ' ', text)
        text = re.sub(r',\s*,', ',', text)
        
        # –í–ê–õ–ò–î–ê–¶–ò–Ø: –ï—Å–ª–∏ —Ç–µ–∫—Å—Ç —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π, –¥–æ–±–∞–≤–ª—è–µ–º –¥–µ—Ç–∞–ª–∏
        if len(text.strip()) < 200:
            # –î–æ–±–∞–≤–ª—è–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –¥–µ—Ç–∞–ª–∏ –¥–ª—è –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è –º–∏–Ω–∏–º—É–º–∞
            additional_details = []
            
            if has_dialogue:
                additional_details.append("–û—Å–æ–±–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ —É–¥–µ–ª—è–µ—Ç—Å—è –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–º—É —Ñ–æ—Ä–º–∞—Ç—É —Å –≤–æ–ø—Ä–æ—Å–∞–º–∏ –∏ –æ—Ç–≤–µ—Ç–∞–º–∏.")
            
            if difficulty_level == "advanced":
                additional_details.append("–ú–∞—Ç–µ—Ä–∏–∞–ª —Ä–∞—Å—Å—á–∏—Ç–∞–Ω –Ω–∞ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π —É—Ä–æ–≤–µ–Ω—å –ø—Ä–∞–∫—Ç–∏–∫—É—é—â–∏—Ö.")
            elif difficulty_level == "beginner":
                additional_details.append("–õ–µ–∫—Ü–∏—è –ø–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è –Ω–∞—á–∏–Ω–∞—é—â–∏—Ö –ø—Ä–∞–∫—Ç–∏–∫–æ–≤ –Ω–µ–π—Ä–æ—Å—Ç–∞–ª–∫–∏–Ω–≥–∞.")
            
            if main_topics and len(main_topics) > 2:
                additional_details.append(f"–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—é—Ç—Å—è –∞—Å–ø–µ–∫—Ç—ã {' –∏ '.join(main_topics[2:4])}.")
            
            # –û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–´–ï –¥–æ–ø–æ–ª–Ω–µ–Ω–∏—è –¥–ª—è –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è –º–∏–Ω–∏–º—É–º–∞
            if collection != "neurostalking_basics":
                additional_details.append(f"–õ–µ–∫—Ü–∏—è –æ—Ç–Ω–æ—Å–∏—Ç—Å—è –∫ –∫–æ–ª–ª–µ–∫—Ü–∏–∏ '{collection}' –∏ —Å–æ–¥–µ—Ä–∂–∏—Ç —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–∞–∫—Ç–∏–∫–∏.")
            
            # –î–æ–±–∞–≤–ª—è–µ–º –¥–µ—Ç–∞–ª–∏ –æ –∫–ª—é—á–µ–≤—ã—Ö –∫–æ–Ω—Ü–µ–ø—Ç–∞—Ö
            if top_entities and len(top_entities) > 0:
                top_concepts = [e[0] for e in top_entities[:3]]
                additional_details.append(f"–ö–ª—é—á–µ–≤—ã–µ –∫–æ–Ω—Ü–µ–ø—Ç—ã –≤–∫–ª—é—á–∞—é—Ç: {', '.join(top_concepts)}.")
            
            # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ñ–æ—Ä–º–∞—Ç–µ
            if has_dialogue:
                additional_details.append("–ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –ø–æ–∑–≤–æ–ª—è–µ—Ç —É—á–∞—Å—Ç–Ω–∏–∫–∞–º –∑–∞–¥–∞–≤–∞—Ç—å –≤–æ–ø—Ä–æ—Å—ã –∏ –ø–æ–ª—É—á–∞—Ç—å –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –æ—Ç–≤–µ—Ç—ã.")
            else:
                additional_details.append("–ú–æ–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π —Ñ–æ—Ä–º–∞—Ç –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –≥–ª—É–±–æ–∫–æ–µ –ø–æ–≥—Ä—É–∂–µ–Ω–∏–µ –≤ —Ç–µ–º—É.")
            
            if additional_details:
                text += " " + " ".join(additional_details)
        
        return text.strip()

    # ---------------------- SAG v2.0 VALIDATION ---------------------- #
    
    def validate_enhanced_json(self, result: dict) -> list:
        """
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ JSON –¥–ª—è SAG v2.0
        """
        errors = []
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –ø–æ–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–∞
        required_doc_fields = [
            "document_title", "document_summary", "overview_summary",
            "document_metadata", "blocks"
        ]
        
        for field in required_doc_fields:
            if not result.get(field):
                errors.append(f"–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ–µ –ø–æ–ª–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞: {field}")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞ SAG v2.0
        metadata = result.get("document_metadata", {})
        required_metadata = [
            "schema_version", "collection_target", "main_topics", "has_dialogue", 
            "difficulty_level", "processing_version"
        ]
        
        for field in required_metadata:
            if field not in metadata:
                errors.append(f"–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –∫—Ä–∏—Ç–∏—á–µ—Å–∫–æ–µ –º–µ—Ç–∞–¥–∞–Ω–Ω–æ–µ SAG: {field}")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤–µ—Ä—Å–∏—é —Å—Ö–µ–º—ã
        if metadata.get("schema_version") != "2.0":
            errors.append(f"–ù–µ–≤–µ—Ä–Ω–∞—è –≤–µ—Ä—Å–∏—è —Å—Ö–µ–º—ã: {metadata.get('schema_version')}, –æ–∂–∏–¥–∞–µ—Ç—Å—è 2.0")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –±–ª–æ–∫–∏
        blocks = result.get("blocks", [])
        if not blocks:
            errors.append("–û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –±–ª–æ–∫–∏ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ")
        
        for i, block in enumerate(blocks):
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–æ–≤—ã–µ –ø–æ–ª—è SAG v2.0
            required_block_fields = [
                "speaker", "block_type", "emotional_tone", "conceptual_depth", 
                "complexity_score", "graph_entities", "contains_practice"
            ]
            
            for field in required_block_fields:
                if field not in block:
                    errors.append(f"–ë–ª–æ–∫ {i}: –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –ø–æ–ª–µ SAG v2.0: {field}")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–æ–≤—ã–µ –ø–æ–ª—è —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä–æ–≤ SAG v2.0
            if "safety" in block:
                safety = block["safety"]
                if not isinstance(safety, dict):
                    errors.append(f"–ë–ª–æ–∫ {i}: –ø–æ–ª–µ 'safety' –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å —Å–ª–æ–≤–∞—Ä–µ–º")
                else:
                    required_safety_keys = ["contraindications", "limitations", "when_to_stop", 
                                           "when_to_seek_professional_help", "notes"]
                    for key in required_safety_keys:
                        if key not in safety:
                            errors.append(f"–ë–ª–æ–∫ {i}: –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –∫–ª—é—á 'safety.{key}'")
                        elif not isinstance(safety[key], list):
                            errors.append(f"–ë–ª–æ–∫ {i}: 'safety.{key}' –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Å–ø–∏—Å–∫–æ–º")
            
            if "causal_chains" in block:
                if not isinstance(block["causal_chains"], list):
                    errors.append(f"–ë–ª–æ–∫ {i}: –ø–æ–ª–µ 'causal_chains' –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å —Å–ø–∏—Å–∫–æ–º")
                else:
                    for j, process in enumerate(block["causal_chains"]):
                        if not isinstance(process, dict):
                            errors.append(f"–ë–ª–æ–∫ {i}, –ø—Ä–æ—Ü–µ—Å—Å {j}: –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Å–ª–æ–≤–∞—Ä–µ–º")
                        elif "name" not in process or "steps" not in process:
                            errors.append(f"–ë–ª–æ–∫ {i}, –ø—Ä–æ—Ü–µ—Å—Å {j}: –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –ø–æ–ª—è 'name' –∏–ª–∏ 'steps'")
            
            if "concept_hierarchy" in block:
                if not isinstance(block["concept_hierarchy"], list):
                    errors.append(f"–ë–ª–æ–∫ {i}: –ø–æ–ª–µ 'concept_hierarchy' –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å —Å–ø–∏—Å–∫–æ–º")
                else:
                    for j, concept in enumerate(block["concept_hierarchy"]):
                        if not isinstance(concept, dict):
                            errors.append(f"–ë–ª–æ–∫ {i}, –∫–æ–Ω—Ü–µ–ø—Ç {j}: –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Å–ª–æ–≤–∞—Ä–µ–º")
                        elif "name" not in concept or "level" not in concept:
                            errors.append(f"–ë–ª–æ–∫ {i}, –∫–æ–Ω—Ü–µ–ø—Ç {j}: –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –ø–æ–ª—è 'name' –∏–ª–∏ 'level'")
            
            if "case_studies" in block:
                if not isinstance(block["case_studies"], list):
                    errors.append(f"–ë–ª–æ–∫ {i}: –ø–æ–ª–µ 'case_studies' –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å —Å–ø–∏—Å–∫–æ–º")
                else:
                    for j, case in enumerate(block["case_studies"]):
                        if not isinstance(case, dict):
                            errors.append(f"–ë–ª–æ–∫ {i}, –∫–µ–π—Å {j}: –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Å–ª–æ–≤–∞—Ä–µ–º")
                        elif "id" not in case or "situation" not in case:
                            errors.append(f"–ë–ª–æ–∫ {i}, –∫–µ–π—Å {j}: –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –ø–æ–ª—è 'id' –∏–ª–∏ 'situation'")
            
            if "prerequisites" in block:
                prereq = block["prerequisites"]
                if not isinstance(prereq, dict):
                    errors.append(f"–ë–ª–æ–∫ {i}: –ø–æ–ª–µ 'prerequisites' –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å —Å–ª–æ–≤–∞—Ä–µ–º")
                else:
                    required_prereq_keys = ["prerequisites", "recommended_sequence", "common_mistakes"]
                    for key in required_prereq_keys:
                        if key not in prereq:
                            errors.append(f"–ë–ª–æ–∫ {i}: –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –∫–ª—é—á 'prerequisites.{key}'")
                        elif not isinstance(prereq[key], list):
                            errors.append(f"–ë–ª–æ–∫ {i}: 'prerequisites.{key}' –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Å–ø–∏—Å–∫–æ–º")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö
            if "complexity_score" in block:
                try:
                    score = float(block["complexity_score"])
                    if not 1.0 <= score <= 10.0:
                        errors.append(f"–ë–ª–æ–∫ {i}: complexity_score –≤–Ω–µ –¥–∏–∞–ø–∞–∑–æ–Ω–∞ 1-10: {score}")
                except (ValueError, TypeError):
                    errors.append(f"–ë–ª–æ–∫ {i}: complexity_score –Ω–µ —è–≤–ª—è–µ—Ç—Å—è —á–∏—Å–ª–æ–º")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≥—Ä–∞—Ñ-—Å—É—â–Ω–æ—Å—Ç–∏
            if "graph_entities" in block:
                entities = block["graph_entities"]
                if not isinstance(entities, list):
                    errors.append(f"–ë–ª–æ–∫ {i}: graph_entities –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Å–ø–∏—Å–∫–æ–º")
                elif len(entities) > 15:
                    errors.append(f"–ë–ª–æ–∫ {i}: —Å–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ –≥—Ä–∞—Ñ-—Å—É—â–Ω–æ—Å—Ç–µ–π: {len(entities)}")
        
        return errors

    def generate_quality_report(self, result: dict) -> dict:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç—á–µ—Ç –æ –∫–∞—á–µ—Å—Ç–≤–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏ SAG v2.0
        """
        blocks = result.get("blocks", [])
        metadata = result.get("document_metadata", {})
        
        # –ë–∞–∑–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        report = {
            "schema_version": metadata.get("schema_version"),
            "processing_version": metadata.get("processing_version"),
            "total_blocks": len(blocks),
            "collection_target": metadata.get("collection_target"),
            "has_dialogue": metadata.get("has_dialogue", False),
            "difficulty_level": metadata.get("difficulty_level"),
        }
        
        if blocks:
            # –ê–Ω–∞–ª–∏–∑ –±–ª–æ–∫–æ–≤
            speakers = [b.get("speaker") for b in blocks]
            block_types = [b.get("block_type") for b in blocks]
            tones = [b.get("emotional_tone") for b in blocks]
            
            report.update({
                # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–ø–∏–∫–µ—Ä–æ–≤
                "sarsekenov_blocks": speakers.count("sarsekenov"),
                "participant_blocks": speakers.count("participant"),
                "unknown_blocks": speakers.count("unknown"),
                "mixed_blocks": speakers.count("mixed"),
                
                # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–æ–≤ –±–ª–æ–∫–æ–≤
                "dialogue_blocks": block_types.count("dialogue"),
                "practice_blocks": block_types.count("practice"),
                "monologue_blocks": block_types.count("monologue"),
                "question_blocks": block_types.count("question"),
                
                # –≠–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–∞—è –∞–Ω–∞–ª–∏—Ç–∏–∫–∞
                "contemplative_blocks": tones.count("contemplative"),
                "explanatory_blocks": tones.count("explanatory"),
                "intense_blocks": tones.count("intense"),
                "neutral_blocks": tones.count("neutral"),
                
                # –°–ª–æ–∂–Ω–æ—Å—Ç—å
                "average_complexity": round(sum(b.get("complexity_score", 0) for b in blocks) / len(blocks), 1),
                "high_complexity_blocks": len([b for b in blocks if b.get("conceptual_depth") == "high"]),
                
                # –ì—Ä–∞—Ñ-–∞–Ω–∞–ª–∏—Ç–∏–∫–∞
                "total_unique_entities": len(set().union(*[b.get("graph_entities", []) for b in blocks])),
                "blocks_with_entities": len([b for b in blocks if b.get("graph_entities")]),
                
                # –ö–∞—á–µ—Å—Ç–≤–æ overview
                "overview_summary_length": len(result.get("overview_summary", "")),
                "overview_toc_length": len(result.get("overview_toc", "")),
            })
        
        # –û—Ü–µ–Ω–∫–∞ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏ –¥–ª—è SAG
        sag_readiness = self._calculate_sag_readiness(result, report)
        report["sag_readiness_score"] = sag_readiness
        
        return report

    def _calculate_sag_readiness(self, result: dict, report: dict) -> float:
        """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç—å –¥–ª—è SAG-—Å–∏—Å—Ç–µ–º—ã (0-100%)"""
        score = 0.0
        
        # –ë–∞–∑–æ–≤—ã–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è (40 –±–∞–ª–ª–æ–≤)
        if result.get("document_metadata", {}).get("schema_version") == "2.0":
            score += 10
        if result.get("document_metadata", {}).get("collection_target"):
            score += 10
        if result.get("overview_summary"):
            score += 10
        if result.get("blocks"):
            score += 10
        
        # –ö–∞—á–µ—Å—Ç–≤–æ –±–ª–æ–∫–æ–≤ (30 –±–∞–ª–ª–æ–≤)
        blocks = result.get("blocks", [])
        if blocks:
            # –ù–∞–ª–∏—á–∏–µ –≥—Ä–∞—Ñ-—Å—É—â–Ω–æ—Å—Ç–µ–π
            entities_coverage = report.get("blocks_with_entities", 0) / len(blocks)
            score += entities_coverage * 10
            
            # –†–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ —Ç–∏–ø–æ–≤ –±–ª–æ–∫–æ–≤
            types_count = len(set(b.get("block_type") for b in blocks))
            score += min(types_count * 5, 10)
            
            # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Å–ø–∏–∫–µ—Ä–æ–≤
            classified_speakers = len([b for b in blocks if b.get("speaker") != "unknown"])
            speaker_coverage = classified_speakers / len(blocks)
            score += speaker_coverage * 10
        
        # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ (20 –±–∞–ª–ª–æ–≤)
        metadata = result.get("document_metadata", {})
        if metadata.get("main_topics"):
            score += 10
        if metadata.get("difficulty_level") != "intermediate":  # –ù–µ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
            score += 5
        if metadata.get("has_dialogue") and report.get("dialogue_blocks", 0) > 0:
            score += 5
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –±–∞–ª–ª—ã (10 –±–∞–ª–ª–æ–≤)
        if report.get("total_unique_entities", 0) > 20:
            score += 5
        if report.get("overview_summary_length", 0) > 100:
            score += 5
        
        return min(100.0, round(score, 1))


def main() -> int:
    ap = argparse.ArgumentParser(description="–ü—Ä–æ—Ü–µ—Å—Å–æ—Ä –ª–µ–∫—Ü–∏–π –°. –°–∞—Ä—Å–µ–∫–µ–Ω–æ–≤–∞")
    ap.add_argument("--input", help="–§–∞–π–ª –∏–ª–∏ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å —Å—É–±—Ç–∏—Ç—Ä–∞–º–∏ (.json). –ï—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω–æ, –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω urls.txt")
    ap.add_argument("--output", default="data/sag_final", help="–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ SAG v2.0")
    ap.add_argument("--primary-model", default="gpt-4o-mini", help="–û—Å–Ω–æ–≤–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –Ω–∞—Ä–µ–∑–∫–∏")
    ap.add_argument("--refine-model", default="gpt-5-mini", help="–ú–æ–¥–µ–ª—å –¥–ª—è –ø–æ–ª–∏—Ä–æ–≤–∫–∏")
    ap.add_argument("--urls-file", default=str(PROJECT_ROOT / "urls.txt"), help="–§–∞–π–ª —Å–æ —Å–ø–∏—Å–∫–æ–º URL (–ø–æ –æ–¥–Ω–æ–º—É –Ω–∞ —Å—Ç—Ä–æ–∫—É)")
    ap.add_argument("--language", default="ru", help="–ü—Ä–µ–¥–ø–æ—á–∏—Ç–∞–µ–º—ã–π —è–∑—ã–∫ —Å—É–±—Ç–∏—Ç—Ä–æ–≤ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏")
    args = ap.parse_args()

    # –ó–∞–≥—Ä—É–∂–∞–µ–º .env –¥–æ –ø—Ä–æ–≤–µ—Ä–∫–∏ –∫–ª—é—á–∞
    try:
        load_env()
    except Exception:
        pass

    if not os.getenv("OPENAI_API_KEY"):
        print("[ERROR] OPENAI_API_KEY –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
        return 1

    proc = SarsekenovProcessor(args.primary_model, args.refine_model)

    # –ï—Å–ª–∏ --input –Ω–µ –∑–∞–¥–∞–Ω, –∏–ª–∏ –≤ –ø–∞–ø–∫–µ –Ω–µ—Ç json, –ø—ã—Ç–∞–µ–º—Å—è –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å—É–±—Ç–∏—Ç—Ä—ã –∏–∑ urls.txt
    input_candidate: Optional[Path] = Path(args.input) if args.input else None
    urls_file = Path(args.urls_file)
    subtitles_dir = PROJECT_ROOT / "data" / "subtitles"

    if input_candidate is None:
        # –ê–≤—Ç–æ–∑–∞–≥—Ä—É–∑–∫–∞ –ø–æ urls.txt
        if urls_file.exists():
            print(f"[INFO] --input –Ω–µ —É–∫–∞–∑–∞–Ω. –ò—Å–ø–æ–ª—å–∑—É—é URLs –∏–∑: {urls_file}")
            extractor = YouTubeSubtitlesExtractor(str(subtitles_dir))
            with open(urls_file, "r", encoding="utf-8") as f:
                urls = [ln.strip() for ln in f if ln.strip()]
            for u in urls:
                try:
                    extractor.process_url(u, args.language)
                except Exception as e:
                    print(f"[ERROR] –ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å URL {u}: {e}")
            input_candidate = subtitles_dir
        else:
            print("[ERROR] –ù–µ —É–∫–∞–∑–∞–Ω --input –∏ –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç urls.txt. –£–∫–∞–∂–∏—Ç–µ –ø—É—Ç—å –∫ .json –∏–ª–∏ —Å–æ–∑–¥–∞–π—Ç–µ urls.txt")
            return 1

    in_path = input_candidate
    out_dir = Path(args.output)
    files: List[Path] = []
    if in_path.is_file():
        files = [in_path]
    else:
        files = list(in_path.glob("*.json"))
    if not files:
        print(f"[ERROR] –ù–µ –Ω–∞–π–¥–µ–Ω–æ JSON —Ñ–∞–π–ª–æ–≤ –≤ {in_path}")
        return 1

    out_dir.mkdir(parents=True, exist_ok=True)
    for jf in files:
        try:
            proc.process_file(jf, out_dir)
        except Exception as e:
            print(f"[ERROR] {jf.name}: {e}")
    return 0


if __name__ == "__main__":
    import sys
    sys.exit(main())


